{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::905847418383:role/service-role/AmazonSageMaker-ExecutionRole-20211005T160629\n",
      "sagemaker bucket: sagemaker-us-east-1-905847418383\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-06 14:12:18--  https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/dev-v0.1.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 458494 (448K) [text/plain]\n",
      "Saving to: ‘dev-v0.1.json.1’\n",
      "\n",
      "dev-v0.1.json.1     100%[===================>] 447.75K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2021-10-06 14:12:18 (92.9 MB/s) - ‘dev-v0.1.json.1’ saved [458494/458494]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/dev-v0.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-06 14:12:29--  https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/train-v0.1.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3078146 (2.9M) [text/plain]\n",
      "Saving to: ‘train-v0.1.json.1’\n",
      "\n",
      "train-v0.1.json.1   100%[===================>]   2.94M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2021-10-06 14:12:29 (251 MB/s) - ‘train-v0.1.json.1’ saved [3078146/3078146]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/train-v0.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train.json to s3://sagemaker-us-east-1-905847418383/qa-tr/data/train.json\n",
      "upload: data/val.json to s3://sagemaker-us-east-1-905847418383/qa-tr/data/val.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/train.json s3://$bucket/qa-tr/data/train.json\n",
    "!aws s3 cp data/val.json s3://$bucket/qa-tr/data/val.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_name_or_path': 'dbmdz/bert-base-turkish-uncased',\n",
    "#     'dataset_name':'squad',\n",
    "    'train_file': '/opt/ml/input/data/train/train.json',\n",
    "    'validation_file': '/opt/ml/input/data/val/val.json',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': True,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'num_train_epochs': 1,\n",
    "    'max_seq_length': 384,\n",
    "    'max_steps': 100,\n",
    "    'pad_to_max_length': True,\n",
    "    'doc_stride': 128,\n",
    "    'output_dir': '/opt/ml/model'\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/marshmellow77/transformers.git','branch': 'v4.6.0-release'}\n",
    "\n",
    "# instance configurations\n",
    "instance_type = 'ml.p3.16xlarge'\n",
    "instance_count = 1\n",
    "volume_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='run_qa.py',\n",
    "                                    source_dir='./examples/pytorch/question-answering',\n",
    "                                    git_config=git_config,\n",
    "#                                     metric_definitions=metric_definitions,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.6',\n",
    "                                    pytorch_version='1.7',\n",
    "                                    py_version='py36',\n",
    "#                                     distribution= distribution,\n",
    "                                    hyperparameters= hyperparameters,\n",
    "                                    disable_profiler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 08:43:29 Starting - Starting the training job...\n",
      "2021-10-07 08:43:34 Starting - Launching requested ML instances............\n",
      "2021-10-07 08:45:34 Starting - Preparing the instances for training......\n",
      "2021-10-07 08:46:56 Downloading - Downloading input data......\n",
      "2021-10-07 08:47:35 Training - Downloading the training image............\n",
      "2021-10-07 08:50:00 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-07 08:50:00,828 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-07 08:50:00,907 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-07 08:50:02,377 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-07 08:50:02,791 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.4.0->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3.0->-r requirements.txt (line 2)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.4.0->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.4.0->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.4.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.4.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-07 08:50:05,608 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"max_steps\": 100,\n",
      "        \"validation_file\": \"/opt/ml/input/data/val/val.json\",\n",
      "        \"do_train\": true,\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"do_eval\": true,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.json\",\n",
      "        \"doc_stride\": 128,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"max_seq_length\": 384,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"model_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-10-07-08-43-23-394\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-10-07-08-43-23-394/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qa.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"dbmdz/bert-base-turkish-uncased\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"train_file\":\"/opt/ml/input/data/train/train.json\",\"validation_file\":\"/opt/ml/input/data/val/val.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qa.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qa\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-10-07-08-43-23-394/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"dbmdz/bert-base-turkish-uncased\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"train_file\":\"/opt/ml/input/data/train/train.json\",\"validation_file\":\"/opt/ml/input/data/val/val.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-10-07-08-43-23-394\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-10-07-08-43-23-394/source/sourcedir.tar.gz\",\"module_name\":\"run_qa\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qa.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--doc_stride\",\"128\",\"--fp16\",\"True\",\"--max_seq_length\",\"384\",\"--max_steps\",\"100\",\"--model_name_or_path\",\"dbmdz/bert-base-turkish-uncased\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\",\"--pad_to_max_length\",\"True\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--train_file\",\"/opt/ml/input/data/train/train.json\",\"--validation_file\",\"/opt/ml/input/data/val/val.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/val/val.json\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.json\u001b[0m\n",
      "\u001b[34mSM_HP_DOC_STRIDE=128\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=384\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=dbmdz/bert-base-turkish-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_qa.py --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --max_steps 100 --model_name_or_path dbmdz/bert-base-turkish-uncased --num_train_epochs 1 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --train_file /opt/ml/input/data/train/train.json --validation_file /opt/ml/input/data/val/val.json\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m10/07/2021 08:50:11 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 8distributed training: False, 16-bits training: True\u001b[0m\n",
      "\u001b[34m10/07/2021 08:50:11 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Oct07_08-50-10_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=8, mp_parameters=)\u001b[0m\n",
      "\u001b[34m10/07/2021 08:50:11 - WARNING - __main__ -   ---TEST---\u001b[0m\n",
      "\u001b[34m10/07/2021 08:50:11 - WARNING - __main__ -   /opt/ml/input/data/train/train.json\u001b[0m\n",
      "\u001b[34m10/07/2021 08:50:11 - WARNING - __main__ -   /opt/ml/input/data/val/val.json\u001b[0m\n",
      "\u001b[34m10/07/2021 08:50:11 - WARNING - datasets.builder -   Using custom data configuration default-2fa624c41936ac76\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-2fa624c41936ac76/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-2fa624c41936ac76/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,373 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn03ug29m\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,395 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,395 >> creating metadata file for /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2021-10-07 08:50:11,395 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2021-10-07 08:50:11,396 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2021-10-07 08:50:11,420 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2021-10-07 08:50:11,420 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,447 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyd2zi9z0\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,479 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,479 >> creating metadata file for /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,570 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn8fpw24z\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,591 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,591 >> creating metadata file for /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,687 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_da07jt6\u001b[0m\n",
      "\n",
      "2021-10-07 08:50:28 Uploading - Uploading generated training model\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:21,559 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:21,559 >> creating metadata file for /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2021-10-07 08:50:21,559 >> loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2021-10-07 08:50:23,596 >> Some weights of the model checkpoint at dbmdz/bert-base-turkish-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2021-10-07 08:50:23,596 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015[INFO|file_utils.py:1532] 2021-10-07 08:50:11,373 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn03ug29m\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 385/385 [00:00<00:00, 569kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,395 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,395 >> creating metadata file for /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2021-10-07 08:50:11,395 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2021-10-07 08:50:11,396 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2021-10-07 08:50:11,420 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2021-10-07 08:50:11,420 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,447 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyd2zi9z0\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 263k/263k [00:00<00:00, 43.7MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,479 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,479 >> creating metadata file for /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,570 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn8fpw24z\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/59.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 59.0/59.0 [00:00<00:00, 73.0kB/s]\u001b[0m\n",
      "\u001b[34m2021-10-07 08:50:25,164 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.6 run_qa.py --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --max_steps 100 --model_name_or_path dbmdz/bert-base-turkish-uncased --num_train_epochs 1 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --train_file /opt/ml/input/data/train/train.json --validation_file /opt/ml/input/data/val/val.json\"\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,591 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,591 >> creating metadata file for /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,687 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_da07jt6\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]#015Downloading:   0%|          | 2.22M/445M [00:00<00:19, 22.2MB/s]#015Downloading:   1%|▏         | 6.20M/445M [00:00<00:17, 25.6MB/s]#015Downloading:   2%|▏         | 8.39M/445M [00:00<00:20, 21.7MB/s]#015Downloading:   3%|▎         | 13.2M/445M [00:00<00:16, 26.0MB/s]#015Downloading:   4%|▍         | 18.1M/445M [00:00<00:14, 30.3MB/s]#015Downloading:   5%|▌         | 23.2M/445M [00:00<00:12, 34.4MB/s]#015Downloading:   6%|▋         | 28.3M/445M [00:00<00:10, 38.1MB/s]#015Downloading:   7%|▋         | 33.0M/445M [00:00<00:10, 40.4MB/s]#015Downloading:   9%|▊         | 38.0M/445M [00:00<00:09, 42.9MB/s]#015Downloading:  10%|▉         | 43.1M/445M [00:01<00:08, 45.0MB/s]#015Downloading:  11%|█         | 48.0M/445M [00:01<00:08, 46.2MB/s]#015Downloading:  12%|█▏        | 53.1M/445M [00:01<00:08, 47.7MB/s]#015Downloading:  13%|█▎        | 58.3M/445M [00:01<00:07, 48.8MB/s]#015Downloading:  14%|█▍        | 63.4M/445M [00:01<00:07, 49.5MB/s]#015Downloading:  15%|█▌        | 68.6M/445M [00:01<00:07, 50.1MB/s]#015Downloading:  17%|█▋        | 73.8M/445M [00:01<00:07, 50.6MB/s]#015Downloading:  18%|█▊        | 78.9M/445M [00:01<00:07, 50.9MB/s]#015Downloading:  19%|█▉        | 84.1M/445M [00:01<00:07, 51.1MB/s]#015Downloading:  20%|██        | 89.3M/445M [00:01<00:06, 51.3MB/s]#015Downloading:  21%|██        | 94.4M/445M [00:02<00:06, 51.3MB/s]#015Downloading:  22%|██▏       | 99.6M/445M [00:02<00:06, 51.4MB/s]#015Downloading:  24%|██▎       | 105M/445M [00:02<00:06, 51.7MB/s] #015Downloading:  25%|██▍       | 110M/445M [00:02<00:07, 42.0MB/s]#015Downloading:  26%|██▌       | 115M/445M [00:02<00:07, 44.3MB/s]#015Downloading:  27%|██▋       | 120M/445M [00:02<00:07, 46.1MB/s]#015Downloading:  28%|██▊       | 125M/445M [00:02<00:06, 47.6MB/s]#015Downloading:  29%|██▉       | 130M/445M [00:02<00:08, 36.8MB/s]#015Downloading:  30%|███       | 135M/445M [00:03<00:07, 40.1MB/s]#015Downloading:  32%|███▏      | 140M/445M [00:03<00:07, 42.7MB/s]#015Downloading:  33%|███▎      | 145M/445M [00:03<00:07, 42.0MB/s]#015Downloading:  34%|███▎      | 150M/445M [00:03<00:06, 44.4MB/s]#015Downloading:  35%|███▍      | 155M/445M [00:03<00:07, 39.4MB/s]#015Downloading:  36%|███▌      | 160M/445M [00:03<00:06, 42.2MB/s]#015Downloading:  37%|███▋      | 165M/445M [00:03<00:06, 44.6MB/s]#015Downloading:  38%|███▊      | 170M/445M [00:03<00:05, 46.3MB/s]#015Downloading:  39%|███▉      | 175M/445M [00:03<00:05, 47.8MB/s]#015Downloading:  40%|████      | 180M/445M [00:03<00:05, 48.8MB/s]#015Downloading:  42%|████▏     | 185M/445M [00:04<00:05, 49.5MB/s]#015Downloading:  43%|████▎     | 191M/445M [00:04<00:05, 50.4MB/s]#015Downloading:  44%|████▍     | 196M/445M [00:04<00:04, 51.0MB/s]#015Downloading:  45%|████▌     | 201M/445M [00:04<00:04, 50.5MB/s]#015Downloading:  46%|████▋     | 206M/445M [00:04<00:06, 39.5MB/s]#015Downloading:  47%|████▋     | 211M/445M [00:04<00:05, 42.4MB/s]#015Downloading:  49%|████▊     | 216M/445M [00:04<00:05, 44.8MB/s]#015Downloading:  50%|████▉     | 221M/445M [00:04<00:04, 46.6MB/s]#015Downloading:  51%|█████     | 227M/445M [00:04<00:04, 48.1MB/s]#015Downloading:  52%|█████▏    | 232M/445M [00:05<00:04, 49.2MB/s]#015Downloading:  53%|█████▎    | 237M/445M [00:05<00:04, 50.1MB/s]#015Downloading:  54%|█████▍    | 242M/445M [00:05<00:04, 50.7MB/s]#015Downloading:  56%|█████▌    | 248M/445M [00:05<00:03, 51.1MB/s]#015Downloading:  57%|█████▋    | 253M/445M [00:05<00:03, 51.4MB/s]#015Downloading:  58%|█████▊    | 258M/445M [00:05<00:03, 51.7MB/s]#015Downloading:  59%|█████▉    | 263M/445M [00:05<00:05, 35.5MB/s]#015Downloading:  60%|██████    | 268M/445M [00:05<00:04, 39.4MB/s]#015Downloading:  61%|██████▏   | 273M/445M [00:06<00:04, 40.7MB/s]#015Downloading:  62%|██████▏   | 277M/445M [00:06<00:04, 41.2MB/s]#015Downloading:  64%|██████▎   | 283M/445M [00:06<00:03, 44.0MB/s]#015Downloading:  65%|██████▍   | 288M/445M [00:06<00:03, 46.1MB/s]#015Downloading:  66%|██████▌   | 293M/445M [00:06<00:03, 47.8MB/s]#015Downloading:  67%|██████▋   | 298M/445M [00:06<00:02, 49.0MB/s]#015Downloading:  68%|██████▊   | 304M/445M [00:06<00:02, 50.1MB/s]#015Downloading:  69%|██████▉   | 309M/445M [00:06<00:02, 47.7MB/s]#015Downloading:  70%|███████   | 314M/445M [00:06<00:03, 43.4MB/s]#015Downloading:  72%|███████▏  | 319M/445M [00:06<00:02, 45.7MB/s]#015Downloading:  73%|███████▎  | 324M/445M [00:07<00:02, 47.5MB/s]#015Downloading:  74%|███████▍  | 329M/445M [00:07<00:02, 41.5MB/s]#015Downloading:  75%|███████▌  | 334M/445M [00:07<00:02, 37.6MB/s]#015Downloading:  76%|███████▌  | 338M/445M [00:07<00:05, 18.7MB/s]#015Downloading:  77%|███████▋  | 343M/445M [00:07<00:04, 23.1MB/s]#015Downloading:  78%|███████▊  | 349M/445M [00:08<00:03, 27.8MB/s]#015Downloading:  80%|███████▉  | 354M/445M [00:08<00:02, 32.4MB/s]#015Downloading:  81%|████████  | 359M/445M [00:08<00:02, 36.7MB/s]#015Downloading:  82%|████████▏ | 365M/445M [00:08<00:01, 40.6MB/s]#015Downloading:  83%|████████▎ | 370M/445M [00:08<00:01, 44.6MB/s]#015Downloading:  85%|████████▍ | 377M/445M [00:08<00:01, 48.7MB/s]#015Downloading:  86%|████████▌ | 383M/445M [00:08<00:01, 52.1MB/s]#015Downloading:  87%|████████▋ | 389M/445M [00:08<00:01, 54.9MB/s]#015Downloading:  89%|████████▉ | 395M/445M [00:08<00:00, 57.1MB/s]#015Downloading:  90%|█████████ | 402M/445M [00:08<00:00, 58.9MB/s]#015Downloading:  92%|█████████▏| 408M/445M [00:09<00:00, 60.2MB/s]#015Downloading:  93%|█████████▎| 414M/445M [00:09<00:00, 61.2MB/s]#015Downloading:  95%|█████████▍| 421M/445M [00:09<00:00, 61.8MB/s]#015Downloading:  96%|█████████▌| 427M/445M [00:09<00:00, 60.9MB/s]#015Downloading:  97%|█████████▋| 433M/445M [00:09<00:00, 61.5MB/s]#015Downloading:  99%|█████████▉| 440M/445M [00:09<00:00, 62.0MB/s]#015Downloading: 100%|██████████| 445M/445M [00:09<00:00, 46.0MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:21,559 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015[INFO|file_utils.py:1532] 2021-10-07 08:50:11,373 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn03ug29m\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 385/385 [00:00<00:00, 569kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,395 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,395 >> creating metadata file for /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2021-10-07 08:50:11,395 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2021-10-07 08:50:11,396 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:21,559 >> creating metadata file for /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2021-10-07 08:50:21,559 >> loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2021-10-07 08:50:23,596 >> Some weights of the model checkpoint at dbmdz/bert-base-turkish-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2021-10-07 08:50:23,596 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/9 [00:00<?, ?ba/s]#015  0%|          | 0/9 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"run_qa.py\", line 627, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 424, in main\n",
      "    load_from_cache_file=not data_args.overwrite_cache,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1498, in map\n",
      "    new_fingerprint=new_fingerprint,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 174, in wrapper\n",
      "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/fingerprint.py\", line 340, in wrapper\n",
      "    out = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1823, in _map_single\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2021-10-07 08:50:11,420 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2021-10-07 08:50:11,420 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,447 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyd2zi9z0\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 263k/263k [00:00<00:00, 43.7MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,479 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,479 >> creating metadata file for /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,570 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn8fpw24z\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/59.0 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 59.0/59.0 [00:00<00:00, 73.0kB/s]\n",
      "    offset=offset,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1715, in apply_function_on_filtered_inputs\n",
      "    function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\n",
      "  File \"run_qa.py\", line 383, in prepare_train_features\n",
      "    end_char = start_char + len(answers[\"text\"][0])\u001b[0m\n",
      "\u001b[34mTypeError: must be str, not int\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:11,591 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:11,591 >> creating metadata file for /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2021-10-07 08:50:11,591 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-10-07 08:50:11,687 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_da07jt6\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]#015Downloading:   0%|          | 2.22M/445M [00:00<00:19, 22.2MB/s]#015Downloading:   1%|â         | 6.20M/445M [00:00<00:17, 25.6MB/s]#015Downloading:   2%|â         | 8.39M/445M [00:00<00:20, 21.7MB/s]#015Downloading:   3%|â         | 13.2M/445M [00:00<00:16, 26.0MB/s]#015Downloading:   4%|â         | 18.1M/445M [00:00<00:14, 30.3MB/s]#015Downloading:   5%|â         | 23.2M/445M [00:00<00:12, 34.4MB/s]#015Downloading:   6%|â         | 28.3M/445M [00:00<00:10, 38.1MB/s]#015Downloading:   7%|â         | 33.0M/445M [00:00<00:10, 40.4MB/s]#015Downloading:   9%|â         | 38.0M/445M [00:00<00:09, 42.9MB/s]#015Downloading:  10%|â         | 43.1M/445M [00:01<00:08, 45.0MB/s]#015Downloading:  11%|â         | 48.0M/445M [00:01<00:08, 46.2MB/s]#015Downloading:  12%|ââ        | 53.1M/445M [00:01<00:08, 47.7MB/s]#015Downloading:  13%|ââ        | 58.3M/445M [00:01<00:07, 48.8MB/s]#015Downloading:  14%|ââ        | 63.4M/445M [00:01<00:07, 49.5MB/s]#015Downloading:  15%|ââ        | 68.6M/445M [00:01<00:07, 50.1MB/s]#015Downloading:  17%|ââ        | 73.8M/445M [00:01<00:07, 50.6MB/s]#015Downloading:  18%|ââ        | 78.9M/445M [00:01<00:07, 50.9MB/s]#015Downloading:  19%|ââ        | 84.1M/445M [00:01<00:07, 51.1MB/s]#015Downloading:  20%|ââ        | 89.3M/445M [00:01<00:06, 51.3MB/s]#015Downloading:  21%|ââ        | 94.4M/445M [00:02<00:06, 51.3MB/s]#015Downloading:  22%|âââ       | 99.6M/445M [00:02<00:06, 51.4MB/s]#015Downloading:  24%|âââ       | 105M/445M [00:02<00:06, 51.7MB/s] #015Downloading:  25%|âââ       | 110M/445M [00:02<00:07, 42.0MB/s]#015Downloading:  26%|âââ       | 115M/445M [00:02<00:07, 44.3MB/s]#015Downloading:  27%|âââ       | 120M/445M [00:02<00:07, 46.1MB/s]#015Downloading:  28%|âââ       | 125M/445M [00:02<00:06, 47.6MB/s]#015Downloading:  29%|âââ       | 130M/445M [00:02<00:08, 36.8MB/s]#015Downloading:  30%|âââ       | 135M/445M [00:03<00:07, 40.1MB/s]#015Downloading:  32%|ââââ      | 140M/445M [00:03<00:07, 42.7MB/s]#015Downloading:  33%|ââââ      | 145M/445M [00:03<00:07, 42.0MB/s]#015Downloading:  34%|ââââ      | 150M/445M [00:03<00:06, 44.4MB/s]#015Downloading:  35%|ââââ      | 155M/445M [00:03<00:07, 39.4MB/s]#015Downloading:  36%|ââââ      | 160M/445M [00:03<00:06, 42.2MB/s]#015Downloading:  37%|ââââ      | 165M/445M [00:03<00:06, 44.6MB/s]#015Downloading:  38%|ââââ      | 170M/445M [00:03<00:05, 46.3MB/s]#015Downloading:  39%|ââââ      | 175M/445M [00:03<00:05, 47.8MB/s]#015Downloading:  40%|ââââ      | 180M/445M [00:03<00:05, 48.8MB/s]#015Downloading:  42%|âââââ     | 185M/445M [00:04<00:05, 49.5MB/s]#015Downloading:  43%|âââââ     | 191M/445M [00:04<00:05, 50.4MB/s]#015Downloading:  44%|âââââ     | 196M/445M [00:04<00:04, 51.0MB/s]#015Downloading:  45%|âââââ     | 201M/445M [00:04<00:04, 50.5MB/s]#015Downloading:  46%|âââââ     | 206M/445M [00:04<00:06, 39.5MB/s]#015Downloading:  47%|âââââ     | 211M/445M [00:04<00:05, 42.4MB/s]#015Downloading:  49%|âââââ     | 216M/445M [00:04<00:05, 44.8MB/s]#015Downloading:  50%|âââââ     | 221M/445M [00:04<00:04, 46.6MB/s]#015Downloading:  51%|âââââ     | 227M/445M [00:04<00:04, 48.1MB/s]#015Downloading:  52%|ââââââ    | 232M/445M [00:05<00:04, 49.2MB/s]#015Downloading:  53%|ââââââ    | 237M/445M [00:05<00:04, 50.1MB/s]#015Downloading:  54%|ââââââ    | 242M/445M [00:05<00:04, 50.7MB/s]#015Downloading:  56%|ââââââ    | 248M/445M [00:05<00:03, 51.1MB/s]#015Downloading:  57%|ââââââ    | 253M/445M [00:05<00:03, 51.4MB/s]#015Downloading:  58%|ââââââ    | 258M/445M [00:05<00:03, 51.7MB/s]#015Downloading:  59%|ââââââ    | 263M/445M [00:05<00:05, 35.5MB/s]#015Downloading:  60%|ââââââ    | 268M/445M [00:05<00:04, 39.4MB/s]#015Downloading:  61%|âââââââ   | 273M/445M [00:06<00:04, 40.7MB/s]#015Downloading:  62%|âââââââ   | 277M/445M [00:06<00:04, 41.2MB/s]#015Downloading:  64%|âââââââ   | 283M/445M [00:06<00:03, 44.0MB/s]#015Downloading:  65%|âââââââ   | 288M/445M [00:06<00:03, 46.1MB/s]#015Downloading:  66%|âââââââ   | 293M/445M [00:06<00:03, 47.8MB/s]#015Downloading:  67%|âââââââ   | 298M/445M [00:06<00:02, 49.0MB/s]#015Downloading:  68%|âââââââ   | 304M/445M [00:06<00:02, 50.1MB/s]#015Downloading:  69%|âââââââ   | 309M/445M [00:06<00:02, 47.7MB/s]#015Downloading:  70%|âââââââ   | 314M/445M [00:06<00:03, 43.4MB/s]#015Downloading:  72%|ââââââââ  | 319M/445M [00:06<00:02, 45.7MB/s]#015Downloading:  73%|ââââââââ  | 324M/445M [00:07<00:02, 47.5MB/s]#015Downloading:  74%|ââââââââ  | 329M/445M [00:07<00:02, 41.5MB/s]#015Downloading:  75%|ââââââââ  | 334M/445M [00:07<00:02, 37.6MB/s]#015Downloading:  76%|ââââââââ  | 338M/445M [00:07<00:05, 18.7MB/s]#015Downloading:  77%|ââââââââ  | 343M/445M [00:07<00:04, 23.1MB/s]#015Downloading:  78%|ââââââââ  | 349M/445M [00:08<00:03, 27.8MB/s]#015Downloading:  80%|ââââââââ  | 354M/445M [00:08<00:02, 32.4MB/s]#015Downloading:  81%|ââââââââ  | 359M/445M [00:08<00:02, 36.7MB/s]#015Downloading:  82%|âââââââââ | 365M/445M [00:08<00:01, 40.6MB/s]#015Downloading:  83%|âââââââââ | 370M/445M [00:08<00:01, 44.6MB/s]#015Downloading:  85%|âââââââââ | 377M/445M [00:08<00:01, 48.7MB/s]#015Downloading:  86%|âââââââââ | 383M/445M [00:08<00:01, 52.1MB/s]#015Downloading:  87%|âââââââââ | 389M/445M [00:08<00:01, 54.9MB/s]#015Downloading:  89%|âââââââââ | 395M/445M [00:08<00:00, 57.1MB/s]#015Downloading:  90%|âââââââââ | 402M/445M [00:08<00:00, 58.9MB/s]#015Downloading:  92%|ââââââââââ| 408M/445M [00:09<00:00, 60.2MB/s]#015Downloading:  93%|ââââââââââ| 414M/445M [00:09<00:00, 61.2MB/s]#015Downloading:  95%|ââââââââââ| 421M/445M [00:09<00:00, 61.8MB/s]#015Downloading:  96%|ââââââââââ| 427M/445M [00:09<00:00, 60.9MB/s]#015Downloading:  97%|ââââââââââ| 433M/445M [00:09<00:00, 61.5MB/s]#015Downloading:  99%|ââââââââââ| 440M/445M [00:09<00:00, 62.0MB/s]#015Downloading: 100%|ââââââââââ| 445M/445M [00:09<00:00, 46.0MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-10-07 08:50:21,559 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-10-07 08:50:21,559 >> creating metadata file for /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2021-10-07 08:50:21,559 >> loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2021-10-07 08:50:23,596 >> Some weights of the model checkpoint at dbmdz/bert-base-turkish-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2021-10-07 08:50:23,596 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/9 [00:00<?, ?ba/s]#015  0%|          | 0/9 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"run_qa.py\", line 627, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 424, in main\n",
      "    load_from_cache_file=not data_args.overwrite_cache,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1498, in map\n",
      "    new_fingerprint=new_fingerprint,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 174, in wrapper\n",
      "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/fingerprint.py\", line 340, in wrapper\n",
      "    out = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1823, in _map_single\n",
      "    offset=offset,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py\", line 1715, in apply_function_on_filtered_inputs\n",
      "    function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\n",
      "  File \"run_qa.py\", line 383, in prepare_train_features\n",
      "    end_char = start_char + len(answers[\"text\"][0])\u001b[0m\n",
      "\u001b[34mTypeError: must be str, not int\u001b[0m\n",
      "\n",
      "2021-10-07 08:50:35 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2021-10-07-08-43-23-394: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 run_qa.py --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --max_steps 100 --model_name_or_path dbmdz/bert-base-turkish-uncased --num_train_epochs 1 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --train_file /opt/ml/input/data/train/train.json --validation_file /opt/ml/input/data/val/val.json\"\n\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \r[INFO|file_utils.py:1532] 2021-10-07 08:50:11,373 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn03ug29m\n\rDownloading:   0%|          | 0.00/385 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 385/385 [00:00<00:00, 569kB/s]\n[INFO|file_utils.py:1536] 2021-10-07 08:50:11,395 >",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-74ad1aff2c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m's3://sagemaker-us-east-1-905847418383/qa-tr/data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m's3://sagemaker-us-east-1-905847418383/qa-tr/data/'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3282\u001b[0m                 ),\n\u001b[1;32m   3283\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3284\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3285\u001b[0m             )\n\u001b[1;32m   3286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2021-10-07-08-43-23-394: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 run_qa.py --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --max_steps 100 --model_name_or_path dbmdz/bert-base-turkish-uncased --num_train_epochs 1 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --train_file /opt/ml/input/data/train/train.json --validation_file /opt/ml/input/data/val/val.json\"\n\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \r[INFO|file_utils.py:1532] 2021-10-07 08:50:11,373 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn03ug29m\n\rDownloading:   0%|          | 0.00/385 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 385/385 [00:00<00:00, 569kB/s]\n[INFO|file_utils.py:1536] 2021-10-07 08:50:11,395 >"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': 's3://sagemaker-us-east-1-905847418383/qa-tr/data/', 'val': 's3://sagemaker-us-east-1-905847418383/qa-tr/data/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
