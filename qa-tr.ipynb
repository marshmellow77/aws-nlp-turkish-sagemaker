{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c21354",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13733083",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "716f1963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::905847418383:role/service-role/AmazonSageMaker-ExecutionRole-20211005T160629\n",
      "sagemaker bucket: sagemaker-us-east-1-905847418383\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5c889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-06 14:12:18--  https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/dev-v0.1.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 458494 (448K) [text/plain]\n",
      "Saving to: ‘dev-v0.1.json.1’\n",
      "\n",
      "dev-v0.1.json.1     100%[===================>] 447.75K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2021-10-06 14:12:18 (92.9 MB/s) - ‘dev-v0.1.json.1’ saved [458494/458494]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/dev-v0.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595bdb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-06 14:12:29--  https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/train-v0.1.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3078146 (2.9M) [text/plain]\n",
      "Saving to: ‘train-v0.1.json.1’\n",
      "\n",
      "train-v0.1.json.1   100%[===================>]   2.94M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2021-10-06 14:12:29 (251 MB/s) - ‘train-v0.1.json.1’ saved [3078146/3078146]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/train-v0.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88676fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc76caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train.json to s3://sagemaker-us-east-1-905847418383/qa-tr/data/train.json\n",
      "upload: data/val.json to s3://sagemaker-us-east-1-905847418383/qa-tr/data/val.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/train.json s3://$bucket/qa-tr/data/train.json\n",
    "!aws s3 cp data/val.json s3://$bucket/qa-tr/data/val.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3c7d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_name_or_path': 'dbmdz/bert-base-turkish-uncased',\n",
    "#     'dataset_name':'squad',\n",
    "    'train_file': '/opt/ml/input/data/train/train.json',\n",
    "    'validation_file': '/opt/ml/input/data/val/val.json',\n",
    "    'do_train': True,\n",
    "    'do_eval': False,\n",
    "    'fp16': True,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'num_train_epochs': 2,\n",
    "    'max_seq_length': 384,\n",
    "#     'max_steps': 100,\n",
    "    'pad_to_max_length': True,\n",
    "    'doc_stride': 128,\n",
    "    'output_dir': '/opt/ml/model'\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "# git_config = {'repo': 'https://github.com/marshmellow77/transformers.git','branch': 'v4.10.0'}\n",
    "\n",
    "# instance configurations\n",
    "instance_type = 'ml.p3.16xlarge'\n",
    "instance_count = 1\n",
    "volume_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bb4db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='run_qa.py',\n",
    "#                                     source_dir='./examples/pytorch/question-answering',\n",
    "                                    source_dir='./scripts',\n",
    "#                                     git_config=git_config,\n",
    "#                                     metric_definitions=metric_definitions,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.10',\n",
    "                                    pytorch_version='1.9',\n",
    "                                    py_version='py38',\n",
    "#                                     distribution= distribution,\n",
    "                                    hyperparameters= hyperparameters,\n",
    "                                    disable_profiler=True,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b6193c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-12 05:07:28 Starting - Starting the training job...\n",
      "2021-10-12 05:07:32 Starting - Launching requested ML instances............\n",
      "2021-10-12 05:09:30 Starting - Preparing the instances for training.........\n",
      "2021-10-12 05:11:21 Downloading - Downloading input data\n",
      "2021-10-12 05:11:21 Training - Downloading the training image...........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-12 05:15:48,985 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-12 05:15:49,060 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-12 05:15:49,067 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-12 05:15:49,729 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"validation_file\": \"/opt/ml/input/data/val/val.json\",\n",
      "        \"do_train\": true,\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"do_eval\": false,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.json\",\n",
      "        \"doc_stride\": 128,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"max_seq_length\": 384,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"model_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-10-12-05-07-28-087\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-10-12-05-07-28-087/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qa.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":false,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"model_name_or_path\":\"dbmdz/bert-base-turkish-uncased\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"train_file\":\"/opt/ml/input/data/train/train.json\",\"validation_file\":\"/opt/ml/input/data/val/val.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qa.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qa\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-10-12-05-07-28-087/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":false,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"model_name_or_path\":\"dbmdz/bert-base-turkish-uncased\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"train_file\":\"/opt/ml/input/data/train/train.json\",\"validation_file\":\"/opt/ml/input/data/val/val.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-10-12-05-07-28-087\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-10-12-05-07-28-087/source/sourcedir.tar.gz\",\"module_name\":\"run_qa\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qa.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"False\",\"--do_train\",\"True\",\"--doc_stride\",\"128\",\"--fp16\",\"True\",\"--max_seq_length\",\"384\",\"--model_name_or_path\",\"dbmdz/bert-base-turkish-uncased\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/opt/ml/model\",\"--pad_to_max_length\",\"True\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--train_file\",\"/opt/ml/input/data/train/train.json\",\"--validation_file\",\"/opt/ml/input/data/val/val.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/val/val.json\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=false\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.json\u001b[0m\n",
      "\u001b[34mSM_HP_DOC_STRIDE=128\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=384\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=dbmdz/bert-base-turkish-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 run_qa.py --do_eval False --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --model_name_or_path dbmdz/bert-base-turkish-uncased --num_train_epochs 2 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --train_file /opt/ml/input/data/train/train.json --validation_file /opt/ml/input/data/val/val.json\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 8distributed training: False, 16-bits training: True\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=8,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.NO,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Oct12_05-15-54_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=2.0,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=model,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=None,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - __main__ - Test1\u001b[0m\n",
      "\u001b[34mTest2\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - WARNING - datasets.builder - Using custom data configuration default-9a2661e06d79b7d7\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-9a2661e06d79b7d7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-9a2661e06d79b7d7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - datasets.builder - Generating split validation\u001b[0m\n",
      "\u001b[34m10/12/2021 05:15:55 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-9a2661e06d79b7d7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-10-12 05:15:55,341 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsaqab1hk\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-10-12 05:15:55,362 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-10-12 05:15:55,362 >> creating metadata file for /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-10-12 05:15:55,363 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-10-12 05:15:55,363 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-10-12 05:15:55,387 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8ma7nq7q\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-10-12 05:15:55,406 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-10-12 05:15:55,406 >> creating metadata file for /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-10-12 05:15:55,428 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-10-12 05:15:55,429 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-10-12 05:15:55,479 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp565mu27t\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-10-12 05:15:55,508 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-10-12 05:15:55,508 >> creating metadata file for /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,600 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,601 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,601 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,601 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,601 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-10-12 05:15:55,623 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-10-12 05:15:55,623 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-10-12 05:15:55,679 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-10-12 05:15:55,680 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-10-12 05:15:55,789 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpphrfn0hh\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-10-12 05:16:08,161 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-10-12 05:16:08,161 >> creating metadata file for /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1279] 2021-10-12 05:16:08,162 >> loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1515] 2021-10-12 05:16:09,877 >> Some weights of the model checkpoint at dbmdz/bert-base-turkish-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1526] 2021-10-12 05:16:09,877 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-9a2661e06d79b7d7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-dd2b3fdf6eac7001.arrow\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqi2cn82f\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/9b6df77cd2566eca67dda976ee18f34e95536ec8e33f9292bfc3daea9b20a092.ab3a5db6a587c35cfd241275240e52547dd1e093c74b3ee4f7798d9f6c6304ec.py\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9b6df77cd2566eca67dda976ee18f34e95536ec8e33f9292bfc3daea9b20a092.ab3a5db6a587c35cfd241275240e52547dd1e093c74b3ee4f7798d9f6c6304ec.py\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpy2qytsp0\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/188f3c7a325773b47d41f3e0f7ab9fd3cb20e597010b3a9d780c878dacc10ce3.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/188f3c7a325773b47d41f3e0f7ab9fd3cb20e597010b3a9d780c878dacc10ce3.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py to /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/squad.py\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/dataset_infos.json\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/squad.json\u001b[0m\n",
      "\u001b[34m10/12/2021 05:16:13 - INFO - datasets.load - Copying local import file from /root/.cache/huggingface/datasets/downloads/188f3c7a325773b47d41f3e0f7ab9fd3cb20e597010b3a9d780c878dacc10ce3.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/evaluate.py\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:414] 2021-10-12 05:16:17,451 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1168] 2021-10-12 05:16:17,463 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1169] 2021-10-12 05:16:17,463 >>   Num examples = 11466\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1170] 2021-10-12 05:16:17,463 >>   Num Epochs = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2021-10-12 05:16:17,463 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2021-10-12 05:16:17,463 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2021-10-12 05:16:17,463 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2021-10-12 05:16:17,463 >>   Total optimization steps = 718\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.662 algo-1:23 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.789 algo-1:23 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.789 algo-1:23 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.789 algo-1:23 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.790 algo-1:23 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.790 algo-1:23 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.948 algo-1:23 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:24576000\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.949 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.950 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.951 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.952 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.953 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.954 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.955 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.956 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.qa_outputs.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:593] Total Trainable Params: 110028290\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.957 algo-1:23 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-10-12 05:16:17.960 algo-1:23 INFO hook.py:488] Hook is writing from the hook with pid: 23\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-12 05:16:23 Training - Training image download completed. Training in progress.\u001b[34malgo-1:23:23 [0] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34mNCCL version 2.7.8+cuda11.1\u001b[0m\n",
      "\u001b[34m{'loss': 1.5666, 'learning_rate': 1.5389972144846798e-05, 'epoch': 1.39}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1935] 2021-10-12 05:19:46,814 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2021-10-12 05:19:46,815 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-10-12 05:19:47,596 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2021-10-12 05:19:47,597 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2021-10-12 05:19:47,597 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1366] 2021-10-12 05:21:10,620 >> \n",
      "\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m{'train_runtime': 293.1569, 'train_samples_per_second': 78.224, 'train_steps_per_second': 2.449, 'train_loss': 1.3551674632972994, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1935] 2021-10-12 05:21:10,621 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2021-10-12 05:21:10,622 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-10-12 05:21:11,367 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2021-10-12 05:21:11,368 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2021-10-12 05:21:11,368 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     1.3552\n",
      "  train_runtime            = 0:04:53.15\n",
      "  train_samples            =      11466\n",
      "  train_samples_per_second =     78.224\n",
      "  train_steps_per_second   =      2.449\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#015100%|██████████| 2/2 [00:00<00:00, 19021.79it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#015100%|██████████| 2/2 [00:00<00:00, 1719.33it/s]\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015#015  0%|          | 0/2 [00:00<?, ?it/s]#015100%|██████████| 2/2 [00:00<00:00, 341.78it/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-10-12 05:15:55,341 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsaqab1hk\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 385/385 [00:00<00:00, 579kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-10-12 05:15:55,362 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-10-12 05:15:55,362 >> creating metadata file for /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-10-12 05:15:55,363 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-10-12 05:15:55,363 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-10-12 05:15:55,387 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8ma7nq7q\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/59.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 59.0/59.0 [00:00<00:00, 99.8kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-10-12 05:15:55,406 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-10-12 05:15:55,406 >> creating metadata file for /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-10-12 05:15:55,428 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-10-12 05:15:55,429 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m2021-10-12 05:21:14,691 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-10-12 05:15:55,479 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp565mu27t\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 263k/263k [00:00<00:00, 36.3MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-10-12 05:15:55,508 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-10-12 05:15:55,508 >> creating metadata file for /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,600 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,601 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,601 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,601 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2021-10-12 05:15:55,601 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-10-12 05:15:55,623 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-10-12 05:15:55,623 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2021-10-12 05:15:55,679 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2021-10-12 05:15:55,680 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2021-10-12 05:15:55,789 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpphrfn0hh\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]#015Downloading:   1%|          | 5.52M/445M [00:00<00:07, 55.2MB/s]#015Downloading:   3%|▎         | 12.0M/445M [00:00<00:07, 61.1MB/s]#015Downloading:   4%|▍         | 18.2M/445M [00:00<00:14, 29.9MB/s]#015Downloading:   5%|▌         | 24.4M/445M [00:00<00:11, 37.7MB/s]#015Downloading:   7%|▋         | 29.4M/445M [00:00<00:10, 38.7MB/s]#015Downloading:   8%|▊         | 34.1M/445M [00:00<00:10, 38.8MB/s]#015Downloading:   9%|▉         | 40.5M/445M [00:00<00:08, 45.5MB/s]#015Downloading:  10%|█         | 45.6M/445M [00:01<00:09, 42.9MB/s]#015Downloading:  11%|█▏        | 50.9M/445M [00:01<00:08, 45.5MB/s]#015Downloading:  13%|█▎        | 57.2M/445M [00:01<00:07, 50.4MB/s]#015Downloading:  14%|█▍        | 63.5M/445M [00:01<00:07, 53.7MB/s]#015Downloading:  16%|█▌        | 69.1M/445M [00:01<00:10, 35.5MB/s]#015Downloading:  17%|█▋        | 73.7M/445M [00:01<00:09, 37.5MB/s]#015Downloading:  18%|█▊        | 78.2M/445M [00:01<00:10, 35.9MB/s]#015Downloading:  19%|█▉        | 83.9M/445M [00:02<00:09, 37.1MB/s]#015Downloading:  20%|██        | 90.3M/445M [00:02<00:08, 43.5MB/s]#015Downloading:  21%|██▏       | 95.1M/445M [00:02<00:08, 41.3MB/s]#015Downloading:  23%|██▎       | 101M/445M [00:02<00:08, 41.9MB/s] #015Downloading:  24%|██▍       | 107M/445M [00:02<00:07, 46.3MB/s]#015Downloading:  25%|██▌       | 111M/445M [00:02<00:09, 35.1MB/s]#015Downloading:  26%|██▋       | 117M/445M [00:02<00:08, 38.9MB/s]#015Downloading:  28%|██▊       | 124M/445M [00:02<00:07, 44.2MB/s]#015Downloading:  29%|██▉       | 129M/445M [00:03<00:10, 31.5MB/s]#015Downloading:  30%|███       | 135M/445M [00:03<00:08, 38.4MB/s]#015Downloading:  32%|███▏      | 141M/445M [00:03<00:07, 41.1MB/s]#015Downloading:  33%|███▎      | 146M/445M [00:03<00:08, 36.7MB/s]#015Downloading:  34%|███▍      | 151M/445M [00:03<00:07, 36.9MB/s]#015Downloading:  35%|███▌      | 157M/445M [00:03<00:06, 42.9MB/s]#015Downloading:  36%|███▋      | 162M/445M [00:04<00:08, 35.2MB/s]#015Downloading:  37%|███▋      | 166M/445M [00:04<00:07, 35.3MB/s]#015Downloading:  38%|███▊      | 170M/445M [00:04<00:07, 36.0MB/s]#015Downloading:  39%|███▉      | 174M/445M [00:04<00:07, 35.4MB/s]#015Downloading:  40%|███▉      | 178M/445M [00:04<00:08, 32.1MB/s]#015Downloading:  41%|████      | 183M/445M [00:04<00:07, 33.9MB/s]#015Downloading:  42%|████▏     | 186M/445M [00:04<00:08, 30.2MB/s]#015Downloading:  43%|████▎     | 191M/445M [00:05<00:08, 28.8MB/s]#015Downloading:  44%|████▎     | 194M/445M [00:05<00:09, 25.4MB/s]#015Downloading:  45%|████▌     | 201M/445M [00:05<00:07, 34.1MB/s]#015Downloading:  46%|████▋     | 207M/445M [00:05<00:05, 40.5MB/s]#015Downloading:  48%|████▊     | 211M/445M [00:05<00:07, 33.1MB/s]#015Downloading:  49%|████▉     | 218M/445M [00:05<00:06, 35.8MB/s]#015Downloading:  50%|█████     | 225M/445M [00:05<00:05, 42.1MB/s]#015Downloading:  52%|█████▏    | 229M/445M [00:06<00:06, 34.3MB/s]#015Downloading:  53%|█████▎    | 235M/445M [00:06<00:06, 33.9MB/s]#015Downloading:  54%|█████▍    | 241M/445M [00:06<00:05, 39.7MB/s]#015Downloading:  55%|█████▌    | 246M/445M [00:06<00:05, 39.3MB/s]#015Downloading:  57%|█████▋    | 252M/445M [00:06<00:04, 45.0MB/s]#015Downloading:  58%|█████▊    | 257M/445M [00:06<00:04, 38.3MB/s]#015Downloading:  59%|█████▊    | 261M/445M [00:06<00:04, 38.6MB/s]#015Downloading:  60%|█████▉    | 265M/445M [00:06<00:04, 38.6MB/s]#015Downloading:  60%|██████    | 269M/445M [00:07<00:04, 36.1MB/s]#015Downloading:  62%|██████▏   | 275M/445M [00:07<00:04, 42.0MB/s]#015Downloading:  63%|██████▎   | 280M/445M [00:07<00:04, 39.9MB/s]#015Downloading:  64%|██████▍   | 285M/445M [00:07<00:04, 35.9MB/s]#015Downloading:  66%|██████▌   | 292M/445M [00:07<00:03, 42.3MB/s]#015Downloading:  67%|██████▋   | 296M/445M [00:07<00:04, 36.4MB/s]#015Downloading:  68%|██████▊   | 302M/445M [00:07<00:03, 39.9MB/s]#015Downloading:  69%|██████▉   | 306M/445M [00:08<00:03, 40.4MB/s]#015Downloading:  70%|██████▉   | 311M/445M [00:08<00:04, 31.7MB/s]#015Downloading:  71%|███████   | 317M/445M [00:08<00:03, 38.3MB/s]#015Downloading:  72%|███████▏  | 322M/445M [00:08<00:02, 41.1MB/s]#015Downloading:  74%|███████▎  | 327M/445M [00:08<00:03, 37.2MB/s]#015Downloading:  75%|███████▌  | 334M/445M [00:08<00:02, 43.9MB/s]#015Downloading:  76%|███████▌  | 339M/445M [00:08<00:02, 41.1MB/s]#015Downloading:  78%|███████▊  | 345M/445M [00:08<00:02, 46.6MB/s]#015Downloading:  79%|███████▉  | 352M/445M [00:09<00:01, 52.8MB/s]#015Downloading:  80%|████████  | 358M/445M [00:09<00:01, 51.9MB/s]#015Downloading:  82%|████████▏ | 363M/445M [00:09<00:01, 41.2MB/s]#015Downloading:  83%|████████▎ | 368M/445M [00:09<00:01, 43.4MB/s]#015Downloading:  84%|████████▍ | 373M/445M [00:09<00:01, 40.9MB/s]#015Downloading:  85%|████████▍ | 377M/445M [00:09<00:01, 39.1MB/s]#015Downloading:  86%|████████▌ | 381M/445M [00:09<00:01, 37.8MB/s]#015Downloading:  87%|████████▋ | 385M/445M [00:09<00:01, 35.6MB/s]#015Downloading:  87%|████████▋ | 389M/445M [00:10<00:01, 35.0MB/s]#015Downloading:  88%|████████▊ | 393M/445M [00:10<00:01, 33.2MB/s]#015Downloading:  89%|████████▉ | 396M/445M [00:10<00:02, 23.4MB/s]#015Downloading:  90%|█████████ | 402M/445M [00:10<00:01, 27.7MB/s]#015Downloading:  91%|█████████ | 405M/445M [00:10<00:01, 25.6MB/s]#015Downloading:  92%|█████████▏| 410M/445M [00:10<00:01, 30.4MB/s]#015Downloading:  93%|█████████▎| 413M/445M [00:11<00:01, 27.4MB/s]#015Downloading:  93%|█████████▎| 416M/445M [00:11<00:01, 24.5MB/s]#015Downloading:  94%|█████████▍| 419M/445M [00:11<00:01, 22.7MB/s]#015Downloading:  96%|█████████▌| 426M/445M [00:11<00:00, 31.5MB/s]#015Downloading:  97%|█████████▋| 429M/445M [00:11<00:00, 29.0MB/s]#015Downloading:  98%|█████████▊| 436M/445M [00:11<00:00, 33.8MB/s]#015Downloading:  99%|█████████▉| 440M/445M [00:12<00:00, 25.1MB/s]#015Downloading:  99%|█████████▉| 443M/445M [00:12<00:00, 23.7MB/s]#015Downloading: 100%|██████████| 445M/445M [00:12<00:00, 36.5MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2021-10-12 05:16:08,161 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2021-10-12 05:16:08,161 >> creating metadata file for /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1279] 2021-10-12 05:16:08,162 >> loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1515] 2021-10-12 05:16:09,877 >> Some weights of the model checkpoint at dbmdz/bert-base-turkish-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1526] 2021-10-12 05:16:09,877 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on train dataset:   0%|          | 0/9 [00:00<?, ?ba/s]#015Running tokenizer on train dataset:  11%|█         | 1/9 [00:00<00:05,  1.51ba/s]#015Running tokenizer on train dataset:  22%|██▏       | 2/9 [00:01<00:03,  1.78ba/s]#015Running tokenizer on train dataset:  33%|███▎      | 3/9 [00:01<00:03,  1.81ba/s]#015Running tokenizer on train dataset:  44%|████▍     | 4/9 [00:02<00:02,  2.05ba/s]#015Running tokenizer on train dataset:  56%|█████▌    | 5/9 [00:02<00:01,  2.28ba/s]#015Running tokenizer on train dataset:  67%|██████▋   | 6/9 [00:02<00:01,  2.63ba/s]#015Running tokenizer on train dataset:  78%|███████▊  | 7/9 [00:03<00:00,  2.61ba/s]#015Running tokenizer on train dataset:  89%|████████▉ | 8/9 [00:03<00:00,  2.56ba/s]#015Running tokenizer on train dataset: 100%|██████████| 9/9 [00:03<00:00,  3.20ba/s]#015Running tokenizer on train dataset: 100%|██████████| 9/9 [00:03<00:00,  2.47ba/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s]#015Downloading: 4.51kB [00:00, 3.98MB/s]                   \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]#015Downloading: 3.31kB [00:00, 3.29MB/s]                   \u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:414] 2021-10-12 05:16:17,451 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1168] 2021-10-12 05:16:17,463 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1169] 2021-10-12 05:16:17,463 >>   Num examples = 11466\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1170] 2021-10-12 05:16:17,463 >>   Num Epochs = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2021-10-12 05:16:17,463 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2021-10-12 05:16:17,463 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2021-10-12 05:16:17,463 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2021-10-12 05:16:17,463 >>   Total optimization steps = 718\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/718 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m#015  0%|          | 1/718 [00:21<4:14:15, 21.28s/it]#015  0%|          | 2/718 [00:21<1:47:45,  9.03s/it]#015  0%|          | 3/718 [00:22<1:00:26,  5.07s/it]#015  1%|          | 4/718 [00:22<38:13,  3.21s/it]  #015  1%|          | 5/718 [00:22<25:58,  2.19s/it]#015  1%|          | 6/718 [00:23<18:35,  1.57s/it]#015  1%|          | 7/718 [00:23<14:07,  1.19s/it]#015  1%|          | 8/718 [00:23<10:59,  1.08it/s]#015  1%|▏         | 9/718 [00:24<08:53,  1.33it/s]#015  1%|▏         | 10/718 [00:24<07:27,  1.58it/s]#015  2%|▏         | 11/718 [00:25<06:28,  1.82it/s]#015  2%|▏         | 12/718 [00:25<05:59,  1.97it/s]#015  2%|▏         | 13/718 [00:25<05:27,  2.15it/s]#015  2%|▏         | 14/718 [00:26<05:06,  2.30it/s]#015  2%|▏         | 15/718 [00:26<04:49,  2.43it/s]#015  2%|▏         | 16/718 [00:26<04:37,  2.53it/s]#015  2%|▏         | 17/718 [00:27<04:43,  2.47it/s]#015  3%|▎         | 18/718 [00:27<04:34,  2.55it/s]#015  3%|▎         | 19/718 [00:28<04:28,  2.60it/s]#015  3%|▎         | 20/718 [00:28<04:22,  2.66it/s]#015  3%|▎         | 21/718 [00:28<04:18,  2.69it/s]#015  3%|▎         | 22/718 [00:29<04:31,  2.57it/s]#015  3%|▎         | 23/718 [00:29<04:24,  2.63it/s]#015  3%|▎         | 24/718 [00:29<04:20,  2.66it/s]#015  3%|▎         | 25/718 [00:30<04:17,  2.69it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m#015  4%|▎         | 26/718 [00:30<04:11,  2.75it/s]#015  4%|▍         | 27/718 [00:31<04:16,  2.69it/s]#015  4%|▍         | 28/718 [00:31<04:09,  2.76it/s]#015  4%|▍         | 29/718 [00:31<04:07,  2.78it/s]#015  4%|▍         | 30/718 [00:32<04:06,  2.79it/s]#015  4%|▍         | 31/718 [00:32<04:05,  2.80it/s]#015  4%|▍         | 32/718 [00:32<04:20,  2.63it/s]#015  5%|▍         | 33/718 [00:33<04:15,  2.68it/s]#015  5%|▍         | 34/718 [00:33<04:11,  2.72it/s]#015  5%|▍         | 35/718 [00:33<04:09,  2.74it/s]#015  5%|▌         | 36/718 [00:34<04:07,  2.76it/s]#015  5%|▌         | 37/718 [00:34<04:05,  2.77it/s]#015  5%|▌         | 38/718 [00:35<04:15,  2.66it/s]#015  5%|▌         | 39/718 [00:35<04:11,  2.70it/s]#015  6%|▌         | 40/718 [00:35<04:09,  2.72it/s]#015  6%|▌         | 41/718 [00:36<04:06,  2.74it/s]#015  6%|▌         | 42/718 [00:36<04:04,  2.76it/s]#015  6%|▌         | 43/718 [00:36<04:12,  2.67it/s]#015  6%|▌         | 44/718 [00:37<04:07,  2.73it/s]#015  6%|▋         | 45/718 [00:37<04:05,  2.75it/s]#015  6%|▋         | 46/718 [00:37<04:02,  2.77it/s]#015  7%|▋         | 47/718 [00:38<04:02,  2.77it/s]#015  7%|▋         | 48/718 [00:38<04:10,  2.67it/s]#015  7%|▋         | 49/718 [00:39<04:07,  2.70it/s]#015  7%|▋         | 50/718 [00:39<04:04,  2.73it/s]#015  7%|▋         | 51/718 [00:39<04:01,  2.76it/s]#015  7%|▋         | 52/718 [00:40<04:01,  2.75it/s]#015  7%|▋         | 53/718 [00:40<04:13,  2.63it/s]#015  8%|▊         | 54/718 [00:40<04:12,  2.63it/s]#015  8%|▊         | 55/718 [00:41<04:09,  2.65it/s]#015  8%|▊         | 56/718 [00:41<04:08,  2.67it/s]#015  8%|▊         | 57/718 [00:42<04:06,  2.68it/s]#015  8%|▊         | 58/718 [00:42<04:15,  2.58it/s]#015  8%|▊         | 59/718 [00:42<04:11,  2.62it/s]#015  8%|▊         | 60/718 [00:43<04:08,  2.64it/s]#015  8%|▊         | 61/718 [00:43<04:06,  2.67it/s]#015  9%|▊         | 62/718 [00:43<04:04,  2.69it/s]#015  9%|▉         | 63/718 [00:44<04:13,  2.58it/s]#015  9%|▉         | 64/718 [00:44<04:12,  2.59it/s]#015  9%|▉         | 65/718 [00:45<04:10,  2.61it/s]#015  9%|▉         | 66/718 [00:45<04:07,  2.64it/s]#015  9%|▉         | 67/718 [00:45<04:04,  2.66it/s]#015  9%|▉         | 68/718 [00:46<04:14,  2.55it/s]#015 10%|▉         | 69/718 [00:46<04:09,  2.60it/s]#015 10%|▉         | 70/718 [00:47<04:05,  2.64it/s]#015 10%|▉         | 71/718 [00:47<04:03,  2.66it/s]#015 10%|█         | 72/718 [00:47<03:58,  2.71it/s]#015 10%|█         | 73/718 [00:48<04:05,  2.63it/s]#015 10%|█         | 74/718 [00:48<04:01,  2.67it/s]#015 10%|█         | 75/718 [00:48<04:02,  2.65it/s]#015 11%|█         | 76/718 [00:49<04:02,  2.64it/s]#015 11%|█         | 77/718 [00:49<04:01,  2.65it/s]#015 11%|█         | 78/718 [00:50<04:06,  2.59it/s]#015 11%|█         | 79/718 [00:50<04:00,  2.65it/s]#015 11%|█         | 80/718 [00:50<03:56,  2.70it/s]#015 11%|█▏        | 81/718 [00:51<03:53,  2.73it/s]#015 11%|█▏        | 82/718 [00:51<03:51,  2.75it/s]#015 12%|█▏        | 83/718 [00:51<04:03,  2.60it/s]#015 12%|█▏        | 84/718 [00:52<03:58,  2.66it/s]#015 12%|█▏        | 85/718 [00:52<03:54,  2.70it/s]#015 12%|█▏        | 86/718 [00:53<03:51,  2.73it/s]#015 12%|█▏        | 87/718 [00:53<03:49,  2.75it/s]#015 12%|█▏        | 88/718 [00:53<04:04,  2.58it/s]#015 12%|█▏        | 89/718 [00:54<04:00,  2.62it/s]#015 13%|█▎        | 90/718 [00:54<03:56,  2.65it/s]#015 13%|█▎        | 91/718 [00:54<03:53,  2.69it/s]#015 13%|█▎        | 92/718 [00:55<03:51,  2.71it/s]#015 13%|█▎        | 93/718 [00:55<04:00,  2.60it/s]#015 13%|█▎        | 94/718 [00:56<03:56,  2.63it/s]#015 13%|█▎        | 95/718 [00:56<03:53,  2.66it/s]#015 13%|█▎        | 96/718 [00:56<03:51,  2.69it/s]#015 14%|█▎        | 97/718 [00:57<03:50,  2.69it/s]#015 14%|█▎        | 98/718 [00:57<03:49,  2.70it/s]#015 14%|█▍        | 99/718 [00:57<03:57,  2.60it/s]#015 14%|█▍        | 100/718 [00:58<03:53,  2.64it/s]#015 14%|█▍        | 101/718 [00:58<03:49,  2.69it/s]#015 14%|█▍        | 102/718 [00:59<03:46,  2.72it/s]#015 14%|█▍        | 103/718 [00:59<03:43,  2.76it/s]#015 14%|█▍        | 104/718 [00:59<03:51,  2.66it/s]#015 15%|█▍        | 105/718 [01:00<03:46,  2.70it/s]#015 15%|█▍        | 106/718 [01:00<03:44,  2.73it/s]#015 15%|█▍        | 107/718 [01:00<03:42,  2.75it/s]#015 15%|█▌        | 108/718 [01:01<03:41,  2.76it/s]#015 15%|█▌        | 109/718 [01:01<03:49,  2.66it/s]#015 15%|█▌        | 110/718 [01:01<03:44,  2.71it/s]#015 15%|█▌        | 111/718 [01:02<03:42,  2.73it/s]#015 16%|█▌        | 112/718 [01:02<03:45,  2.68it/s]#015 16%|█▌        | 113/718 [01:03<03:43,  2.71it/s]#015 16%|█▌        | 114/718 [01:03<03:51,  2.61it/s]#015 16%|█▌        | 115/718 [01:03<03:46,  2.66it/s]#015 16%|█▌        | 116/718 [01:04<03:43,  2.69it/s]#015 16%|█▋        | 117/718 [01:04<03:41,  2.71it/s]#015 16%|█▋        | 118/718 [01:04<03:42,  2.69it/s]#015 17%|█▋        | 119/718 [01:05<03:50,  2.60it/s]#015 17%|█▋        | 120/718 [01:05<03:47,  2.63it/s]#015 17%|█▋        | 121/718 [01:06<03:44,  2.66it/s]#015 17%|█▋        | 122/718 [01:06<03:40,  2.70it/s]#015 17%|█▋        | 123/718 [01:06<03:39,  2.71it/s]#015 17%|█▋        | 124/718 [01:07<03:46,  2.62it/s]#015 17%|█▋        | 125/718 [01:07<03:40,  2.68it/s]#015 18%|█▊        | 126/718 [01:07<03:41,  2.68it/s]#015 18%|█▊        | 127/718 [01:08<03:37,  2.72it/s]#015 18%|█▊        | 128/718 [01:08<03:40,  2.68it/s]#015 18%|█▊        | 129/718 [01:09<03:46,  2.60it/s]#015 18%|█▊        | 130/718 [01:09<03:44,  2.61it/s]#015 18%|█▊        | 131/718 [01:09<03:43,  2.63it/s]#015 18%|█▊        | 132/718 [01:10<03:38,  2.68it/s]#015 19%|█▊        | 133/718 [01:10<03:37,  2.69it/s]#015 19%|█▊        | 134/718 [01:11<03:49,  2.55it/s]#015 19%|█▉        | 135/718 [01:11<03:42,  2.62it/s]#015 19%|█▉        | 136/718 [01:11<03:37,  2.68it/s]#015 19%|█▉        | 137/718 [01:12<03:35,  2.69it/s]#015 19%|█▉        | 138/718 [01:12<03:32,  2.73it/s]#015 19%|█▉        | 139/718 [01:12<03:42,  2.61it/s]#015 19%|█▉        | 140/718 [01:13<03:42,  2.60it/s]#015 20%|█▉        | 141/718 [01:13<03:38,  2.64it/s]#015 20%|█▉        | 142/718 [01:14<03:34,  2.69it/s]#015 20%|█▉        | 143/718 [01:14<03:29,  2.74it/s]#015 20%|██        | 144/718 [01:14<03:38,  2.63it/s]#015 20%|██        | 145/718 [01:15<03:38,  2.62it/s]#015 20%|██        | 146/718 [01:15<03:38,  2.62it/s]#015 20%|██        | 147/718 [01:15<03:34,  2.66it/s]#015 21%|██        | 148/718 [01:16<03:30,  2.71it/s]#015 21%|██        | 149/718 [01:16<03:38,  2.61it/s]#015 21%|██        | 150/718 [01:17<03:33,  2.67it/s]#015 21%|██        | 151/718 [01:17<03:28,  2.72it/s]#015 21%|██        | 152/718 [01:17<03:27,  2.73it/s]#015 21%|██▏       | 153/718 [01:18<03:27,  2.73it/s]#015 21%|██▏       | 154/718 [01:18<03:38,  2.58it/s]#015 22%|██▏       | 155/718 [01:18<03:32,  2.65it/s]#015 22%|██▏       | 156/718 [01:19<03:29,  2.68it/s]#015 22%|██▏       | 157/718 [01:19<03:29,  2.68it/s]#015 22%|██▏       | 158/718 [01:19<03:25,  2.73it/s]#015 22%|██▏       | 159/718 [01:20<03:22,  2.76it/s]#015 22%|██▏       | 160/718 [01:20<03:32,  2.63it/s]#015 22%|██▏       | 161/718 [01:21<03:27,  2.68it/s]#015 23%|██▎       | 162/718 [01:21<03:23,  2.73it/s]#015 23%|██▎       | 163/718 [01:21<03:21,  2.76it/s]#015 23%|██▎       | 164/718 [01:22<03:23,  2.72it/s]#015 23%|██▎       | 165/718 [01:22<03:29,  2.63it/s]#015 23%|██▎       | 166/718 [01:22<03:25,  2.68it/s]#015 23%|██▎       | 167/718 [01:23<03:22,  2.72it/s]#015 23%|██▎       | 168/718 [01:23<03:19,  2.75it/s]#015 24%|██▎       | 169/718 [01:24<03:22,  2.72it/s]#015 24%|██▎       | 170/718 [01:24<03:28,  2.63it/s]#015 24%|██▍       | 171/718 [01:24<03:25,  2.67it/s]#015 24%|██▍       | 172/718 [01:25<03:20,  2.72it/s]#015 24%|██▍       | 173/718 [01:25<03:20,  2.72it/s]#015 24%|██▍       | 174/718 [01:25<03:23,  2.68it/s]#015 24%|██▍       | 175/718 [01:26<03:29,  2.60it/s]#015 25%|██▍       | 176/718 [01:26<03:24,  2.65it/s]#015 25%|██▍       | 177/718 [01:27<03:19,  2.71it/s]#015 25%|██▍       | 178/718 [01:27<03:16,  2.75it/s]#015 25%|██▍       | 179/718 [01:27<03:15,  2.76it/s]#015 25%|██▌       | 180/718 [01:28<03:22,  2.65it/s]#015 25%|██▌       | 181/718 [01:28<03:18,  2.71it/s]#015 25%|██▌       | 182/718 [01:28<03:16,  2.73it/s]#015 25%|██▌       | 183/718 [01:29<03:15,  2.74it/s]#015 26%|██▌       | 184/718 [01:29<03:13,  2.76it/s]#015 26%|██▌       | 185/718 [01:30<03:21,  2.65it/s]#015 26%|██▌       | 186/718 [01:30<03:16,  2.71it/s]#015 26%|██▌       | 187/718 [01:30<03:13,  2.74it/s]#015 26%|██▌       | 188/718 [01:31<03:11,  2.77it/s]#015 26%|██▋       | 189/718 [01:31<03:09,  2.79it/s]#015 26%|██▋       | 190/718 [01:31<03:18,  2.66it/s]#015 27%|██▋       | 191/718 [01:32<03:14,  2.70it/s]#015 27%|██▋       | 192/718 [01:32<03:12,  2.73it/s]#015 27%|██▋       | 193/718 [01:32<03:14,  2.70it/s]#015 27%|██▋       | 194/718 [01:33<03:11,  2.74it/s]#015 27%|██▋       | 195/718 [01:33<03:21,  2.60it/s]#015 27%|██▋       | 196/718 [01:34<03:16,  2.66it/s]#015 27%|██▋       | 197/718 [01:34<03:12,  2.71it/s]#015 28%|██▊       | 198/718 [01:34<03:10,  2.73it/s]#015 28%|██▊       | 199/718 [01:35<03:07,  2.77it/s]#015 28%|██▊       | 200/718 [01:35<03:17,  2.62it/s]#015 28%|██▊       | 201/718 [01:35<03:11,  2.69it/s]#015 28%|██▊       | 202/718 [01:36<03:09,  2.72it/s]#015 28%|██▊       | 203/718 [01:36<03:07,  2.74it/s]#015 28%|██▊       | 204/718 [01:36<03:04,  2.78it/s]#015 29%|██▊       | 205/718 [01:37<03:12,  2.66it/s]#015 29%|██▊       | 206/718 [01:37<03:08,  2.71it/s]#015 29%|██▉       | 207/718 [01:38<03:06,  2.75it/s]#015 29%|██▉       | 208/718 [01:38<03:04,  2.76it/s]#015 29%|██▉       | 209/718 [01:38<03:03,  2.78it/s]#015 29%|██▉       | 210/718 [01:39<03:10,  2.67it/s]#015 29%|██▉       | 211/718 [01:39<03:07,  2.71it/s]#015 30%|██▉       | 212/718 [01:39<03:04,  2.74it/s]#015 30%|██▉       | 213/718 [01:40<03:02,  2.76it/s]#015 30%|██▉       | 214/718 [01:40<03:01,  2.78it/s]#015 30%|██▉       | 215/718 [01:41<03:09,  2.65it/s]#015 30%|███       | 216/718 [01:41<03:06,  2.69it/s]#015 30%|███       | 217/718 [01:41<03:03,  2.73it/s]#015 30%|███       | 218/718 [01:42<03:02,  2.74it/s]#015 31%|███       | 219/718 [01:42<03:00,  2.76it/s]#015 31%|███       | 220/718 [01:42<02:59,  2.78it/s]#015 31%|███       | 221/718 [01:43<03:09,  2.62it/s]#015 31%|███       | 222/718 [01:43<03:05,  2.67it/s]#015 31%|███       | 223/718 [01:43<03:02,  2.72it/s]#015 31%|███       | 224/718 [01:44<02:58,  2.76it/s]#015 31%|███▏      | 225/718 [01:44<02:57,  2.78it/s]#015 31%|███▏      | 226/718 [01:45<03:12,  2.55it/s]#015 32%|███▏      | 227/718 [01:45<03:09,  2.59it/s]#015 32%|███▏      | 228/718 [01:45<03:06,  2.63it/s]#015 32%|███▏      | 229/718 [01:46<03:03,  2.66it/s]#015 32%|███▏      | 230/718 [01:46<03:01,  2.69it/s]#015 32%|███▏      | 231/718 [01:47<03:13,  2.51it/s]#015 32%|███▏      | 232/718 [01:47<03:08,  2.57it/s]#015 32%|███▏      | 233/718 [01:47<03:05,  2.61it/s]#015 33%|███▎      | 234/718 [01:48<03:05,  2.61it/s]#015 33%|███▎      | 235/718 [01:48<03:07,  2.57it/s]#015 33%|███▎      | 236/718 [01:49<03:14,  2.48it/s]#015 33%|███▎      | 237/718 [01:49<03:08,  2.55it/s]#015 33%|███▎      | 238/718 [01:49<03:06,  2.57it/s]#015 33%|███▎      | 239/718 [01:50<03:05,  2.58it/s]#015 33%|███▎      | 240/718 [01:50<03:02,  2.62it/s]#015 34%|███▎      | 241/718 [01:50<03:10,  2.51it/s]#015 34%|███▎      | 242/718 [01:51<03:05,  2.56it/s]#015 34%|███▍      | 243/718 [01:51<03:01,  2.61it/s]#015 34%|███▍      | 244/718 [01:52<02:59,  2.63it/s]#015 34%|███▍      | 245/718 [01:52<02:58,  2.65it/s]#015 34%|███▍      | 246/718 [01:52<03:07,  2.52it/s]#015 34%|███▍      | 247/718 [01:53<03:06,  2.53it/s]#015 35%|███▍      | 248/718 [01:53<03:04,  2.55it/s]#015 35%|███▍      | 249/718 [01:54<03:01,  2.59it/s]#015 35%|███▍      | 250/718 [01:54<02:58,  2.62it/s]#015 35%|███▍      | 251/718 [01:54<03:06,  2.51it/s]#015 35%|███▌      | 252/718 [01:55<03:04,  2.52it/s]#015 35%|███▌      | 253/718 [01:55<03:01,  2.57it/s]#015 35%|███▌      | 254/718 [01:56<02:58,  2.60it/s]#015 36%|███▌      | 255/718 [01:56<02:56,  2.62it/s]#015 36%|███▌      | 256/718 [01:56<03:04,  2.51it/s]#015 36%|███▌      | 257/718 [01:57<03:00,  2.55it/s]#015 36%|███▌      | 258/718 [01:57<02:57,  2.59it/s]#015 36%|███▌      | 259/718 [01:57<02:55,  2.62it/s]#015 36%|███▌      | 260/718 [01:58<02:52,  2.66it/s]#015 36%|███▋      | 261/718 [01:58<03:01,  2.52it/s]#015 36%|███▋      | 262/718 [01:59<02:57,  2.57it/s]#015 37%|███▋      | 263/718 [01:59<02:53,  2.62it/s]#015 37%|███▋      | 264/718 [01:59<02:51,  2.65it/s]#015 37%|███▋      | 265/718 [02:00<02:49,  2.67it/s]#015 37%|███▋      | 266/718 [02:00<02:57,  2.55it/s]#015 37%|███▋      | 267/718 [02:01<02:53,  2.60it/s]#015 37%|███▋      | 268/718 [02:01<02:50,  2.64it/s]#015 37%|███▋      | 269/718 [02:01<02:48,  2.66it/s]#015 38%|███▊      | 270/718 [02:02<02:47,  2.68it/s]#015 38%|███▊      | 271/718 [02:02<02:54,  2.56it/s]#015 38%|███▊      | 272/718 [02:02<02:51,  2.60it/s]#015 38%|███▊      | 273/718 [02:03<02:49,  2.63it/s]#015 38%|███▊      | 274/718 [02:03<02:47,  2.64it/s]#015 38%|███▊      | 275/718 [02:04<02:46,  2.66it/s]#015 38%|███▊      | 276/718 [02:04<02:53,  2.55it/s]#015 39%|███▊      | 277/718 [02:04<02:51,  2.57it/s]#015 39%|███▊      | 278/718 [02:05<02:48,  2.61it/s]#015 39%|███▉      | 279/718 [02:05<02:45,  2.65it/s]#015 39%|███▉      | 280/718 [02:05<02:43,  2.68it/s]#015 39%|███▉      | 281/718 [02:06<02:42,  2.69it/s]#015 39%|███▉      | 282/718 [02:06<02:49,  2.57it/s]#015 39%|███▉      | 283/718 [02:07<02:46,  2.61it/s]#015 40%|███▉      | 284/718 [02:07<02:42,  2.67it/s]#015 40%|███▉      | 285/718 [02:07<02:41,  2.69it/s]#015 40%|███▉      | 286/718 [02:08<02:39,  2.71it/s]#015 40%|███▉      | 287/718 [02:08<02:46,  2.58it/s]#015 40%|████      | 288/718 [02:09<02:46,  2.58it/s]#015 40%|████      | 289/718 [02:09<02:43,  2.63it/s]#015 40%|████      | 290/718 [02:09<02:40,  2.66it/s]#015 41%|████      | 291/718 [02:10<02:38,  2.69it/s]#015 41%|████      | 292/718 [02:10<02:46,  2.55it/s]#015 41%|████      | 293/718 [02:10<02:42,  2.61it/s]#015 41%|████      | 294/718 [02:11<02:40,  2.64it/s]#015 41%|████      | 295/718 [02:11<02:38,  2.67it/s]#015 41%|████      | 296/718 [02:12<02:36,  2.69it/s]#015 41%|████▏     | 297/718 [02:12<02:46,  2.53it/s]#015 42%|████▏     | 298/718 [02:12<02:44,  2.56it/s]#015 42%|████▏     | 299/718 [02:13<02:40,  2.61it/s]#015 42%|████▏     | 300/718 [02:13<02:38,  2.63it/s]#015 42%|████▏     | 301/718 [02:13<02:36,  2.67it/s]#015 42%|████▏     | 302/718 [02:14<02:44,  2.53it/s]#015 42%|████▏     | 303/718 [02:14<02:39,  2.60it/s]#015 42%|████▏     | 304/718 [02:15<02:36,  2.65it/s]#015 42%|████▏     | 305/718 [02:15<02:34,  2.68it/s]#015 43%|████▎     | 306/718 [02:15<02:32,  2.70it/s]#015 43%|████▎     | 307/718 [02:16<02:40,  2.57it/s]#015 43%|████▎     | 308/718 [02:16<02:36,  2.62it/s]#015 43%|████▎     | 309/718 [02:17<02:34,  2.65it/s]#015 43%|████▎     | 310/718 [02:17<02:32,  2.68it/s]#015 43%|████▎     | 311/718 [02:17<02:31,  2.70it/s]#015 43%|████▎     | 312/718 [02:18<02:38,  2.56it/s]#015 44%|████▎     | 313/718 [02:18<02:37,  2.56it/s]#015 44%|████▎     | 314/718 [02:18<02:35,  2.61it/s]#015 44%|████▍     | 315/718 [02:19<02:32,  2.64it/s]#015 44%|████▍     | 316/718 [02:19<02:30,  2.68it/s]#015 44%|████▍     | 317/718 [02:20<02:37,  2.55it/s]#015 44%|████▍     | 318/718 [02:20<02:33,  2.60it/s]#015 44%|████▍     | 319/718 [02:20<02:34,  2.59it/s]#015 45%|████▍     | 320/718 [02:21<02:30,  2.64it/s]#015 45%|████▍     | 321/718 [02:21<02:31,  2.62it/s]#015 \u001b[0m\n",
      "\u001b[34m45%|████▍     | 322/718 [02:22<02:39,  2.49it/s]#015 45%|████▍     | 323/718 [02:22<02:34,  2.55it/s]#015 45%|████▌     | 324/718 [02:22<02:31,  2.60it/s]#015 45%|████▌     | 325/718 [02:23<02:29,  2.64it/s]#015 45%|████▌     | 326/718 [02:23<02:26,  2.67it/s]#015 46%|████▌     | 327/718 [02:23<02:35,  2.51it/s]#015 46%|████▌     | 328/718 [02:24<02:30,  2.59it/s]#015 46%|████▌     | 329/718 [02:24<02:27,  2.63it/s]#015 46%|████▌     | 330/718 [02:25<02:27,  2.63it/s]#015 46%|████▌     | 331/718 [02:25<02:25,  2.67it/s]#015 46%|████▌     | 332/718 [02:25<02:29,  2.58it/s]#015 46%|████▋     | 333/718 [02:26<02:25,  2.65it/s]#015 47%|████▋     | 334/718 [02:26<02:22,  2.69it/s]#015 47%|████▋     | 335/718 [02:26<02:24,  2.66it/s]#015 47%|████▋     | 336/718 [02:27<02:21,  2.70it/s]#015 47%|████▋     | 337/718 [02:27<02:31,  2.51it/s]#015 47%|████▋     | 338/718 [02:28<02:27,  2.58it/s]#015 47%|████▋     | 339/718 [02:28<02:23,  2.64it/s]#015 47%|████▋     | 340/718 [02:28<02:21,  2.68it/s]#015 47%|████▋     | 341/718 [02:29<02:18,  2.72it/s]#015 48%|████▊     | 342/718 [02:29<02:16,  2.75it/s]#015 48%|████▊     | 343/718 [02:29<02:22,  2.63it/s]#015 48%|████▊     | 344/718 [02:30<02:20,  2.67it/s]#015 48%|████▊     | 345/718 [02:30<02:19,  2.68it/s]#015 48%|████▊     | 346/718 [02:31<02:18,  2.69it/s]#015 48%|████▊     | 347/718 [02:31<02:19,  2.67it/s]#015 48%|████▊     | 348/718 [02:31<02:26,  2.53it/s]#015 49%|████▊     | 349/718 [02:32<02:22,  2.58it/s]#015 49%|████▊     | 350/718 [02:32<02:20,  2.62it/s]#015 49%|████▉     | 351/718 [02:33<02:20,  2.62it/s]#015 49%|████▉     | 352/718 [02:33<02:18,  2.64it/s]#015 49%|████▉     | 353/718 [02:33<02:23,  2.54it/s]#015 49%|████▉     | 354/718 [02:34<02:20,  2.58it/s]#015 49%|████▉     | 355/718 [02:34<02:19,  2.61it/s]#015 50%|████▉     | 356/718 [02:34<02:17,  2.64it/s]#015 50%|████▉     | 357/718 [02:35<02:15,  2.66it/s]#015 50%|████▉     | 358/718 [02:35<02:20,  2.56it/s]#015 50%|█████     | 359/718 [02:36<02:46,  2.15it/s]#015 50%|█████     | 360/718 [02:36<02:36,  2.29it/s]#015 50%|█████     | 361/718 [02:37<02:31,  2.35it/s]#015 50%|█████     | 362/718 [02:37<02:25,  2.45it/s]#015 51%|█████     | 363/718 [02:37<02:29,  2.38it/s]#015 51%|█████     | 364/718 [02:38<02:26,  2.42it/s]#015 51%|█████     | 365/718 [02:38<02:20,  2.52it/s]#015 51%|█████     | 366/718 [02:39<02:15,  2.59it/s]#015 51%|█████     | 367/718 [02:39<02:12,  2.64it/s]#015 51%|█████▏    | 368/718 [02:39<02:16,  2.57it/s]#015 51%|█████▏    | 369/718 [02:40<02:13,  2.61it/s]#015 52%|█████▏    | 370/718 [02:40<02:10,  2.66it/s]#015 52%|█████▏    | 371/718 [02:40<02:09,  2.68it/s]#015 52%|█████▏    | 372/718 [02:41<02:10,  2.65it/s]#015 52%|█████▏    | 373/718 [02:41<02:15,  2.55it/s]#015 52%|█████▏    | 374/718 [02:42<02:11,  2.62it/s]#015 52%|█████▏    | 375/718 [02:42<02:08,  2.67it/s]#015 52%|█████▏    | 376/718 [02:42<02:06,  2.70it/s]#015 53%|█████▎    | 377/718 [02:43<02:05,  2.71it/s]#015 53%|█████▎    | 378/718 [02:43<02:05,  2.70it/s]#015 53%|█████▎    | 379/718 [02:44<02:12,  2.56it/s]#015 53%|█████▎    | 380/718 [02:44<02:10,  2.59it/s]#015 53%|█████▎    | 381/718 [02:44<02:08,  2.62it/s]#015 53%|█████▎    | 382/718 [02:45<02:07,  2.64it/s]#015 53%|█████▎    | 383/718 [02:45<02:05,  2.67it/s]#015 53%|█████▎    | 384/718 [02:45<02:10,  2.55it/s]#015 54%|█████▎    | 385/718 [02:46<02:08,  2.59it/s]#015 54%|█████▍    | 386/718 [02:46<02:08,  2.58it/s]#015 54%|█████▍    | 387/718 [02:47<02:08,  2.58it/s]#015 54%|█████▍    | 388/718 [02:47<02:07,  2.59it/s]#015 54%|█████▍    | 389/718 [02:47<02:11,  2.50it/s]#015 54%|█████▍    | 390/718 [02:48<02:08,  2.56it/s]#015 54%|█████▍    | 391/718 [02:48<02:05,  2.60it/s]#015 55%|█████▍    | 392/718 [02:49<02:04,  2.62it/s]#015 55%|█████▍    | 393/718 [02:49<02:01,  2.67it/s]#015 55%|█████▍    | 394/718 [02:49<02:05,  2.59it/s]#015 55%|█████▌    | 395/718 [02:50<02:02,  2.64it/s]#015 55%|█████▌    | 396/718 [02:50<02:02,  2.63it/s]#015 55%|█████▌    | 397/718 [02:50<01:59,  2.68it/s]#015 55%|█████▌    | 398/718 [02:51<02:00,  2.65it/s]#015 56%|█████▌    | 399/718 [02:51<02:03,  2.57it/s]#015 56%|█████▌    | 400/718 [02:52<02:01,  2.61it/s]#015 56%|█████▌    | 401/718 [02:52<01:58,  2.67it/s]#015 56%|█████▌    | 402/718 [02:52<01:56,  2.71it/s]#015 56%|█████▌    | 403/718 [02:53<01:54,  2.75it/s]#015 56%|█████▋    | 404/718 [02:53<02:00,  2.61it/s]#015 56%|█████▋    | 405/718 [02:53<01:57,  2.66it/s]#015 57%|█████▋    | 406/718 [02:54<01:55,  2.70it/s]#015 57%|█████▋    | 407/718 [02:54<01:54,  2.72it/s]#015 57%|█████▋    | 408/718 [02:54<01:52,  2.74it/s]#015 57%|█████▋    | 409/718 [02:55<01:57,  2.63it/s]#015 57%|█████▋    | 410/718 [02:55<01:55,  2.68it/s]#015 57%|█████▋    | 411/718 [02:56<01:53,  2.70it/s]#015 57%|█████▋    | 412/718 [02:56<01:54,  2.66it/s]#015 58%|█████▊    | 413/718 [02:56<01:55,  2.65it/s]#015 58%|█████▊    | 414/718 [02:57<01:57,  2.58it/s]#015 58%|█████▊    | 415/718 [02:57<01:54,  2.65it/s]#015 58%|█████▊    | 416/718 [02:58<01:53,  2.67it/s]#015 58%|█████▊    | 417/718 [02:58<01:50,  2.72it/s]#015 58%|█████▊    | 418/718 [02:58<01:49,  2.74it/s]#015 58%|█████▊    | 419/718 [02:59<01:54,  2.61it/s]#015 58%|█████▊    | 420/718 [02:59<01:51,  2.67it/s]#015 59%|█████▊    | 421/718 [02:59<01:49,  2.72it/s]#015 59%|█████▉    | 422/718 [03:00<01:49,  2.70it/s]#015 59%|█████▉    | 423/718 [03:00<01:50,  2.68it/s]#015 59%|█████▉    | 424/718 [03:01<01:53,  2.59it/s]#015 59%|█████▉    | 425/718 [03:01<01:51,  2.63it/s]#015 59%|█████▉    | 426/718 [03:01<01:51,  2.61it/s]#015 59%|█████▉    | 427/718 [03:02<01:50,  2.64it/s]#015 60%|█████▉    | 428/718 [03:02<01:48,  2.67it/s]#015 60%|█████▉    | 429/718 [03:02<01:52,  2.58it/s]#015 60%|█████▉    | 430/718 [03:03<01:51,  2.58it/s]#015 60%|██████    | 431/718 [03:03<01:48,  2.64it/s]#015 60%|██████    | 432/718 [03:04<01:46,  2.69it/s]#015 60%|██████    | 433/718 [03:04<01:44,  2.73it/s]#015 60%|██████    | 434/718 [03:04<01:52,  2.52it/s]#015 61%|██████    | 435/718 [03:05<01:49,  2.58it/s]#015 61%|██████    | 436/718 [03:05<01:49,  2.58it/s]#015 61%|██████    | 437/718 [03:05<01:46,  2.64it/s]#015 61%|██████    | 438/718 [03:06<01:43,  2.70it/s]#015 61%|██████    | 439/718 [03:06<01:41,  2.74it/s]#015 61%|██████▏   | 440/718 [03:07<01:47,  2.59it/s]#015 61%|██████▏   | 441/718 [03:07<01:44,  2.65it/s]#015 62%|██████▏   | 442/718 [03:07<01:44,  2.63it/s]#015 62%|██████▏   | 443/718 [03:08<01:42,  2.69it/s]#015 62%|██████▏   | 444/718 [03:08<01:42,  2.68it/s]#015 62%|██████▏   | 445/718 [03:08<01:44,  2.61it/s]#015 62%|██████▏   | 446/718 [03:09<01:41,  2.67it/s]#015 62%|██████▏   | 447/718 [03:09<01:39,  2.71it/s]#015 62%|██████▏   | 448/718 [03:10<01:40,  2.67it/s]#015 63%|██████▎   | 449/718 [03:10<01:38,  2.72it/s]#015 63%|██████▎   | 450/718 [03:10<01:42,  2.62it/s]#015 63%|██████▎   | 451/718 [03:11<01:39,  2.68it/s]#015 63%|██████▎   | 452/718 [03:11<01:38,  2.70it/s]#015 63%|██████▎   | 453/718 [03:11<01:36,  2.74it/s]#015 63%|██████▎   | 454/718 [03:12<01:36,  2.74it/s]#015 63%|██████▎   | 455/718 [03:12<01:39,  2.64it/s]#015 64%|██████▎   | 456/718 [03:13<01:38,  2.65it/s]#015 64%|██████▎   | 457/718 [03:13<01:36,  2.71it/s]#015 64%|██████▍   | 458/718 [03:13<01:35,  2.73it/s]#015 64%|██████▍   | 459/718 [03:14<01:34,  2.74it/s]#015 64%|██████▍   | 460/718 [03:14<01:37,  2.64it/s]#015 64%|██████▍   | 461/718 [03:14<01:35,  2.69it/s]#015 64%|██████▍   | 462/718 [03:15<01:34,  2.70it/s]#015 64%|██████▍   | 463/718 [03:15<01:35,  2.68it/s]#015 65%|██████▍   | 464/718 [03:16<01:34,  2.69it/s]#015 65%|██████▍   | 465/718 [03:16<01:37,  2.60it/s]#015 65%|██████▍   | 466/718 [03:16<01:35,  2.65it/s]#015 65%|██████▌   | 467/718 [03:17<01:33,  2.70it/s]#015 65%|██████▌   | 468/718 [03:17<01:31,  2.74it/s]#015 65%|██████▌   | 469/718 [03:17<01:30,  2.75it/s]#015 65%|██████▌   | 470/718 [03:18<01:33,  2.64it/s]#015 66%|██████▌   | 471/718 [03:18<01:31,  2.70it/s]#015 66%|██████▌   | 472/718 [03:18<01:29,  2.75it/s]#015 66%|██████▌   | 473/718 [03:19<01:29,  2.75it/s]#015 66%|██████▌   | 474/718 [03:19<01:28,  2.76it/s]#015 66%|██████▌   | 475/718 [03:20<01:31,  2.64it/s]#015 66%|██████▋   | 476/718 [03:20<01:30,  2.69it/s]#015 66%|██████▋   | 477/718 [03:20<01:28,  2.72it/s]#015 67%|██████▋   | 478/718 [03:21<01:27,  2.74it/s]#015 67%|██████▋   | 479/718 [03:21<01:26,  2.75it/s]#015 67%|██████▋   | 480/718 [03:21<01:29,  2.66it/s]#015 67%|██████▋   | 481/718 [03:22<01:27,  2.70it/s]#015 67%|██████▋   | 482/718 [03:22<01:26,  2.73it/s]#015 67%|██████▋   | 483/718 [03:23<01:25,  2.75it/s]#015 67%|██████▋   | 484/718 [03:23<01:24,  2.76it/s]#015 68%|██████▊   | 485/718 [03:23<01:29,  2.62it/s]#015 68%|██████▊   | 486/718 [03:24<01:27,  2.67it/s]#015 68%|██████▊   | 487/718 [03:24<01:25,  2.70it/s]#015 68%|██████▊   | 488/718 [03:24<01:24,  2.74it/s]#015 68%|██████▊   | 489/718 [03:25<01:23,  2.75it/s]#015 68%|██████▊   | 490/718 [03:25<01:28,  2.59it/s]#015 68%|██████▊   | 491/718 [03:26<01:25,  2.64it/s]#015 69%|██████▊   | 492/718 [03:26<01:24,  2.68it/s]#015 69%|██████▊   | 493/718 [03:26<01:22,  2.71it/s]#015 69%|██████▉   | 494/718 [03:27<01:21,  2.74it/s]#015 69%|██████▉   | 495/718 [03:27<01:24,  2.64it/s]#015 69%|██████▉   | 496/718 [03:27<01:23,  2.67it/s]#015 69%|██████▉   | 497/718 [03:28<01:21,  2.71it/s]#015 69%|██████▉   | 498/718 [03:28<01:20,  2.74it/s]#015 69%|██████▉   | 499/718 [03:28<01:20,  2.72it/s]#015 70%|██████▉   | 500/718 [03:29<01:19,  2.75it/s]#015                                                 #015#015 70%|██████▉   | 500/718 [03:29<01:19,  2.75it/s][INFO|trainer.py:1935] 2021-10-12 05:19:46,814 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2021-10-12 05:19:46,815 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-10-12 05:19:47,596 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2021-10-12 05:19:47,597 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2021-10-12 05:19:47,597 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m#015 70%|██████▉   | 501/718 [03:32<03:53,  1.08s/it]#015 70%|██████▉   | 502/718 [03:32<03:06,  1.16it/s]#015 70%|███████   | 503/718 [03:32<02:32,  1.41it/s]#015 70%|███████   | 504/718 [03:33<02:09,  1.66it/s]#015 70%|███████   | 505/718 [03:33<01:56,  1.83it/s]#015 70%|███████   | 506/718 [03:33<01:45,  2.01it/s]#015 71%|███████   | 507/718 [03:34<01:35,  2.20it/s]#015 71%|███████   | 508/718 [03:34<01:29,  2.36it/s]#015 71%|███████   | 509/718 [03:35<01:24,  2.49it/s]#015 71%|███████   | 510/718 [03:35<01:20,  2.57it/s]#015 71%|███████   | 511/718 [03:35<01:21,  2.54it/s]#015 71%|███████▏  | 512/718 [03:36<01:18,  2.61it/s]#015 71%|███████▏  | 513/718 [03:36<01:16,  2.67it/s]#015 72%|███████▏  | 514/718 [03:36<01:15,  2.71it/s]#015 72%|███████▏  | 515/718 [03:37<01:14,  2.74it/s]#015 72%|███████▏  | 516/718 [03:37<01:16,  2.64it/s]#015 72%|███████▏  | 517/718 [03:37<01:14,  2.70it/s]#015 72%|███████▏  | 518/718 [03:38<01:13,  2.73it/s]#015 72%|███████▏  | 519/718 [03:38<01:12,  2.75it/s]#015 72%|███████▏  | 520/718 [03:39<01:11,  2.78it/s]#015 73%|███████▎  | 521/718 [03:39<01:13,  2.68it/s]#015 73%|███████▎  | 522/718 [03:39<01:12,  2.72it/s]#015 73%|███████▎  | 523/718 [03:40<01:11,  2.74it/s]#015 73%|███████▎  | 524/718 [03:40<01:10,  2.74it/s]#015 73%|███████▎  | 525/718 [03:40<01:10,  2.75it/s]#015 73%|███████▎  | 526/718 [03:41<01:12,  2.65it/s]#015 73%|███████▎  | 527/718 [03:41<01:10,  2.69it/s]#015 74%|███████▎  | 528/718 [03:41<01:09,  2.73it/s]#015 74%|███████▎  | 529/718 [03:42<01:08,  2.76it/s]#015 74%|███████▍  | 530/718 [03:42<01:07,  2.77it/s]#015 74%|███████▍  | 531/718 [03:43<01:10,  2.66it/s]#015 74%|███████▍  | 532/718 [03:43<01:08,  2.70it/s]#015 74%|███████▍  | 533/718 [03:43<01:08,  2.72it/s]#015 74%|███████▍  | 534/718 [03:44<01:07,  2.71it/s]#015 75%|███████▍  | 535/718 [03:44<01:08,  2.68it/s]#015 75%|███████▍  | 536/718 [03:45<01:10,  2.59it/s]#015 75%|███████▍  | 537/718 [03:45<01:08,  2.64it/s]#015 75%|███████▍  | 538/718 [03:45<01:07,  2.68it/s]#015 75%|███████▌  | 539/718 [03:46<01:05,  2.72it/s]#015 75%|███████▌  | 540/718 [03:46<01:06,  2.69it/s]#015 75%|███████▌  | 541/718 [03:46<01:08,  2.58it/s]#015 75%|███████▌  | 542/718 [03:47<01:07,  2.60it/s]#015 76%|███████▌  | 543/718 [03:47<01:05,  2.66it/s]#015 76%|███████▌  | 544/718 [03:47<01:04,  2.71it/s]#015 76%|███████▌  | 545/718 [03:48<01:03,  2.73it/s]#015 76%|███████▌  | 546/718 [03:48<01:06,  2.58it/s]#015 76%|███████▌  | 547/718 [03:49<01:05,  2.62it/s]#015 76%|███████▋  | 548/718 [03:49<01:03,  2.69it/s]#015 76%|███████▋  | 549/718 [03:49<01:01,  2.75it/s]#015 77%|███████▋  | 550/718 [03:50<01:01,  2.75it/s]#015 77%|███████▋  | 551/718 [03:50<01:03,  2.63it/s]#015 77%|███████▋  | 552/718 [03:50<01:02,  2.66it/s]#015 77%|███████▋  | 553/718 [03:51<01:01,  2.68it/s]#015 77%|███████▋  | 554/718 [03:51<01:00,  2.69it/s]#015 77%|███████▋  | 555/718 [03:52<01:00,  2.70it/s]#015 77%|███████▋  | 556/718 [03:52<01:03,  2.56it/s]#015 78%|███████▊  | 557/718 [03:52<01:01,  2.61it/s]#015 78%|███████▊  | 558/718 [03:53<01:00,  2.64it/s]#015 78%|███████▊  | 559/718 [03:53<01:00,  2.65it/s]#015 78%|███████▊  | 560/718 [03:54<00:59,  2.66it/s]#015 78%|███████▊  | 561/718 [03:54<01:03,  2.49it/s]#015 78%|███████▊  | 562/718 [03:54<01:01,  2.55it/s]#015 78%|███████▊  | 563/718 [03:55<00:59,  2.60it/s]#015 79%|███████▊  | 564/718 [03:55<00:58,  2.63it/s]#015 79%|███████▊  | 565/718 [03:55<00:57,  2.65it/s]#015 79%|███████▉  | 566/718 [03:56<00:59,  2.55it/s]#015 79%|███████▉  | 567/718 [03:56<00:58,  2.60it/s]#015 79%|███████▉  | 568/718 [03:57<00:56,  2.63it/s]#015 79%|███████▉  | 569/718 [03:57<00:56,  2.66it/s]#015 79%|███████▉  | 570/718 [03:57<00:55,  2.68it/s]#015 80%|███████▉  | 571/718 [03:58<00:54,  2.69it/s]#015 80%|███████▉  | 572/718 [03:58<00:56,  2.58it/s]#015 80%|███████▉  | 573/718 [03:58<00:55,  2.62it/s]#015 80%|███████▉  | 574/718 [03:59<00:54,  2.64it/s]#015 80%|████████  | 575/718 [03:59<00:54,  2.64it/s]#015 80%|████████  | 576/718 [04:00<00:52,  2.68it/s]#015 80%|████████  | 577/718 [04:00<00:54,  2.59it/s]#015 81%|████████  | 578/718 [04:00<00:53,  2.64it/s]#015 81%|████████  | 579/718 [04:01<00:51,  2.69it/s]#015 81%|████████  | 580/718 [04:01<00:50,  2.72it/s]#015 81%|████████  | 581/718 [04:01<00:49,  2.75it/s]#015 81%|████████  | 582/718 [04:02<00:51,  2.66it/s]#015 81%|████████  | 583/718 [04:02<00:49,  2.70it/s]#015 81%|████████▏ | 584/718 [04:03<00:48,  2.74it/s]#015 81%|████████▏ | 585/718 [04:03<00:48,  2.76it/s]#015 82%|████████▏ | 586/718 [04:03<00:47,  2.77it/s]#015 82%|████████▏ | 587/718 [04:04<00:49,  2.65it/s]#015 82%|████████▏ | 588/718 [04:04<00:48,  2.70it/s]#015 82%|████████▏ | 589/718 [04:04<00:48,  2.68it/s]#015 82%|████████▏ | 590/718 [04:05<00:46,  2.75it/s]#015 82%|████████▏ | 591/718 [04:05<00:45,  2.76it/s]#015 82%|████████▏ | 592/718 [04:06<00:47,  2.65it/s]#015 83%|████████▎ | 593/718 [04:06<00:46,  2.69it/s]#015 83%|████████▎ | 594/718 [04:06<00:45,  2.72it/s]#015 83%|████████▎ | 595/718 [04:07<00:44,  2.74it/s]#015 83%|████████▎ | 596/718 [04:07<00:44,  2.75it/s]#015 83%|████████▎ | 597/718 [04:07<00:45,  2.66it/s]#015 83%|████████▎ | 598/718 [04:08<00:44,  2.70it/s]#015 83%|████████▎ | 599/718 [04:08<00:43,  2.74it/s]#015 84%|████████▎ | 600/718 [04:08<00:42,  2.76it/s]#015 84%|████████▎ | 601/718 [04:09<00:42,  2.78it/s]#015 84%|████████▍ | 602/718 [04:09<00:43,  2.67it/s]#015 84%|████████▍ | 603/718 [04:10<00:42,  2.71it/s]#015 84%|████████▍ | 604/718 [04:10<00:41,  2.74it/s]#015 84%|████████▍ | 605/718 [04:10<00:40,  2.76it/s]#015 84%|████████▍ | 606/718 [04:11<00:40,  2.77it/s]#015 85%|████████▍ | 607/718 [04:11<00:41,  2.67it/s]#015 85%|████████▍ | 608/718 [04:11<00:40,  2.70it/s]#015 85%|████████▍ | 609/718 [04:12<00:39,  2.73it/s]#015 85%|████████▍ | 610/718 [04:12<00:39,  2.75it/s]#015 85%|████████▌ | 611/718 [04:12<00:38,  2.77it/s]#015 85%|████████▌ | 612/718 [04:13<00:39,  2.66it/s]#015 85%|████████▌ | 613/718 [04:13<00:39,  2.69it/s]#015 86%|████████▌ | 614/718 [04:14<00:38,  2.70it/s]#015 86%|████████▌ | 615/718 [04:14<00:37,  2.71it/s]#015 86%|████████▌ | 616/718 [04:14<00:37,  2.73it/s]#015 86%|████████▌ | 617/718 [04:15<00:38,  2.64it/s]#015 86%|████████▌ | 618/718 [04:15<00:37,  2.67it/s]#015 86%|████████▌ | 619/718 [04:15<00:36,  2.69it/s]#015 86%|████████▋ | 620/718 [04:16<00:36,  2.70it/s]#015 86%|████████▋ | 621/718 [04:16<00:35,  2.71it/s]#015 87%|████████▋ | 622/718 [04:17<00:37,  2.56it/s]#015 87%|████████▋ | 623/718 [04:17<00:36,  2.62it/s]#015 87%|████████▋ | 624/718 [04:17<00:35,  2.66it/s]#015 87%|████████▋ | 625/718 [04:18<00:34,  2.68it/s]#015 87%|████████▋ | 626/718 [04:18<00:34,  2.70it/s]#015 87%|████████▋ | 627/718 [04:19<00:35,  2.58it/s]#015 87%|████████▋ | 628/718 [04:19<00:34,  2.63it/s]#015 88%|████████▊ | 629/718 [04:19<00:33,  2.67it/s]#015 88%|████████▊ | 630/718 [04:20<00:32,  2.69it/s]#015 88%|████████▊ | 631/718 [04:20<00:32,  2.72it/s]#015 88%|████████▊ | 632/718 [04:20<00:31,  2.72it/s]#015 88%|████████▊ | 633/718 [04:21<00:32,  2.62it/s]#015 88%|████████▊ | 634/718 [04:21<00:31,  2.66it/s]#015 88%|████████▊ | 635/718 [04:21<00:30,  2.69it/s]#015 89%|████████▊ | 636/718 [04:22<00:30,  2.71it/s]#015 89%|████████▊ | 637/718 [04:22<00:29,  2.72it/s]#015 89%|████████▉ | 638/718 [04:23<00:30,  2.62it/s]#015 89%|████████▉ | 639/718 [04:23<00:29,  2.64it/s]#015 89%|████████▉ | 640/718 [04:23<00:29,  2.67it/s]#015 89%|████████▉ | 641/718 [04:24<00:28,  2.68it/s]#015 89%|████████▉ | 642/718 [04:24<00:28,  2.70it/s]#015 90%|████████▉ | 643/718 [04:25<00:28,  2.59it/s]#015 90%|████████▉ | 644/718 [04:25<00:28,  2.64it/s]#015 90%|████████▉ | 645/718 [04:25<00:27,  2.67it/s]#015 90%|████████▉ | 646/718 [04:26<00:26,  2.69it/s]#015 90%|█████████ | 647/718 [04:26<00:26,  2.70it/s]#015 90%|█████████ | 648/718 [04:26<00:26,  2.60it/s]#015 90%|█████████ | 649/718 [04:27<00:26,  2.65it/s]#015 91%|█████████ | 650/718 [04:27<00:25,  2.68it/s]#015 91%|█████████ | 651/718 [04:27<00:24,  2.70it/s]#015 91%|█████████ | 652/718 [04:28<00:24,  2.71it/s]#015 91%|█████████ | 653/718 [04:28<00:24,  2.61it/s]#015 91%|█████████ | 654/718 [04:29<00:24,  2.64it/s]#015 91%|█████████ | 655/718 [04:29<00:23,  2.67it/s]#015 91%|█████████▏| 656/718 [04:29<00:22,  2.70it/s]#015 92%|█████████▏| 657/718 [04:30<00:22,  2.71it/s]#015 92%|█████████▏| 658/718 [04:30<00:23,  2.60it/s]#015 92%|█████████▏| 659/718 [04:31<00:22,  2.63it/s]#015 92%|█████████▏| 660/718 [04:31<00:21,  2.66it/s]#015 92%|█████████▏| 661/718 [04:31<00:21,  2.69it/s]#015 92%|█████████▏| 662/718 [04:32<00:20,  2.70it/s]#015 92%|█████████▏| 663/718 [04:32<00:21,  2.59it/s]#015 92%|█████████▏| 664/718 [04:32<00:20,  2.63it/s]#015 93%|█████████▎| 665/718 [04:33<00:19,  2.66it/s]#015 93%|█████████▎| 666/718 [04:33<00:19,  2.68it/s]#015 93%|█████████▎| 667/718 [04:34<00:18,  2.69it/s]#015 93%|█████████▎| 668/718 [04:34<00:19,  2.58it/s]#015 93%|█████████▎| 669/718 [04:34<00:18,  2.62it/s]#015 93%|█████████▎| 670/718 [04:35<00:17,  2.68it/s]#015 93%|█████████▎| 671/718 [04:35<00:17,  2.64it/s]#015 94%|█████████▎| 672/718 [04:35<00:17,  2.67it/s]#015 94%|█████████▎| 673/718 [04:36<00:17,  2.58it/s]#015 94%|█████████▍| 674/718 [04:36<00:16,  2.62it/s]#015 94%|█████████▍| 675/718 [04:37<00:16,  2.66it/s]#015 94%|█████████▍| 676/718 [04:37<00:15,  2.69it/s]#015 94%|█████████▍| 677/718 [04:37<00:15,  2.71it/s]#015 94%|█████████▍| 678/718 [04:38<00:15,  2.57it/s]#015 95%|█████████▍| 679/718 [04:38<00:14,  2.64it/s]#015 95%|█████████▍| 680/718 [04:38<00:14,  2.67it/s]#015 95%|█████████▍| 681/718 [04:39<00:13,  2.70it/s]#015 95%|█████████▍| 682/718 [04:39<00:13,  2.71it/s]#015 95%|█████████▌| 683/718 [04:40<00:13,  2.57it/s]#015 95%|█████████▌| 684/718 [04:40<00:12,  2.62it/s]#015 95%|█████████▌| 685/718 [04:40<00:12,  2.66it/s]#015 96%|█████████▌| 686/718 [04:41<00:11,  2.68it/s]#015 96%|█████████▌| 687/718 [04:41<00:11,  2.65it/s]#015 96%|█████████▌| 688/718 [04:42<00:11,  2.55it/s]#015 96%|█████████▌| 689/718 [04:42<00:11,  2.59it/s]#015 96%|█████████▌| 690/718 [04:42<00:10,  2.62it/s]#015 96%|█████████▌| 691/718 [04:43<00:10,  2.66it/s]#015 96%|█████████▋| 692/718 [04:43<00:09,  2.68it/s]#015 97%|█████████▋| 693/718 [04:43<00:09,  2.70it/s]#015 97%|█████████▋| 694/718 [04:44<00:09,  2.58it/s]#015 97%|█████████▋| 695/718 [04:44<00:08,  2.63it/s]#015 97%|█████████▋| 696/718 [04:45<00:08,  2.64it/s]#015 97%|█████████▋| 697/718 [04:45<00:07,  2.66it/s]#015 97%|█████████▋| 698/718 [04:45<00:07,  2.68it/s]#015 97%|█████████▋| 699/718 [04:46<00:07,  2.58it/s]#015 97%|█████████▋| 700/718 [04:46<00:06,  2.59it/s]#015 98%|█████████▊| 701/718 [04:46<00:06,  2.63it/s]#015 98%|█████████▊| 702/718 [04:47<00:06,  2.63it/s]#015 98%|█████████▊| 703/718 [04:47<00:05,  2.67it/s]#015 98%|█████████▊| 704/718 [04:48<00:05,  2.58it/s]#015 98%|█████████▊| 705/718 [04:48<00:04,  2.64it/s]#015 98%|█████████▊| 706/718 [04:48<00:04,  2.63it/s]#015 98%|█████████▊| 707/718 [04:49<00:04,  2.68it/s]#015 99%|█████████▊| 708/718 [04:49<00:03,  2.66it/s]#015 99%|█████████▊| 709/718 [04:49<00:03,  2.55it/s]#015 99%|█████████▉| 710/718 [04:50<00:03,  2.61it/s]#015 99%|█████████▉| 711/718 [04:50<00:02,  2.66it/s]#015 99%|█████████▉| 712/718 [04:51<00:02,  2.70it/s]#015 99%|█████████▉| 713/718 [04:51<00:01,  2.73it/s]#015 99%|█████████▉| 714/718 [04:51<00:01,  2.63it/s]#015100%|█████████▉| 715/718 [04:52<00:01,  2.67it/s]#015100%|█████████▉| 716/718 [04:52<00:00,  2.70it/s]#015100%|█████████▉| 717/718 [04:52<00:00,  2.72it/s]#015100%|██████████| 718/718 [04:53<00:00,  3.03it/s][INFO|trainer.py:1366] 2021-10-12 05:21:10,620 >> \n",
      "\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015                                                 #015#015100%|██████████| 718/718 [04:53<00:00,  3.03it/s]#015100%|██████████| 718/718 [04:53<00:00,  2.45it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1935] 2021-10-12 05:21:10,621 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2021-10-12 05:21:10,622 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2021-10-12 05:21:11,367 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2021-10-12 05:21:11,368 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2021-10-12 05:21:11,368 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-12 05:21:28 Uploading - Uploading generated training model\n",
      "2021-10-12 05:24:52 Completed - Training job completed\n",
      "Training seconds: 831\n",
      "Billable seconds: 831\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': 's3://sagemaker-us-east-1-905847418383/qa-tr/data/', 'val': 's3://sagemaker-us-east-1-905847418383/qa-tr/data/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb3333c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2021-10-11-15-01-25-153/output/model.tar.gz\",  # path to your trained sagemaker model\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.10\", # transformers version used\n",
    "   pytorch_version=\"1.9\", # pytorch version used\n",
    "   py_version=\"py38\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1eb89ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_estimator.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aebca595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.24896939098834991,\n",
       " 'start': 752,\n",
       " 'end': 775,\n",
       " 'answer': 'Babasının ölümü üzerine'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "\"inputs\": {\n",
    "    \"question\": \"Ne zaman avare bir hayata başladı?\",\n",
    "    \"context\": \"\"\"ABASIYANIK, Sait Faik. Hikayeci (Adapazarı 23 Kasım 1906-İstanbul 11 Mayıs 1954). \\\n",
    "İlk öğrenimine Adapazarı’nda Rehber-i Terakki Mektebi’nde başladı. İki yıl kadar Adapazarı İdadisi’nde okudu.\\\n",
    "İstanbul Erkek Lisesi’nde devam ettiği orta öğrenimini Bursa Lisesi’nde tamamladı (1928). İstanbul Edebiyat \\\n",
    "Fakültesi’ne iki yıl devam ettikten sonra babasının isteği üzerine iktisat öğrenimi için İsviçre’ye gitti. \\\n",
    "Kısa süre sonra iktisat öğrenimini bırakarak Lozan’dan Grenoble’a geçti. Üç yıl başıboş bir edebiyat öğrenimi \\\n",
    "gördükten sonra babası tarafından geri çağrıldı (1933). Bir müddet Halıcıoğlu Ermeni Yetim Mektebi'nde Türkçe \\\n",
    "gurup dersleri öğretmenliği yaptı. Ticarete atıldıysa da tutunamadı. Bir ay Haber gazetesinde adliye muhabirliği\\\n",
    "yaptı (1942). Babasının ölümü üzerine aileden kalan emlakin geliri ile avare bir hayata başladı. Evlenemedi.\\\n",
    "Yazları Burgaz adasındaki köşklerinde, kışları Şişli’deki apartmanlarında annesi ile beraber geçen bu fazla \\\n",
    "içkili bohem hayatı ömrünün sonuna kadar sürdü.\"\"\"\n",
    "    }\n",
    "}\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b196022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
