{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving NLP (Natural Language Processing) Problems with HuggingFace Library on AWS for Turkish Language\n",
    "\n",
    "## Use Case\n",
    "In this demo, we will demonstrate two NLP use cases.\n",
    "- _Sentiment Analysis:_ You can find positive, negative and neutral mentions about your business, competitors or any topic provided as text to the Machine Learning model.\n",
    "- _Question Answering:_ Question-Answering Models are  deep learning models that can answer questions given some context, and sometimes without any context (e.g. open-domain QA). They can extract answer phrases from paragraphs, paraphrase the answer generatively, or choose one option out of a list of given options, and so on.\n",
    "\n",
    "## Dataset\n",
    "We will use following datasets:\n",
    "- _Dataset Card for Turkish Product Reviews:_ This Turkish Product Reviews Dataset contains 235.165 product reviews collected online. There are 220.284 positive, 14881 negative reviews.\n",
    "- _Turkish NLP Q&A Dataset:_ This dataset is the Turkish Question & Answer dataset on Turkish Science History.\n",
    "\n",
    "## Approach\n",
    "Instead of creating a new Machine Learning (ML) model for every new task, we can leverage the concept of *Transfer Learning*.\n",
    "In particular, we can use generic language models and teach it new tasks by fine-tuning them using corresponding datasets.\n",
    "In this notebook we will use a Turkish language model created by the MDZ Digital Library team (dbmdz) at the Bavarian State Library (https://github.com/stefan-it/turkish-bert). We will use the Hugging Face Model Hub to download the model (https://huggingface.co/dbmdz/bert-base-turkish-uncased) and then fine-tune it to two  different tasks. We will deploy to SageMaker for real-time inferencing.\n",
    "- Sentiment Analysis: We will see how the fine tuned model achieves SoTA (State of the Art) performance for Sentiment Analysis for Turkish easily.\n",
    "- Question Answering:\n",
    "\n",
    "## How to Run this Notebook in Amazon SageMaker\n",
    "You can run this notebook in SageMaker Studio. Please select the `PyTorch 1.6 Python 3.6 CPU Optimized` kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets IProgress -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.19.60 requires botocore==1.20.60, but you have botocore 1.23.37 which is incompatible.\n",
      "awscli 1.19.60 requires s3transfer<0.5.0,>=0.4.0, but you have s3transfer 0.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::905847418383:role/service-role/AmazonSageMaker-ExecutionRole-20220115T164022\n",
      "sagemaker bucket: sagemaker-us-east-1-905847418383\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dbmdz/bert-base-turkish-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset and splitting into test and training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will downlaod the data directly from Huggingface: https://huggingface.co/datasets/turkish_product_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.huggingface.model import HuggingFacePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'turkish_product_reviews'\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only take 10% of the data to reduce training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset['train'].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into training set (90%) and test set (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sample['test']\n",
    "train_test = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_test['train']\n",
    "test_dataset = train_test['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inspect = pd.concat([df_train[df_train['sentiment']==0].head(3), df_train[df_train['sentiment']==1].head(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start the training we need to tokenize the data save it in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  train_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix_sentiment = 'datasets/turkish_product_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sagemaker_session_bucket}/{s3_prefix_sentiment}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sagemaker_session_bucket}/{s3_prefix_sentiment}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters_sentiment={'epochs': 1,\n",
    "                 'train_batch_size': 8,\n",
    "                 'model_name': model_name\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator_sentiment = HuggingFace(entry_point='train.py',\n",
    "                                    source_dir='./scripts',\n",
    "                                    instance_type='ml.p3.2xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.6',\n",
    "                                    pytorch_version='1.7',\n",
    "                                    py_version='py36',\n",
    "                                    hyperparameters=hyperparameters_sentiment,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator_sentiment.fit({'train': training_input_path, 'test': test_input_path}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_sentiment = huggingface_estimator_sentiment.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    wait=False,\n",
    "    endpoint_name=\"turkish-sentiment-endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only required to create a predictor from an already deployed model\n",
    "predictor_sentiment = HuggingFacePredictor('turkish-sentiment-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text: \"This is a pretty bad product, I wouldn't recommend this to anyone\"\n",
    "sentiment_input= {\"inputs\": \"Bu oldukça kötü bir ürün, bunu kimseye tavsiye etmem\"}\n",
    "predictor_sentiment.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input text: \"I love this shampoo, it makes my hair so shiny\"\n",
    "sentiment_input= {\"inputs\": \"Bu şampuanı seviyorum, saçlarımı çok parlak yapıyor\"}\n",
    "predictor_sentiment.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "Taken from https://github.com/TQuad/turkish-nlp-qa-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/train-v0.1.json -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/TQuad/turkish-nlp-qa-dataset/master/dev-v0.1.json -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv train-v0.1.json data/train-v0.1.json\n",
    "!mv dev-v0.1.json data/dev-v0.1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON files must be converted so that they can be used in a Q&A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_json(input_filename, output_filename):\n",
    "    with open(input_filename) as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for article in dataset[\"data\"]:\n",
    "            title = article[\"title\"]\n",
    "            for paragraph in article[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]\n",
    "                answers = {}\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    question = qa[\"question\"]\n",
    "                    idx = qa[\"id\"]\n",
    "                    answers[\"text\"] = [str(a[\"text\"]) for a in qa[\"answers\"]]\n",
    "                    answers[\"answer_start\"] = [int(a[\"answer_start\"]) for a in qa[\"answers\"]]\n",
    "                    f.write(\n",
    "                        json.dumps(\n",
    "                            {\n",
    "                                \"id\": str(idx),\n",
    "                                \"title\": str(title),\n",
    "                                \"context\": str(context),\n",
    "                                \"question\": str(question),\n",
    "                                \"answers\": answers,\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_json('data/train-v0.1.json', 'data/train.json')\n",
    "convert_json('data/dev-v0.1.json', 'data/val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = 'data/train.json'\n",
    "data_files[\"validation\"] = 'data/val.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f36d5928021e5d85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f36d5928021e5d85/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce868d2ef83940ed87ac09d6e58cb308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdde34258ca45d19450270a82242483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f36d5928021e5d85/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30491e05c314805878341ee4981f0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7518</th>\n",
       "      <td>8348</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevi</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevi Araştırma ve U...</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevi Araştırma ve U...</td>\n",
       "      <td>{'text': ['&lt;generator object convert_json.&lt;loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7519</th>\n",
       "      <td>8349</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevi</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevi Araştırma ve U...</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevinin ne gibi ama...</td>\n",
       "      <td>{'text': ['&lt;generator object convert_json.&lt;loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>8350</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevi</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevi Araştırma ve U...</td>\n",
       "      <td>İstanbul Üniversitesi Gözlemevi hangi amaçla a...</td>\n",
       "      <td>{'text': ['&lt;generator object convert_json.&lt;loc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                            title  \\\n",
       "7518  8348  İstanbul Üniversitesi Gözlemevi   \n",
       "7519  8349  İstanbul Üniversitesi Gözlemevi   \n",
       "7520  8350  İstanbul Üniversitesi Gözlemevi   \n",
       "\n",
       "                                                context  \\\n",
       "7518  İstanbul Üniversitesi Gözlemevi Araştırma ve U...   \n",
       "7519  İstanbul Üniversitesi Gözlemevi Araştırma ve U...   \n",
       "7520  İstanbul Üniversitesi Gözlemevi Araştırma ve U...   \n",
       "\n",
       "                                               question  \\\n",
       "7518  İstanbul Üniversitesi Gözlemevi Araştırma ve U...   \n",
       "7519  İstanbul Üniversitesi Gözlemevinin ne gibi ama...   \n",
       "7520  İstanbul Üniversitesi Gözlemevi hangi amaçla a...   \n",
       "\n",
       "                                                answers  \n",
       "7518  {'text': ['<generator object convert_json.<loc...  \n",
       "7519  {'text': ['<generator object convert_json.<loc...  \n",
       "7520  {'text': ['<generator object convert_json.<loc...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[7518:7521]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix_qa = 'datasets/turkish_qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train.json to s3://sagemaker-us-east-1-905847418383/datasets/turkish_qa/train.json\n",
      "upload: data/val.json to s3://sagemaker-us-east-1-905847418383/datasets/turkish_qa/val.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/train.json s3://$sagemaker_session_bucket/$s3_prefix_qa/train.json\n",
    "!aws s3 cp data/val.json s3://$sagemaker_session_bucket/$s3_prefix_qa/val.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters_qa={\n",
    "    'model_name_or_path': model_name,\n",
    "    'train_file': '/opt/ml/input/data/train/train.json',\n",
    "    'validation_file': '/opt/ml/input/data/val/val.json',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': True,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'num_train_epochs': 2,\n",
    "    'max_seq_length': 384,\n",
    "    'pad_to_max_length': True,\n",
    "    'doc_stride': 128,\n",
    "    'output_dir': '/opt/ml/model'\n",
    "}\n",
    "\n",
    "metric_definitions=[\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]\n",
    "\n",
    "instance_type = 'ml.p3.16xlarge'\n",
    "instance_count = 1\n",
    "volume_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator_qa = HuggingFace(entry_point='run_qa.py',\n",
    "                                       source_dir='./scripts',\n",
    "                                       metric_definitions=metric_definitions,\n",
    "                                       instance_type=instance_type,\n",
    "                                       instance_count=instance_count,\n",
    "                                       volume_size=volume_size,\n",
    "                                       role=role,\n",
    "                                       transformers_version='4.10',\n",
    "                                       pytorch_version='1.9',\n",
    "                                       py_version='py38',\n",
    "                                       hyperparameters=hyperparameters_qa,\n",
    "                                       disable_profiler=True,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-15 18:04:00 Starting - Starting the training job...\n",
      "2022-01-15 18:04:03 Starting - Launching requested ML instances.........\n",
      "2022-01-15 18:05:58 Starting - Preparing the instances for training............\n",
      "2022-01-15 18:08:01 Downloading - Downloading input data...\n",
      "2022-01-15 18:08:08 Training - Downloading the training image.........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-15 18:12:30,261 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-15 18:12:30,336 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-01-15 18:12:30,968 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-01-15 18:12:31,714 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"validation_file\": \"/opt/ml/input/data/val/val.json\",\n",
      "        \"do_train\": true,\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"do_eval\": true,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.json\",\n",
      "        \"doc_stride\": 128,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"max_seq_length\": 384,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"model_name_or_path\": \"dbmdz/bert-base-turkish-uncased\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-01-15-18-04-00-125\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2022-01-15-18-04-00-125/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qa.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"model_name_or_path\":\"dbmdz/bert-base-turkish-uncased\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"train_file\":\"/opt/ml/input/data/train/train.json\",\"validation_file\":\"/opt/ml/input/data/val/val.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qa.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qa\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2022-01-15-18-04-00-125/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"model_name_or_path\":\"dbmdz/bert-base-turkish-uncased\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"train_file\":\"/opt/ml/input/data/train/train.json\",\"validation_file\":\"/opt/ml/input/data/val/val.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-01-15-18-04-00-125\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905847418383/huggingface-pytorch-training-2022-01-15-18-04-00-125/source/sourcedir.tar.gz\",\"module_name\":\"run_qa\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qa.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--doc_stride\",\"128\",\"--fp16\",\"True\",\"--max_seq_length\",\"384\",\"--model_name_or_path\",\"dbmdz/bert-base-turkish-uncased\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/opt/ml/model\",\"--pad_to_max_length\",\"True\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--train_file\",\"/opt/ml/input/data/train/train.json\",\"--validation_file\",\"/opt/ml/input/data/val/val.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/val/val.json\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.json\u001b[0m\n",
      "\u001b[34mSM_HP_DOC_STRIDE=128\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=384\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=dbmdz/bert-base-turkish-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 run_qa.py --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --model_name_or_path dbmdz/bert-base-turkish-uncased --num_train_epochs 2 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --train_file /opt/ml/input/data/train/train.json --validation_file /opt/ml/input/data/val/val.json\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:37 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 8distributed training: False, 16-bits training: True\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=8,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.NO,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Jan15_18-12-36_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=2.0,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=model,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=None,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:37 - INFO - __main__ - Test1\u001b[0m\n",
      "\u001b[34mTest2\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:38 - WARNING - datasets.builder - Using custom data configuration default-68de5053649ec49d\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:38 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-68de5053649ec49d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-68de5053649ec49d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:38 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:38 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:38 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:38 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:38 - INFO - datasets.builder - Generating split validation\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:38 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-68de5053649ec49d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-01-15 18:12:38,183 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpje7r27_1\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-01-15 18:12:38,207 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-01-15 18:12:38,207 >> creating metadata file for /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2022-01-15 18:12:38,208 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-15 18:12:38,208 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-01-15 18:12:38,235 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpal9dli75\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-01-15 18:12:38,259 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-01-15 18:12:38,260 >> creating metadata file for /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2022-01-15 18:12:38,286 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-15 18:12:38,286 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-01-15 18:12:38,346 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9woo6mw9\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-01-15 18:12:38,378 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-01-15 18:12:38,378 >> creating metadata file for /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2022-01-15 18:12:38,511 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-15 18:12:38,511 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2022-01-15 18:12:38,569 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-15 18:12:38,569 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-01-15 18:12:38,669 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdke10uk6\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-01-15 18:12:45,386 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-01-15 18:12:45,386 >> creating metadata file for /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1279] 2022-01-15 18:12:45,386 >> loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1515] 2022-01-15 18:12:47,084 >> Some weights of the model checkpoint at dbmdz/bert-base-turkish-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1526] 2022-01-15 18:12:47,085 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-68de5053649ec49d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-41d64a96e2ddbb7e.arrow\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-68de5053649ec49d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-18326f49fe210a0c.arrow\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqu8vflzb\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/9b6df77cd2566eca67dda976ee18f34e95536ec8e33f9292bfc3daea9b20a092.ab3a5db6a587c35cfd241275240e52547dd1e093c74b3ee4f7798d9f6c6304ec.py\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9b6df77cd2566eca67dda976ee18f34e95536ec8e33f9292bfc3daea9b20a092.ab3a5db6a587c35cfd241275240e52547dd1e093c74b3ee4f7798d9f6c6304ec.py\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_fnz2zmr\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/188f3c7a325773b47d41f3e0f7ab9fd3cb20e597010b3a9d780c878dacc10ce3.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/188f3c7a325773b47d41f3e0f7ab9fd3cb20e597010b3a9d780c878dacc10ce3.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py to /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/squad.py\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/dataset_infos.json\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/squad/squad.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/squad.json\u001b[0m\n",
      "\u001b[34m01/15/2022 18:12:53 - INFO - datasets.load - Copying local import file from /root/.cache/huggingface/datasets/downloads/188f3c7a325773b47d41f3e0f7ab9fd3cb20e597010b3a9d780c878dacc10ce3.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py at /root/.cache/huggingface/modules/datasets_modules/metrics/squad/513bf9facd7f12b0871a3d74c6999c866ce28196c9cdb151dcf934848655d77e/evaluate.py\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:414] 2022-01-15 18:12:58,473 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1168] 2022-01-15 18:12:58,484 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1169] 2022-01-15 18:12:58,485 >>   Num examples = 11466\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1170] 2022-01-15 18:12:58,485 >>   Num Epochs = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2022-01-15 18:12:58,485 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2022-01-15 18:12:58,485 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2022-01-15 18:12:58,485 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2022-01-15 18:12:58,485 >>   Total optimization steps = 718\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.691 algo-1:24 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.825 algo-1:24 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.825 algo-1:24 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.826 algo-1:24 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.826 algo-1:24 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.826 algo-1:24 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.987 algo-1:24 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:24576000\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.987 algo-1:24 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.988 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.989 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.990 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.991 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.992 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.993 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.994 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.qa_outputs.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:591] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.995 algo-1:24 INFO hook.py:593] Total Trainable Params: 110028290\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.996 algo-1:24 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-01-15 18:12:58.997 algo-1:24 INFO hook.py:488] Hook is writing from the hook with pid: 24\u001b[0m\n",
      "\u001b[34malgo-1:24:24 [0] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34mNCCL version 2.7.8+cuda11.1\u001b[0m\n",
      "\n",
      "2022-01-15 18:13:10 Training - Training image download completed. Training in progress.\u001b[34m{'loss': 1.5666, 'learning_rate': 1.5389972144846798e-05, 'epoch': 1.39}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1935] 2022-01-15 18:16:26,639 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2022-01-15 18:16:26,640 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2022-01-15 18:16:27,369 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2022-01-15 18:16:27,369 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2022-01-15 18:16:27,369 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1366] 2022-01-15 18:17:51,045 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 292.5605, 'train_samples_per_second': 78.384, 'train_steps_per_second': 2.454, 'train_loss': 1.3551674632972994, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1935] 2022-01-15 18:17:51,046 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2022-01-15 18:17:51,047 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2022-01-15 18:17:51,774 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2022-01-15 18:17:51,775 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2022-01-15 18:17:51,775 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     1.3552\n",
      "  train_runtime            = 0:04:52.56\n",
      "  train_samples            =      11466\n",
      "  train_samples_per_second =     78.384\n",
      "  train_steps_per_second   =      2.454\u001b[0m\n",
      "\u001b[34m01/15/2022 18:17:51 - INFO - __main__ - *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:520] 2022-01-15 18:17:51,825 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2181] 2022-01-15 18:17:51,828 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2183] 2022-01-15 18:17:51,828 >>   Num examples = 1442\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2186] 2022-01-15 18:17:51,828 >>   Batch size = 32\u001b[0m\n",
      "\n",
      "2022-01-15 18:18:16 Uploading - Uploading generated training model\u001b[34m01/15/2022 18:18:04 - INFO - utils_qa - Post-processing 892 example predictions split into 1442 features.\u001b[0m\n",
      "\u001b[34m01/15/2022 18:18:08 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m01/15/2022 18:18:08 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m01/15/2022 18:18:08 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\n",
      "  epoch            =     2.0\n",
      "  eval_exact_match =  54.148\n",
      "  eval_f1          = 74.4366\n",
      "  eval_samples     =    1442\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#015100%|██████████| 2/2 [00:00<00:00, 18850.80it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?it/s]#015100%|██████████| 2/2 [00:00<00:00, 1813.75it/s]\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015#015  0%|          | 0/2 [00:00<?, ?it/s]#015100%|██████████| 2/2 [00:00<00:00, 330.44it/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-01-15 18:12:38,183 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpje7r27_1\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 385/385 [00:00<00:00, 665kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-01-15 18:12:38,207 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-01-15 18:12:38,207 >> creating metadata file for /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2022-01-15 18:12:38,208 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-15 18:12:38,208 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\u001b[0m\n",
      "\u001b[34m2022-01-15 18:18:12,221 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-01-15 18:12:38,235 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpal9dli75\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/59.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 59.0/59.0 [00:00<00:00, 99.6kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-01-15 18:12:38,259 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-01-15 18:12:38,260 >> creating metadata file for /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2022-01-15 18:12:38,286 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-15 18:12:38,286 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-01-15 18:12:38,346 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9woo6mw9\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 263k/263k [00:00<00:00, 44.7MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-01-15 18:12:38,378 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-01-15 18:12:38,378 >> creating metadata file for /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/79be37af8c4dff9ec826fae8061b4e805ea5b81e12ac77fd93021d445d90d46b.4ff1dc178b93b6275853fbacc77602ab780339fca423db0c4327e3f03209a772\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-15 18:12:38,485 >> loading file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9cb640d980c71e1b39f560fec126669724429503421cb2df1d82afee38e1ada1.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2022-01-15 18:12:38,511 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-15 18:12:38,511 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:561] 2022-01-15 18:12:38,569 >> loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1bba11007b0853a5f63dc091fc5dd36989ea017cb369f660839de9efcf15847e.d34b61340faaa3122b620c12ba54ab8b1016bfeb2ff97c1d5e80b86808783b7a\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:598] 2022-01-15 18:12:38,569 >> Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1665] 2022-01-15 18:12:38,669 >> https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdke10uk6\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]#015Downloading:   1%|▏         | 5.72M/445M [00:00<00:07, 57.2MB/s]#015Downloading:   3%|▎         | 12.1M/445M [00:00<00:07, 60.8MB/s]#015Downloading:   4%|▍         | 18.4M/445M [00:00<00:06, 62.0MB/s]#015Downloading:   6%|▌         | 24.7M/445M [00:00<00:06, 62.4MB/s]#015Downloading:   7%|▋         | 31.0M/445M [00:00<00:06, 62.6MB/s]#015Downloading:   8%|▊         | 37.2M/445M [00:00<00:06, 62.6MB/s]#015Downloading:  10%|▉         | 43.6M/445M [00:00<00:06, 62.8MB/s]#015Downloading:  11%|█         | 49.9M/445M [00:00<00:06, 62.8MB/s]#015Downloading:  13%|█▎        | 56.1M/445M [00:00<00:06, 62.8MB/s]#015Downloading:  14%|█▍        | 62.5M/445M [00:01<00:06, 63.0MB/s]#015Downloading:  15%|█▌        | 68.8M/445M [00:01<00:05, 62.9MB/s]#015Downloading:  17%|█▋        | 75.1M/445M [00:01<00:05, 63.1MB/s]#015Downloading:  18%|█▊        | 81.5M/445M [00:01<00:05, 63.2MB/s]#015Downloading:  20%|█▉        | 87.8M/445M [00:01<00:05, 63.2MB/s]#015Downloading:  21%|██        | 94.1M/445M [00:01<00:05, 63.3MB/s]#015Downloading:  23%|██▎       | 100M/445M [00:01<00:05, 63.2MB/s] #015Downloading:  24%|██▍       | 107M/445M [00:01<00:05, 63.3MB/s]#015Downloading:  25%|██▌       | 113M/445M [00:01<00:05, 63.3MB/s]#015Downloading:  27%|██▋       | 119M/445M [00:01<00:05, 63.4MB/s]#015Downloading:  28%|██▊       | 126M/445M [00:02<00:05, 63.1MB/s]#015Downloading:  30%|██▉       | 132M/445M [00:02<00:04, 63.0MB/s]#015Downloading:  31%|███       | 138M/445M [00:02<00:04, 63.0MB/s]#015Downloading:  33%|███▎      | 145M/445M [00:02<00:04, 62.9MB/s]#015Downloading:  34%|███▍      | 151M/445M [00:02<00:04, 63.0MB/s]#015Downloading:  35%|███▌      | 157M/445M [00:02<00:04, 63.1MB/s]#015Downloading:  37%|███▋      | 164M/445M [00:02<00:04, 63.0MB/s]#015Downloading:  38%|███▊      | 170M/445M [00:02<00:04, 63.1MB/s]#015Downloading:  40%|███▉      | 176M/445M [00:02<00:04, 63.1MB/s]#015Downloading:  41%|████      | 183M/445M [00:02<00:04, 63.1MB/s]#015Downloading:  42%|████▏     | 189M/445M [00:03<00:04, 62.9MB/s]#015Downloading:  44%|████▍     | 195M/445M [00:03<00:03, 62.7MB/s]#015Downloading:  45%|████▌     | 202M/445M [00:03<00:03, 62.8MB/s]#015Downloading:  47%|████▋     | 208M/445M [00:03<00:03, 63.2MB/s]#015Downloading:  48%|████▊     | 215M/445M [00:03<00:03, 65.6MB/s]#015Downloading:  50%|█████     | 223M/445M [00:03<00:03, 69.8MB/s]#015Downloading:  52%|█████▏    | 231M/445M [00:03<00:02, 73.1MB/s]#015Downloading:  54%|█████▍    | 239M/445M [00:03<00:02, 75.3MB/s]#015Downloading:  56%|█████▌    | 247M/445M [00:03<00:02, 77.1MB/s]#015Downloading:  57%|█████▋    | 255M/445M [00:03<00:02, 78.4MB/s]#015Downloading:  59%|█████▉    | 264M/445M [00:04<00:02, 79.2MB/s]#015Downloading:  61%|██████    | 272M/445M [00:04<00:02, 79.8MB/s]#015Downloading:  63%|██████▎   | 280M/445M [00:04<00:02, 80.3MB/s]#015Downloading:  65%|██████▍   | 288M/445M [00:04<00:01, 80.6MB/s]#015Downloading:  67%|██████▋   | 296M/445M [00:04<00:01, 81.0MB/s]#015Downloading:  68%|██████▊   | 304M/445M [00:04<00:01, 77.4MB/s]#015Downloading:  70%|███████   | 312M/445M [00:04<00:01, 73.6MB/s]#015Downloading:  72%|███████▏  | 319M/445M [00:04<00:01, 70.8MB/s]#015Downloading:  73%|███████▎  | 327M/445M [00:04<00:01, 69.7MB/s]#015Downloading:  75%|███████▍  | 334M/445M [00:04<00:01, 68.5MB/s]#015Downloading:  77%|███████▋  | 340M/445M [00:05<00:01, 67.9MB/s]#015Downloading:  78%|███████▊  | 347M/445M [00:05<00:01, 67.3MB/s]#015Downloading:  80%|███████▉  | 354M/445M [00:05<00:01, 67.0MB/s]#015Downloading:  81%|████████  | 361M/445M [00:05<00:01, 66.7MB/s]#015Downloading:  83%|████████▎ | 367M/445M [00:05<00:01, 66.5MB/s]#015Downloading:  84%|████████▍ | 374M/445M [00:05<00:01, 66.3MB/s]#015Downloading:  86%|████████▌ | 381M/445M [00:05<00:00, 66.1MB/s]#015Downloading:  87%|████████▋ | 387M/445M [00:05<00:00, 65.7MB/s]#015Downloading:  89%|████████▊ | 394M/445M [00:05<00:00, 65.7MB/s]#015Downloading:  90%|████████▉ | 400M/445M [00:05<00:00, 65.7MB/s]#015Downloading:  91%|█████████▏| 407M/445M [00:06<00:00, 65.7MB/s]#015Downloading:  93%|█████████▎| 414M/445M [00:06<00:00, 65.7MB/s]#015Downloading:  94%|█████████▍| 420M/445M [00:06<00:00, 65.8MB/s]#015Downloading:  96%|█████████▌| 427M/445M [00:06<00:00, 65.6MB/s]#015Downloading:  97%|█████████▋| 433M/445M [00:06<00:00, 65.6MB/s]#015Downloading:  99%|█████████▉| 440M/445M [00:06<00:00, 65.8MB/s]#015Downloading: 100%|██████████| 445M/445M [00:06<00:00, 66.8MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1669] 2022-01-15 18:12:45,386 >> storing https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1677] 2022-01-15 18:12:45,386 >> creating metadata file for /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1279] 2022-01-15 18:12:45,386 >> loading weights file https://huggingface.co/dbmdz/bert-base-turkish-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2cde030ef941e010ac168a81f127fc4dc920c9402403f7adf8377728e30d898e.a4faeaa8e5e3c20f3a8a00668f2250edfb32c44bda6d7b9dc9e75aaddef93dcc\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1515] 2022-01-15 18:12:47,084 >> Some weights of the model checkpoint at dbmdz/bert-base-turkish-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1526] 2022-01-15 18:12:47,085 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on train dataset:   0%|          | 0/9 [00:00<?, ?ba/s]#015Running tokenizer on train dataset:  11%|█         | 1/9 [00:00<00:05,  1.47ba/s]#015Running tokenizer on train dataset:  22%|██▏       | 2/9 [00:01<00:04,  1.75ba/s]#015Running tokenizer on train dataset:  33%|███▎      | 3/9 [00:01<00:03,  1.80ba/s]#015Running tokenizer on train dataset:  44%|████▍     | 4/9 [00:02<00:02,  2.01ba/s]#015Running tokenizer on train dataset:  56%|█████▌    | 5/9 [00:02<00:01,  2.16ba/s]#015Running tokenizer on train dataset:  67%|██████▋   | 6/9 [00:02<00:01,  2.52ba/s]#015Running tokenizer on train dataset:  78%|███████▊  | 7/9 [00:03<00:00,  2.51ba/s]#015Running tokenizer on train dataset:  89%|████████▉ | 8/9 [00:03<00:00,  2.41ba/s]#015Running tokenizer on train dataset: 100%|██████████| 9/9 [00:03<00:00,  3.00ba/s]#015Running tokenizer on train dataset: 100%|██████████| 9/9 [00:03<00:00,  2.37ba/s]\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:02<00:00,  2.26s/ba]#015Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:02<00:00,  2.26s/ba]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s]#015Downloading: 4.51kB [00:00, 3.98MB/s]                   \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]#015Downloading: 3.31kB [00:00, 3.14MB/s]                   \u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:414] 2022-01-15 18:12:58,473 >> Using amp fp16 backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1168] 2022-01-15 18:12:58,484 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1169] 2022-01-15 18:12:58,485 >>   Num examples = 11466\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1170] 2022-01-15 18:12:58,485 >>   Num Epochs = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1171] 2022-01-15 18:12:58,485 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1172] 2022-01-15 18:12:58,485 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1173] 2022-01-15 18:12:58,485 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1174] 2022-01-15 18:12:58,485 >>   Total optimization steps = 718\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/718 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m#015  0%|          | 1/718 [00:21<4:15:24, 21.37s/it]#015  0%|          | 2/718 [00:21<1:47:51,  9.04s/it]#015  0%|          | 3/718 [00:22<1:00:35,  5.08s/it]#015  1%|          | 4/718 [00:22<38:20,  3.22s/it]  #015  1%|          | 5/718 [00:22<26:00,  2.19s/it]#015  1%|          | 6/718 [00:23<18:35,  1.57s/it]#015  1%|          | 7/718 [00:23<14:05,  1.19s/it]#015  1%|          | 8/718 [00:24<11:04,  1.07it/s]#015  1%|▏         | 9/718 [00:24<08:53,  1.33it/s]#015  1%|▏         | 10/718 [00:24<07:29,  1.57it/s]#015  2%|▏         | 11/718 [00:25<06:35,  1.79it/s]#015  2%|▏         | 12/718 [00:25<06:02,  1.95it/s]#015  2%|▏         | 13/718 [00:25<05:26,  2.16it/s]#015  2%|▏         | 14/718 [00:26<05:08,  2.28it/s]#015  2%|▏         | 15/718 [00:26<04:50,  2.42it/s]#015  2%|▏         | 16/718 [00:27<04:43,  2.47it/s]#015  2%|▏         | 17/718 [00:27<04:51,  2.41it/s]#015  3%|▎         | 18/718 [00:27<04:37,  2.52it/s]#015  3%|▎         | 19/718 [00:28<04:28,  2.61it/s]#015  3%|▎         | 20/718 [00:28<04:20,  2.67it/s]#015  3%|▎         | 21/718 [00:28<04:17,  2.71it/s]#015  3%|▎         | 22/718 [00:29<04:13,  2.75it/s]#015  3%|▎         | 23/718 [00:29<04:27,  2.60it/s]#015  3%|▎         | 24/718 [00:30<04:20,  2.67it/s]#015  3%|▎         | 25/718 [00:30<04:21,  2.65it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m#015  4%|▎         | 26/718 [00:30<04:13,  2.73it/s]#015  4%|▍         | 27/718 [00:31<04:07,  2.80it/s]#015  4%|▍         | 28/718 [00:31<04:14,  2.71it/s]#015  4%|▍         | 29/718 [00:31<04:11,  2.74it/s]#015  4%|▍         | 30/718 [00:32<04:08,  2.77it/s]#015  4%|▍         | 31/718 [00:32<04:08,  2.77it/s]#015  4%|▍         | 32/718 [00:32<04:07,  2.77it/s]#015  5%|▍         | 33/718 [00:33<04:19,  2.64it/s]#015  5%|▍         | 34/718 [00:33<04:14,  2.69it/s]#015  5%|▍         | 35/718 [00:34<04:10,  2.72it/s]#015  5%|▌         | 36/718 [00:34<04:13,  2.69it/s]#015  5%|▌         | 37/718 [00:34<04:13,  2.69it/s]#015  5%|▌         | 38/718 [00:35<04:20,  2.61it/s]#015  5%|▌         | 39/718 [00:35<04:12,  2.69it/s]#015  6%|▌         | 40/718 [00:35<04:08,  2.73it/s]#015  6%|▌         | 41/718 [00:36<04:05,  2.76it/s]#015  6%|▌         | 42/718 [00:36<04:05,  2.76it/s]#015  6%|▌         | 43/718 [00:37<04:18,  2.61it/s]#015  6%|▌         | 44/718 [00:37<04:12,  2.67it/s]#015  6%|▋         | 45/718 [00:37<04:08,  2.71it/s]#015  6%|▋         | 46/718 [00:38<04:04,  2.74it/s]#015  7%|▋         | 47/718 [00:38<04:05,  2.73it/s]#015  7%|▋         | 48/718 [00:38<04:15,  2.62it/s]#015  7%|▋         | 49/718 [00:39<04:09,  2.68it/s]#015  7%|▋         | 50/718 [00:39<04:05,  2.72it/s]#015  7%|▋         | 51/718 [00:39<04:06,  2.71it/s]#015  7%|▋         | 52/718 [00:40<04:03,  2.74it/s]#015  7%|▋         | 53/718 [00:40<04:13,  2.62it/s]#015  8%|▊         | 54/718 [00:41<04:08,  2.67it/s]#015  8%|▊         | 55/718 [00:41<04:04,  2.72it/s]#015  8%|▊         | 56/718 [00:41<04:00,  2.75it/s]#015  8%|▊         | 57/718 [00:42<04:04,  2.71it/s]#015  8%|▊         | 58/718 [00:42<04:14,  2.59it/s]#015  8%|▊         | 59/718 [00:43<04:13,  2.60it/s]#015  8%|▊         | 60/718 [00:43<04:12,  2.60it/s]#015  8%|▊         | 61/718 [00:43<04:08,  2.64it/s]#015  9%|▊         | 62/718 [00:44<04:11,  2.61it/s]#015  9%|▉         | 63/718 [00:44<04:16,  2.55it/s]#015  9%|▉         | 64/718 [00:44<04:09,  2.63it/s]#015  9%|▉         | 65/718 [00:45<04:03,  2.69it/s]#015  9%|▉         | 66/718 [00:45<03:59,  2.72it/s]#015  9%|▉         | 67/718 [00:45<03:56,  2.75it/s]#015  9%|▉         | 68/718 [00:46<04:13,  2.56it/s]#015 10%|▉         | 69/718 [00:46<04:07,  2.62it/s]#015 10%|▉         | 70/718 [00:47<04:01,  2.68it/s]#015 10%|▉         | 71/718 [00:47<03:58,  2.71it/s]#015 10%|█         | 72/718 [00:47<03:56,  2.73it/s]#015 10%|█         | 73/718 [00:48<04:05,  2.63it/s]#015 10%|█         | 74/718 [00:48<04:04,  2.64it/s]#015 10%|█         | 75/718 [00:49<03:59,  2.68it/s]#015 11%|█         | 76/718 [00:49<03:55,  2.72it/s]#015 11%|█         | 77/718 [00:49<03:55,  2.72it/s]#015 11%|█         | 78/718 [00:50<04:04,  2.62it/s]#015 11%|█         | 79/718 [00:50<04:03,  2.63it/s]#015 11%|█         | 80/718 [00:50<03:56,  2.70it/s]#015 11%|█▏        | 81/718 [00:51<03:52,  2.73it/s]#015 11%|█▏        | 82/718 [00:51<03:49,  2.77it/s]#015 12%|█▏        | 83/718 [00:51<03:53,  2.72it/s]#015 12%|█▏        | 84/718 [00:52<04:01,  2.63it/s]#015 12%|█▏        | 85/718 [00:52<03:55,  2.69it/s]#015 12%|█▏        | 86/718 [00:53<03:51,  2.73it/s]#015 12%|█▏        | 87/718 [00:53<03:47,  2.77it/s]#015 12%|█▏        | 88/718 [00:53<03:47,  2.77it/s]#015 12%|█▏        | 89/718 [00:54<03:57,  2.65it/s]#015 13%|█▎        | 90/718 [00:54<03:52,  2.71it/s]#015 13%|█▎        | 91/718 [00:54<03:53,  2.69it/s]#015 13%|█▎        | 92/718 [00:55<03:48,  2.74it/s]#015 13%|█▎        | 93/718 [00:55<03:51,  2.70it/s]#015 13%|█▎        | 94/718 [00:56<03:58,  2.62it/s]#015 13%|█▎        | 95/718 [00:56<03:51,  2.69it/s]#015 13%|█▎        | 96/718 [00:56<03:49,  2.71it/s]#015 14%|█▎        | 97/718 [00:57<03:46,  2.75it/s]#015 14%|█▎        | 98/718 [00:57<03:42,  2.78it/s]#015 14%|█▍        | 99/718 [00:57<03:52,  2.66it/s]#015 14%|█▍        | 100/718 [00:58<03:53,  2.65it/s]#015 14%|█▍        | 101/718 [00:58<03:54,  2.63it/s]#015 14%|█▍        | 102/718 [00:59<03:50,  2.68it/s]#015 14%|█▍        | 103/718 [00:59<03:45,  2.73it/s]#015 14%|█▍        | 104/718 [00:59<03:57,  2.59it/s]#015 15%|█▍        | 105/718 [01:00<03:51,  2.65it/s]#015 15%|█▍        | 106/718 [01:00<03:52,  2.63it/s]#015 15%|█▍        | 107/718 [01:00<03:51,  2.63it/s]#015 15%|█▌        | 108/718 [01:01<03:50,  2.65it/s]#015 15%|█▌        | 109/718 [01:01<03:56,  2.57it/s]#015 15%|█▌        | 110/718 [01:02<03:56,  2.57it/s]#015 15%|█▌        | 111/718 [01:02<03:49,  2.65it/s]#015 16%|█▌        | 112/718 [01:02<03:48,  2.66it/s]#015 16%|█▌        | 113/718 [01:03<03:47,  2.66it/s]#015 16%|█▌        | 114/718 [01:03<03:55,  2.57it/s]#015 16%|█▌        | 115/718 [01:04<03:54,  2.57it/s]#015 16%|█▌        | 116/718 [01:04<03:51,  2.60it/s]#015 16%|█▋        | 117/718 [01:04<03:51,  2.60it/s]#015 16%|█▋        | 118/718 [01:05<03:46,  2.65it/s]#015 17%|█▋        | 119/718 [01:05<03:55,  2.54it/s]#015 17%|█▋        | 120/718 [01:05<03:50,  2.60it/s]#015 17%|█▋        | 121/718 [01:06<03:45,  2.65it/s]#015 17%|█▋        | 122/718 [01:06<03:39,  2.72it/s]#015 17%|█▋        | 123/718 [01:07<03:40,  2.70it/s]#015 17%|█▋        | 124/718 [01:07<03:51,  2.57it/s]#015 17%|█▋        | 125/718 [01:07<03:46,  2.62it/s]#015 18%|█▊        | 126/718 [01:08<03:40,  2.68it/s]#015 18%|█▊        | 127/718 [01:08<03:37,  2.72it/s]#015 18%|█▊        | 128/718 [01:08<03:35,  2.74it/s]#015 18%|█▊        | 129/718 [01:09<03:43,  2.63it/s]#015 18%|█▊        | 130/718 [01:09<03:38,  2.69it/s]#015 18%|█▊        | 131/718 [01:10<03:35,  2.73it/s]#015 18%|█▊        | 132/718 [01:10<03:32,  2.75it/s]#015 19%|█▊        | 133/718 [01:10<03:30,  2.77it/s]#015 19%|█▊        | 134/718 [01:11<03:45,  2.59it/s]#015 19%|█▉        | 135/718 [01:11<03:40,  2.65it/s]#015 19%|█▉        | 136/718 [01:11<03:38,  2.66it/s]#015 19%|█▉        | 137/718 [01:12<03:37,  2.67it/s]#015 19%|█▉        | 138/718 [01:12<03:33,  2.71it/s]#015 19%|█▉        | 139/718 [01:13<03:44,  2.58it/s]#015 19%|█▉        | 140/718 [01:13<03:38,  2.64it/s]#015 20%|█▉        | 141/718 [01:13<03:34,  2.69it/s]#015 20%|█▉        | 142/718 [01:14<03:30,  2.74it/s]#015 20%|█▉        | 143/718 [01:14<03:28,  2.76it/s]#015 20%|██        | 144/718 [01:14<03:26,  2.78it/s]#015 20%|██        | 145/718 [01:15<03:35,  2.65it/s]#015 20%|██        | 146/718 [01:15<03:33,  2.67it/s]#015 20%|██        | 147/718 [01:15<03:30,  2.72it/s]#015 21%|██        | 148/718 [01:16<03:27,  2.74it/s]#015 21%|██        | 149/718 [01:16<03:25,  2.77it/s]#015 21%|██        | 150/718 [01:17<03:36,  2.63it/s]#015 21%|██        | 151/718 [01:17<03:32,  2.67it/s]#015 21%|██        | 152/718 [01:17<03:29,  2.71it/s]#015 21%|██▏       | 153/718 [01:18<03:30,  2.69it/s]#015 21%|██▏       | 154/718 [01:18<03:26,  2.73it/s]#015 22%|██▏       | 155/718 [01:18<03:36,  2.61it/s]#015 22%|██▏       | 156/718 [01:19<03:35,  2.60it/s]#015 22%|██▏       | 157/718 [01:19<03:30,  2.66it/s]#015 22%|██▏       | 158/718 [01:20<03:26,  2.71it/s]#015 22%|██▏       | 159/718 [01:20<03:24,  2.73it/s]#015 22%|██▏       | 160/718 [01:20<03:33,  2.61it/s]#015 22%|██▏       | 161/718 [01:21<03:28,  2.67it/s]#015 23%|██▎       | 162/718 [01:21<03:24,  2.72it/s]#015 23%|██▎       | 163/718 [01:21<03:23,  2.73it/s]#015 23%|██▎       | 164/718 [01:22<03:19,  2.77it/s]#015 23%|██▎       | 165/718 [01:22<03:28,  2.65it/s]#015 23%|██▎       | 166/718 [01:23<03:25,  2.68it/s]#015 23%|██▎       | 167/718 [01:23<03:26,  2.66it/s]#015 23%|██▎       | 168/718 [01:23<03:23,  2.70it/s]#015 24%|██▎       | 169/718 [01:24<03:21,  2.73it/s]#015 24%|██▎       | 170/718 [01:24<03:31,  2.59it/s]#015 24%|██▍       | 171/718 [01:24<03:25,  2.66it/s]#015 24%|██▍       | 172/718 [01:25<03:25,  2.66it/s]#015 24%|██▍       | 173/718 [01:25<03:19,  2.73it/s]#015 24%|██▍       | 174/718 [01:25<03:16,  2.76it/s]#015 24%|██▍       | 175/718 [01:26<03:26,  2.63it/s]#015 25%|██▍       | 176/718 [01:26<03:24,  2.64it/s]#015 25%|██▍       | 177/718 [01:27<03:20,  2.69it/s]#015 25%|██▍       | 178/718 [01:27<03:17,  2.74it/s]#015 25%|██▍       | 179/718 [01:27<03:17,  2.73it/s]#015 25%|██▌       | 180/718 [01:28<03:26,  2.60it/s]#015 25%|██▌       | 181/718 [01:28<03:21,  2.66it/s]#015 25%|██▌       | 182/718 [01:29<03:18,  2.70it/s]#015 25%|██▌       | 183/718 [01:29<03:14,  2.75it/s]#015 26%|██▌       | 184/718 [01:29<03:14,  2.75it/s]#015 26%|██▌       | 185/718 [01:30<03:23,  2.62it/s]#015 26%|██▌       | 186/718 [01:30<03:19,  2.67it/s]#015 26%|██▌       | 187/718 [01:30<03:17,  2.69it/s]#015 26%|██▌       | 188/718 [01:31<03:14,  2.72it/s]#015 26%|██▋       | 189/718 [01:31<03:12,  2.74it/s]#015 26%|██▋       | 190/718 [01:31<03:20,  2.63it/s]#015 27%|██▋       | 191/718 [01:32<03:16,  2.69it/s]#015 27%|██▋       | 192/718 [01:32<03:14,  2.71it/s]#015 27%|██▋       | 193/718 [01:33<03:11,  2.74it/s]#015 27%|██▋       | 194/718 [01:33<03:08,  2.78it/s]#015 27%|██▋       | 195/718 [01:33<03:15,  2.67it/s]#015 27%|██▋       | 196/718 [01:34<03:13,  2.70it/s]#015 27%|██▋       | 197/718 [01:34<03:09,  2.75it/s]#015 28%|██▊       | 198/718 [01:34<03:08,  2.77it/s]#015 28%|██▊       | 199/718 [01:35<03:06,  2.79it/s]#015 28%|██▊       | 200/718 [01:35<03:18,  2.61it/s]#015 28%|██▊       | 201/718 [01:36<03:18,  2.61it/s]#015 28%|██▊       | 202/718 [01:36<03:14,  2.65it/s]#015 28%|██▊       | 203/718 [01:36<03:09,  2.71it/s]#015 28%|██▊       | 204/718 [01:37<03:05,  2.77it/s]#015 29%|██▊       | 205/718 [01:37<03:03,  2.80it/s]#015 29%|██▊       | 206/718 [01:37<03:12,  2.66it/s]#015 29%|██▉       | 207/718 [01:38<03:08,  2.71it/s]#015 29%|██▉       | 208/718 [01:38<03:06,  2.73it/s]#015 29%|██▉       | 209/718 [01:38<03:05,  2.75it/s]#015 29%|██▉       | 210/718 [01:39<03:04,  2.76it/s]#015 29%|██▉       | 211/718 [01:39<03:13,  2.63it/s]#015 30%|██▉       | 212/718 [01:40<03:09,  2.68it/s]#015 30%|██▉       | 213/718 [01:40<03:07,  2.69it/s]#015 30%|██▉       | 214/718 [01:40<03:05,  2.71it/s]#015 30%|██▉       | 215/718 [01:41<03:07,  2.69it/s]#015 30%|███       | 216/718 [01:41<03:14,  2.58it/s]#015 30%|███       | 217/718 [01:41<03:09,  2.64it/s]#015 30%|███       | 218/718 [01:42<03:04,  2.71it/s]#015 31%|███       | 219/718 [01:42<03:01,  2.74it/s]#015 31%|███       | 220/718 [01:43<02:59,  2.77it/s]#015 31%|███       | 221/718 [01:43<03:08,  2.63it/s]#015 31%|███       | 222/718 [01:43<03:04,  2.68it/s]#015 31%|███       | 223/718 [01:44<03:02,  2.72it/s]#015 31%|███       | 224/718 [01:44<02:59,  2.75it/s]#015 31%|███▏      | 225/718 [01:44<02:58,  2.76it/s]#015 31%|███▏      | 226/718 [01:45<03:09,  2.59it/s]#015 32%|███▏      | 227/718 [01:45<03:05,  2.64it/s]#015 32%|███▏      | 228/718 [01:46<03:02,  2.69it/s]#015 32%|███▏      | 229/718 [01:46<02:59,  2.72it/s]#015 32%|███▏      | 230/718 [01:46<02:58,  2.74it/s]#015 32%|███▏      | 231/718 [01:47<03:08,  2.59it/s]#015 32%|███▏      | 232/718 [01:47<03:03,  2.65it/s]#015 32%|███▏      | 233/718 [01:47<02:59,  2.70it/s]#015 33%|███▎      | 234/718 [01:48<02:57,  2.73it/s]#015 33%|███▎      | 235/718 [01:48<02:54,  2.76it/s]#015 33%|███▎      | 236/718 [01:49<03:03,  2.62it/s]#015 33%|███▎      | 237/718 [01:49<03:00,  2.67it/s]#015 33%|███▎      | 238/718 [01:49<02:56,  2.73it/s]#015 33%|███▎      | 239/718 [01:50<02:54,  2.75it/s]#015 33%|███▎      | 240/718 [01:50<02:53,  2.76it/s]#015 34%|███▎      | 241/718 [01:50<03:01,  2.63it/s]#015 34%|███▎      | 242/718 [01:51<02:59,  2.65it/s]#015 34%|███▍      | 243/718 [01:51<02:56,  2.68it/s]#015 34%|███▍      | 244/718 [01:51<02:54,  2.72it/s]#015 34%|███▍      | 245/718 [01:52<02:53,  2.73it/s]#015 34%|███▍      | 246/718 [01:52<02:59,  2.62it/s]#015 34%|███▍      | 247/718 [01:53<02:55,  2.68it/s]#015 35%|███▍      | 248/718 [01:53<02:53,  2.71it/s]#015 35%|███▍      | 249/718 [01:53<02:52,  2.72it/s]#015 35%|███▍      | 250/718 [01:54<02:50,  2.75it/s]#015 35%|███▍      | 251/718 [01:54<02:57,  2.63it/s]#015 35%|███▌      | 252/718 [01:54<02:54,  2.67it/s]#015 35%|███▌      | 253/718 [01:55<02:50,  2.72it/s]#015 35%|███▌      | 254/718 [01:55<02:49,  2.74it/s]#015 36%|███▌      | 255/718 [01:56<02:47,  2.76it/s]#015 36%|███▌      | 256/718 [01:56<02:54,  2.65it/s]#015 36%|███▌      | 257/718 [01:56<02:51,  2.69it/s]#015 36%|███▌      | 258/718 [01:57<02:48,  2.72it/s]#015 36%|███▌      | 259/718 [01:57<02:47,  2.74it/s]#015 36%|███▌      | 260/718 [01:57<02:45,  2.76it/s]#015 36%|███▋      | 261/718 [01:58<02:54,  2.62it/s]#015 36%|███▋      | 262/718 [01:58<02:51,  2.66it/s]#015 37%|███▋      | 263/718 [01:59<02:47,  2.71it/s]#015 37%|███▋      | 264/718 [01:59<02:45,  2.75it/s]#015 37%|███▋      | 265/718 [01:59<02:43,  2.77it/s]#015 37%|███▋      | 266/718 [02:00<02:42,  2.78it/s]#015 37%|███▋      | 267/718 [02:00<02:50,  2.64it/s]#015 37%|███▋      | 268/718 [02:00<02:47,  2.69it/s]#015 37%|███▋      | 269/718 [02:01<02:44,  2.72it/s]#015 38%|███▊      | 270/718 [02:01<02:43,  2.74it/s]#015 38%|███▊      | 271/718 [02:01<02:40,  2.78it/s]#015 38%|███▊      | 272/718 [02:02<02:49,  2.63it/s]#015 38%|███▊      | 273/718 [02:02<02:48,  2.65it/s]#015 38%|███▊      | 274/718 [02:03<02:44,  2.70it/s]#015 38%|███▊      | 275/718 [02:03<02:43,  2.71it/s]#015 38%|███▊      | 276/718 [02:03<02:42,  2.71it/s]#015 39%|███▊      | 277/718 [02:04<02:52,  2.56it/s]#015 39%|███▊      | 278/718 [02:04<02:48,  2.61it/s]#015 39%|███▉      | 279/718 [02:04<02:43,  2.69it/s]#015 39%|███▉      | 280/718 [02:05<02:41,  2.71it/s]#015 39%|███▉      | 281/718 [02:05<02:39,  2.74it/s]#015 39%|███▉      | 282/718 [02:06<02:49,  2.57it/s]#015 39%|███▉      | 283/718 [02:06<02:44,  2.65it/s]#015 40%|███▉      | 284/718 [02:06<02:40,  2.70it/s]#015 40%|███▉      | 285/718 [02:07<02:40,  2.70it/s]#015 40%|███▉      | 286/718 [02:07<02:38,  2.72it/s]#015 40%|███▉      | 287/718 [02:07<02:45,  2.60it/s]#015 40%|████      | 288/718 [02:08<02:42,  2.65it/s]#015 40%|████      | 289/718 [02:08<02:39,  2.69it/s]#015 40%|████      | 290/718 [02:09<02:37,  2.72it/s]#015 41%|████      | 291/718 [02:09<02:36,  2.73it/s]#015 41%|████      | 292/718 [02:09<02:45,  2.58it/s]#015 41%|████      | 293/718 [02:10<02:40,  2.65it/s]#015 41%|████      | 294/718 [02:10<02:38,  2.68it/s]#015 41%|████      | 295/718 [02:10<02:35,  2.72it/s]#015 41%|████      | 296/718 [02:11<02:34,  2.74it/s]#015 41%|████▏     | 297/718 [02:11<02:42,  2.60it/s]#015 42%|████▏     | 298/718 [02:12<02:38,  2.65it/s]#015 42%|████▏     | 299/718 [02:12<02:35,  2.69it/s]#015 42%|████▏     | 300/718 [02:12<02:33,  2.73it/s]#015 42%|████▏     | 301/718 [02:13<02:35,  2.69it/s]#015 42%|████▏     | 302/718 [02:13<02:41,  2.58it/s]#015 42%|████▏     | 303/718 [02:13<02:36,  2.64it/s]#015 42%|████▏     | 304/718 [02:14<02:34,  2.68it/s]#015 42%|████▏     | 305/718 [02:14<02:31,  2.73it/s]#015 43%|████▎     | 306/718 [02:15<02:30,  2.75it/s]#015 43%|████▎     | 307/718 [02:15<02:37,  2.62it/s]#015 43%|████▎     | 308/718 [02:15<02:33,  2.67it/s]#015 43%|████▎     | 309/718 [02:16<02:32,  2.69it/s]#015 43%|████▎     | 310/718 [02:16<02:29,  2.72it/s]#015 43%|████▎     | 311/718 [02:16<02:28,  2.75it/s]#015 43%|████▎     | 312/718 [02:17<02:34,  2.63it/s]#015 44%|████▎     | 313/718 [02:17<02:31,  2.68it/s]#015 44%|████▎     | 314/718 [02:18<02:28,  2.72it/s]#015 44%|████▍     | 315/718 [02:18<02:27,  2.74it/s]#015 44%|████▍     | 316/718 [02:18<02:25,  2.76it/s]#015 44%|████▍     | 317/718 [02:19<02:31,  2.65it/s]#015 44%|████▍     | 318/718 [02:19<02:29,  2.67it/s]#015 44%|████▍     | 319/718 [02:19<02:28,  2.69it/s]#015 45%|████▍     | 320/718 [02:20<02:26,  2.72it/s]#015 45%|████▍     | 321/718 [02:20<02:24,  2.74it/s]#015 \u001b[0m\n",
      "\u001b[34m45%|████▍     | 322/718 [02:21<02:34,  2.56it/s]#015 45%|████▍     | 323/718 [02:21<02:31,  2.61it/s]#015 45%|████▌     | 324/718 [02:21<02:29,  2.63it/s]#015 45%|████▌     | 325/718 [02:22<02:27,  2.66it/s]#015 45%|████▌     | 326/718 [02:22<02:26,  2.68it/s]#015 46%|████▌     | 327/718 [02:22<02:25,  2.69it/s]#015 46%|████▌     | 328/718 [02:23<02:32,  2.56it/s]#015 46%|████▌     | 329/718 [02:23<02:28,  2.62it/s]#015 46%|████▌     | 330/718 [02:24<02:27,  2.63it/s]#015 46%|████▌     | 331/718 [02:24<02:25,  2.66it/s]#015 46%|████▌     | 332/718 [02:24<02:24,  2.67it/s]#015 46%|████▋     | 333/718 [02:25<02:33,  2.51it/s]#015 47%|████▋     | 334/718 [02:25<02:29,  2.57it/s]#015 47%|████▋     | 335/718 [02:25<02:26,  2.62it/s]#015 47%|████▋     | 336/718 [02:26<02:23,  2.67it/s]#015 47%|████▋     | 337/718 [02:26<02:22,  2.68it/s]#015 47%|████▋     | 338/718 [02:27<02:29,  2.55it/s]#015 47%|████▋     | 339/718 [02:27<02:25,  2.60it/s]#015 47%|████▋     | 340/718 [02:27<02:23,  2.63it/s]#015 47%|████▋     | 341/718 [02:28<02:22,  2.65it/s]#015 48%|████▊     | 342/718 [02:28<02:21,  2.66it/s]#015 48%|████▊     | 343/718 [02:29<02:28,  2.53it/s]#015 48%|████▊     | 344/718 [02:29<02:25,  2.58it/s]#015 48%|████▊     | 345/718 [02:29<02:22,  2.61it/s]#015 48%|████▊     | 346/718 [02:30<02:20,  2.64it/s]#015 48%|████▊     | 347/718 [02:30<02:19,  2.66it/s]#015 48%|████▊     | 348/718 [02:31<02:26,  2.52it/s]#015 49%|████▊     | 349/718 [02:31<02:22,  2.60it/s]#015 49%|████▊     | 350/718 [02:31<02:19,  2.63it/s]#015 49%|████▉     | 351/718 [02:32<02:17,  2.66it/s]#015 49%|████▉     | 352/718 [02:32<02:17,  2.67it/s]#015 49%|████▉     | 353/718 [02:32<02:25,  2.52it/s]#015 49%|████▉     | 354/718 [02:33<02:21,  2.57it/s]#015 49%|████▉     | 355/718 [02:33<02:18,  2.62it/s]#015 50%|████▉     | 356/718 [02:34<02:16,  2.65it/s]#015 50%|████▉     | 357/718 [02:34<02:16,  2.64it/s]#015 50%|████▉     | 358/718 [02:34<02:22,  2.52it/s]#015 50%|█████     | 359/718 [02:35<02:45,  2.17it/s]#015 50%|█████     | 360/718 [02:35<02:36,  2.28it/s]#015 50%|█████     | 361/718 [02:36<02:27,  2.42it/s]#015 50%|█████     | 362/718 [02:36<02:20,  2.53it/s]#015 51%|█████     | 363/718 [02:36<02:23,  2.47it/s]#015 51%|█████     | 364/718 [02:37<02:19,  2.54it/s]#015 51%|█████     | 365/718 [02:37<02:15,  2.60it/s]#015 51%|█████     | 366/718 [02:38<02:14,  2.62it/s]#015 51%|█████     | 367/718 [02:38<02:10,  2.68it/s]#015 51%|█████▏    | 368/718 [02:38<02:11,  2.66it/s]#015 51%|█████▏    | 369/718 [02:39<02:16,  2.57it/s]#015 52%|█████▏    | 370/718 [02:39<02:11,  2.64it/s]#015 52%|█████▏    | 371/718 [02:39<02:08,  2.70it/s]#015 52%|█████▏    | 372/718 [02:40<02:06,  2.74it/s]#015 52%|█████▏    | 373/718 [02:40<02:04,  2.76it/s]#015 52%|█████▏    | 374/718 [02:41<02:10,  2.64it/s]#015 52%|█████▏    | 375/718 [02:41<02:06,  2.70it/s]#015 52%|█████▏    | 376/718 [02:41<02:04,  2.74it/s]#015 53%|█████▎    | 377/718 [02:42<02:05,  2.73it/s]#015 53%|█████▎    | 378/718 [02:42<02:05,  2.70it/s]#015 53%|█████▎    | 379/718 [02:42<02:10,  2.59it/s]#015 53%|█████▎    | 380/718 [02:43<02:08,  2.64it/s]#015 53%|█████▎    | 381/718 [02:43<02:04,  2.70it/s]#015 53%|█████▎    | 382/718 [02:44<02:05,  2.68it/s]#015 53%|█████▎    | 383/718 [02:44<02:03,  2.71it/s]#015 53%|█████▎    | 384/718 [02:44<02:10,  2.56it/s]#015 54%|█████▎    | 385/718 [02:45<02:05,  2.65it/s]#015 54%|█████▍    | 386/718 [02:45<02:04,  2.67it/s]#015 54%|█████▍    | 387/718 [02:45<02:01,  2.72it/s]#015 54%|█████▍    | 388/718 [02:46<01:59,  2.76it/s]#015 54%|█████▍    | 389/718 [02:46<02:07,  2.57it/s]#015 54%|█████▍    | 390/718 [02:47<02:05,  2.61it/s]#015 54%|█████▍    | 391/718 [02:47<02:02,  2.68it/s]#015 55%|█████▍    | 392/718 [02:47<01:59,  2.73it/s]#015 55%|█████▍    | 393/718 [02:48<01:57,  2.77it/s]#015 55%|█████▍    | 394/718 [02:48<02:03,  2.62it/s]#015 55%|█████▌    | 395/718 [02:48<02:00,  2.69it/s]#015 55%|█████▌    | 396/718 [02:49<02:00,  2.68it/s]#015 55%|█████▌    | 397/718 [02:49<01:57,  2.72it/s]#015 55%|█████▌    | 398/718 [02:49<01:56,  2.76it/s]#015 56%|█████▌    | 399/718 [02:50<02:02,  2.60it/s]#015 56%|█████▌    | 400/718 [02:50<02:02,  2.60it/s]#015 56%|█████▌    | 401/718 [02:51<01:58,  2.67it/s]#015 56%|█████▌    | 402/718 [02:51<01:57,  2.68it/s]#015 56%|█████▌    | 403/718 [02:51<01:55,  2.73it/s]#015 56%|█████▋    | 404/718 [02:52<02:00,  2.61it/s]#015 56%|█████▋    | 405/718 [02:52<01:59,  2.61it/s]#015 57%|█████▋    | 406/718 [02:53<01:59,  2.61it/s]#015 57%|█████▋    | 407/718 [02:53<01:56,  2.68it/s]#015 57%|█████▋    | 408/718 [02:53<01:54,  2.71it/s]#015 57%|█████▋    | 409/718 [02:54<01:58,  2.60it/s]#015 57%|█████▋    | 410/718 [02:54<01:57,  2.63it/s]#015 57%|█████▋    | 411/718 [02:54<01:54,  2.69it/s]#015 57%|█████▋    | 412/718 [02:55<01:51,  2.74it/s]#015 58%|█████▊    | 413/718 [02:55<01:50,  2.76it/s]#015 58%|█████▊    | 414/718 [02:56<01:55,  2.63it/s]#015 58%|█████▊    | 415/718 [02:56<01:53,  2.68it/s]#015 58%|█████▊    | 416/718 [02:56<01:54,  2.65it/s]#015 58%|█████▊    | 417/718 [02:57<01:51,  2.71it/s]#015 58%|█████▊    | 418/718 [02:57<01:49,  2.74it/s]#015 58%|█████▊    | 419/718 [02:57<01:56,  2.56it/s]#015 58%|█████▊    | 420/718 [02:58<01:52,  2.64it/s]#015 59%|█████▊    | 421/718 [02:58<01:49,  2.71it/s]#015 59%|█████▉    | 422/718 [02:58<01:48,  2.74it/s]#015 59%|█████▉    | 423/718 [02:59<01:46,  2.77it/s]#015 59%|█████▉    | 424/718 [02:59<01:49,  2.68it/s]#015 59%|█████▉    | 425/718 [03:00<01:47,  2.72it/s]#015 59%|█████▉    | 426/718 [03:00<01:45,  2.76it/s]#015 59%|█████▉    | 427/718 [03:00<01:44,  2.78it/s]#015 60%|█████▉    | 428/718 [03:01<01:43,  2.79it/s]#015 60%|█████▉    | 429/718 [03:01<01:42,  2.81it/s]#015 60%|█████▉    | 430/718 [03:01<01:48,  2.66it/s]#015 60%|██████    | 431/718 [03:02<01:48,  2.64it/s]#015 60%|██████    | 432/718 [03:02<01:46,  2.69it/s]#015 60%|██████    | 433/718 [03:03<01:47,  2.65it/s]#015 60%|██████    | 434/718 [03:03<01:49,  2.60it/s]#015 61%|██████    | 435/718 [03:03<01:54,  2.47it/s]#015 61%|██████    | 436/718 [03:04<01:51,  2.53it/s]#015 61%|██████    | 437/718 [03:04<01:49,  2.57it/s]#015 61%|██████    | 438/718 [03:05<01:46,  2.64it/s]#015 61%|██████    | 439/718 [03:05<01:43,  2.68it/s]#015 61%|██████▏   | 440/718 [03:05<01:48,  2.56it/s]#015 61%|██████▏   | 441/718 [03:06<01:47,  2.57it/s]#015 62%|██████▏   | 442/718 [03:06<01:43,  2.66it/s]#015 62%|██████▏   | 443/718 [03:06<01:41,  2.70it/s]#015 62%|██████▏   | 444/718 [03:07<01:40,  2.73it/s]#015 62%|██████▏   | 445/718 [03:07<01:46,  2.56it/s]#015 62%|██████▏   | 446/718 [03:08<01:43,  2.64it/s]#015 62%|██████▏   | 447/718 [03:08<01:40,  2.69it/s]#015 62%|██████▏   | 448/718 [03:08<01:39,  2.71it/s]#015 63%|██████▎   | 449/718 [03:09<01:39,  2.70it/s]#015 63%|██████▎   | 450/718 [03:09<01:43,  2.58it/s]#015 63%|██████▎   | 451/718 [03:09<01:42,  2.60it/s]#015 63%|██████▎   | 452/718 [03:10<01:40,  2.65it/s]#015 63%|██████▎   | 453/718 [03:10<01:38,  2.70it/s]#015 63%|██████▎   | 454/718 [03:11<01:37,  2.72it/s]#015 63%|██████▎   | 455/718 [03:11<01:41,  2.60it/s]#015 64%|██████▎   | 456/718 [03:11<01:39,  2.65it/s]#015 64%|██████▎   | 457/718 [03:12<01:37,  2.68it/s]#015 64%|██████▍   | 458/718 [03:12<01:35,  2.72it/s]#015 64%|██████▍   | 459/718 [03:12<01:34,  2.74it/s]#015 64%|██████▍   | 460/718 [03:13<01:37,  2.64it/s]#015 64%|██████▍   | 461/718 [03:13<01:35,  2.70it/s]#015 64%|██████▍   | 462/718 [03:14<01:35,  2.69it/s]#015 64%|██████▍   | 463/718 [03:14<01:33,  2.73it/s]#015 65%|██████▍   | 464/718 [03:14<01:31,  2.76it/s]#015 65%|██████▍   | 465/718 [03:15<01:35,  2.66it/s]#015 65%|██████▍   | 466/718 [03:15<01:33,  2.71it/s]#015 65%|██████▌   | 467/718 [03:15<01:31,  2.75it/s]#015 65%|██████▌   | 468/718 [03:16<01:30,  2.77it/s]#015 65%|██████▌   | 469/718 [03:16<01:29,  2.80it/s]#015 65%|██████▌   | 470/718 [03:16<01:33,  2.65it/s]#015 66%|██████▌   | 471/718 [03:17<01:32,  2.67it/s]#015 66%|██████▌   | 472/718 [03:17<01:32,  2.67it/s]#015 66%|██████▌   | 473/718 [03:18<01:30,  2.70it/s]#015 66%|██████▌   | 474/718 [03:18<01:30,  2.70it/s]#015 66%|██████▌   | 475/718 [03:18<01:32,  2.62it/s]#015 66%|██████▋   | 476/718 [03:19<01:30,  2.68it/s]#015 66%|██████▋   | 477/718 [03:19<01:29,  2.71it/s]#015 67%|██████▋   | 478/718 [03:19<01:27,  2.75it/s]#015 67%|██████▋   | 479/718 [03:20<01:27,  2.74it/s]#015 67%|██████▋   | 480/718 [03:20<01:29,  2.65it/s]#015 67%|██████▋   | 481/718 [03:21<01:29,  2.64it/s]#015 67%|██████▋   | 482/718 [03:21<01:27,  2.69it/s]#015 67%|██████▋   | 483/718 [03:21<01:26,  2.70it/s]#015 67%|██████▋   | 484/718 [03:22<01:24,  2.76it/s]#015 68%|██████▊   | 485/718 [03:22<01:29,  2.59it/s]#015 68%|██████▊   | 486/718 [03:22<01:27,  2.65it/s]#015 68%|██████▊   | 487/718 [03:23<01:26,  2.66it/s]#015 68%|██████▊   | 488/718 [03:23<01:25,  2.69it/s]#015 68%|██████▊   | 489/718 [03:24<01:25,  2.69it/s]#015 68%|██████▊   | 490/718 [03:24<01:25,  2.66it/s]#015 68%|██████▊   | 491/718 [03:24<01:27,  2.59it/s]#015 69%|██████▊   | 492/718 [03:25<01:25,  2.63it/s]#015 69%|██████▊   | 493/718 [03:25<01:23,  2.69it/s]#015 69%|██████▉   | 494/718 [03:25<01:22,  2.73it/s]#015 69%|██████▉   | 495/718 [03:26<01:21,  2.74it/s]#015 69%|██████▉   | 496/718 [03:26<01:25,  2.60it/s]#015 69%|██████▉   | 497/718 [03:27<01:24,  2.60it/s]#015 69%|██████▉   | 498/718 [03:27<01:22,  2.67it/s]#015 69%|██████▉   | 499/718 [03:27<01:20,  2.71it/s]#015 70%|██████▉   | 500/718 [03:28<01:19,  2.74it/s]#015                                                 #015#015 70%|██████▉   | 500/718 [03:28<01:19,  2.74it/s][INFO|trainer.py:1935] 2022-01-15 18:16:26,639 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2022-01-15 18:16:26,640 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2022-01-15 18:16:27,369 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2022-01-15 18:16:27,369 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2022-01-15 18:16:27,369 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m#015 70%|██████▉   | 501/718 [03:30<03:52,  1.07s/it]#015 70%|██████▉   | 502/718 [03:31<03:06,  1.16it/s]#015 70%|███████   | 503/718 [03:31<02:33,  1.40it/s]#015 70%|███████   | 504/718 [03:31<02:11,  1.63it/s]#015 70%|███████   | 505/718 [03:32<01:54,  1.86it/s]#015 70%|███████   | 506/718 [03:32<01:45,  2.00it/s]#015 71%|███████   | 507/718 [03:33<01:35,  2.20it/s]#015 71%|███████   | 508/718 [03:33<01:29,  2.34it/s]#015 71%|███████   | 509/718 [03:33<01:24,  2.46it/s]#015 71%|███████   | 510/718 [03:34<01:21,  2.54it/s]#015 71%|███████   | 511/718 [03:34<01:22,  2.50it/s]#015 71%|███████▏  | 512/718 [03:35<01:21,  2.53it/s]#015 71%|███████▏  | 513/718 [03:35<01:18,  2.60it/s]#015 72%|███████▏  | 514/718 [03:35<01:18,  2.59it/s]#015 72%|███████▏  | 515/718 [03:36<01:16,  2.66it/s]#015 72%|███████▏  | 516/718 [03:36<01:18,  2.59it/s]#015 72%|███████▏  | 517/718 [03:36<01:15,  2.65it/s]#015 72%|███████▏  | 518/718 [03:37<01:13,  2.71it/s]#015 72%|███████▏  | 519/718 [03:37<01:13,  2.71it/s]#015 72%|███████▏  | 520/718 [03:37<01:13,  2.70it/s]#015 73%|███████▎  | 521/718 [03:38<01:14,  2.63it/s]#015 73%|███████▎  | 522/718 [03:38<01:14,  2.62it/s]#015 73%|███████▎  | 523/718 [03:39<01:13,  2.65it/s]#015 73%|███████▎  | 524/718 [03:39<01:11,  2.71it/s]#015 73%|███████▎  | 525/718 [03:39<01:10,  2.74it/s]#015 73%|███████▎  | 526/718 [03:40<01:13,  2.62it/s]#015 73%|███████▎  | 527/718 [03:40<01:12,  2.65it/s]#015 74%|███████▎  | 528/718 [03:40<01:10,  2.68it/s]#015 74%|███████▎  | 529/718 [03:41<01:09,  2.70it/s]#015 74%|███████▍  | 530/718 [03:41<01:09,  2.72it/s]#015 74%|███████▍  | 531/718 [03:42<01:11,  2.60it/s]#015 74%|███████▍  | 532/718 [03:42<01:10,  2.64it/s]#015 74%|███████▍  | 533/718 [03:42<01:10,  2.63it/s]#015 74%|███████▍  | 534/718 [03:43<01:09,  2.66it/s]#015 75%|███████▍  | 535/718 [03:43<01:07,  2.70it/s]#015 75%|███████▍  | 536/718 [03:44<01:10,  2.59it/s]#015 75%|███████▍  | 537/718 [03:44<01:08,  2.63it/s]#015 75%|███████▍  | 538/718 [03:44<01:07,  2.66it/s]#015 75%|███████▌  | 539/718 [03:45<01:06,  2.68it/s]#015 75%|███████▌  | 540/718 [03:45<01:06,  2.67it/s]#015 75%|███████▌  | 541/718 [03:45<01:10,  2.52it/s]#015 75%|███████▌  | 542/718 [03:46<01:08,  2.58it/s]#015 76%|███████▌  | 543/718 [03:46<01:06,  2.62it/s]#015 76%|███████▌  | 544/718 [03:47<01:07,  2.59it/s]#015 76%|███████▌  | 545/718 [03:47<01:05,  2.64it/s]#015 76%|███████▌  | 546/718 [03:47<01:07,  2.56it/s]#015 76%|███████▌  | 547/718 [03:48<01:05,  2.61it/s]#015 76%|███████▋  | 548/718 [03:48<01:03,  2.67it/s]#015 76%|███████▋  | 549/718 [03:48<01:02,  2.70it/s]#015 77%|███████▋  | 550/718 [03:49<01:01,  2.72it/s]#015 77%|███████▋  | 551/718 [03:49<01:05,  2.56it/s]#015 77%|███████▋  | 552/718 [03:50<01:03,  2.60it/s]#015 77%|███████▋  | 553/718 [03:50<01:02,  2.63it/s]#015 77%|███████▋  | 554/718 [03:50<01:01,  2.67it/s]#015 77%|███████▋  | 555/718 [03:51<01:00,  2.70it/s]#015 77%|███████▋  | 556/718 [03:51<01:02,  2.58it/s]#015 78%|███████▊  | 557/718 [03:51<01:01,  2.63it/s]#015 78%|███████▊  | 558/718 [03:52<00:59,  2.68it/s]#015 78%|███████▊  | 559/718 [03:52<00:59,  2.67it/s]#015 78%|███████▊  | 560/718 [03:53<00:58,  2.70it/s]#015 78%|███████▊  | 561/718 [03:53<00:57,  2.71it/s]#015 78%|███████▊  | 562/718 [03:53<00:59,  2.61it/s]#015 78%|███████▊  | 563/718 [03:54<00:58,  2.66it/s]#015 79%|███████▊  | 564/718 [03:54<00:57,  2.68it/s]#015 79%|███████▊  | 565/718 [03:54<00:56,  2.71it/s]#015 79%|███████▉  | 566/718 [03:55<00:56,  2.70it/s]#015 79%|███████▉  | 567/718 [03:55<00:57,  2.61it/s]#015 79%|███████▉  | 568/718 [03:56<00:56,  2.66it/s]#015 79%|███████▉  | 569/718 [03:56<00:55,  2.69it/s]#015 79%|███████▉  | 570/718 [03:56<00:54,  2.73it/s]#015 80%|███████▉  | 571/718 [03:57<00:53,  2.76it/s]#015 80%|███████▉  | 572/718 [03:57<00:54,  2.66it/s]#015 80%|███████▉  | 573/718 [03:57<00:53,  2.72it/s]#015 80%|███████▉  | 574/718 [03:58<00:52,  2.76it/s]#015 80%|████████  | 575/718 [03:58<00:51,  2.78it/s]#015 80%|████████  | 576/718 [03:58<00:50,  2.79it/s]#015 80%|████████  | 577/718 [03:59<00:52,  2.68it/s]#015 81%|████████  | 578/718 [03:59<00:51,  2.73it/s]#015 81%|████████  | 579/718 [04:00<00:50,  2.76it/s]#015 81%|████████  | 580/718 [04:00<00:50,  2.75it/s]#015 81%|████████  | 581/718 [04:00<00:49,  2.78it/s]#015 81%|████████  | 582/718 [04:01<00:50,  2.68it/s]#015 81%|████████  | 583/718 [04:01<00:50,  2.65it/s]#015 81%|████████▏ | 584/718 [04:01<00:49,  2.68it/s]#015 81%|████████▏ | 585/718 [04:02<00:48,  2.72it/s]#015 82%|████████▏ | 586/718 [04:02<00:47,  2.76it/s]#015 82%|████████▏ | 587/718 [04:03<00:49,  2.62it/s]#015 82%|████████▏ | 588/718 [04:03<00:48,  2.65it/s]#015 82%|████████▏ | 589/718 [04:03<00:47,  2.69it/s]#015 82%|████████▏ | 590/718 [04:04<00:47,  2.67it/s]#015 82%|████████▏ | 591/718 [04:04<00:47,  2.68it/s]#015 82%|████████▏ | 592/718 [04:04<00:48,  2.59it/s]#015 83%|████████▎ | 593/718 [04:05<00:46,  2.67it/s]#015 83%|████████▎ | 594/718 [04:05<00:46,  2.69it/s]#015 83%|████████▎ | 595/718 [04:06<00:45,  2.68it/s]#015 83%|████████▎ | 596/718 [04:06<00:45,  2.68it/s]#015 83%|████████▎ | 597/718 [04:06<00:47,  2.53it/s]#015 83%|████████▎ | 598/718 [04:07<00:46,  2.55it/s]#015 83%|████████▎ | 599/718 [04:07<00:45,  2.59it/s]#015 84%|████████▎ | 600/718 [04:08<00:45,  2.60it/s]#015 84%|████████▎ | 601/718 [04:08<00:43,  2.67it/s]#015 84%|████████▍ | 602/718 [04:08<00:44,  2.59it/s]#015 84%|████████▍ | 603/718 [04:09<00:43,  2.66it/s]#015 84%|████████▍ | 604/718 [04:09<00:42,  2.70it/s]#015 84%|████████▍ | 605/718 [04:09<00:42,  2.68it/s]#015 84%|████████▍ | 606/718 [04:10<00:41,  2.70it/s]#015 85%|████████▍ | 607/718 [04:10<00:42,  2.58it/s]#015 85%|████████▍ | 608/718 [04:11<00:41,  2.62it/s]#015 85%|████████▍ | 609/718 [04:11<00:41,  2.65it/s]#015 85%|████████▍ | 610/718 [04:11<00:40,  2.68it/s]#015 85%|████████▌ | 611/718 [04:12<00:39,  2.69it/s]#015 85%|████████▌ | 612/718 [04:12<00:40,  2.59it/s]#015 85%|████████▌ | 613/718 [04:12<00:39,  2.63it/s]#015 86%|████████▌ | 614/718 [04:13<00:39,  2.63it/s]#015 86%|████████▌ | 615/718 [04:13<00:39,  2.62it/s]#015 86%|████████▌ | 616/718 [04:14<00:38,  2.65it/s]#015 86%|████████▌ | 617/718 [04:14<00:39,  2.53it/s]#015 86%|████████▌ | 618/718 [04:14<00:38,  2.59it/s]#015 86%|████████▌ | 619/718 [04:15<00:37,  2.63it/s]#015 86%|████████▋ | 620/718 [04:15<00:37,  2.61it/s]#015 86%|████████▋ | 621/718 [04:15<00:36,  2.66it/s]#015 87%|████████▋ | 622/718 [04:16<00:35,  2.69it/s]#015 87%|████████▋ | 623/718 [04:16<00:37,  2.56it/s]#015 87%|████████▋ | 624/718 [04:17<00:35,  2.61it/s]#015 87%|████████▋ | 625/718 [04:17<00:35,  2.65it/s]#015 87%|████████▋ | 626/718 [04:17<00:34,  2.63it/s]#015 87%|████████▋ | 627/718 [04:18<00:34,  2.67it/s]#015 87%|████████▋ | 628/718 [04:18<00:34,  2.58it/s]#015 88%|████████▊ | 629/718 [04:19<00:33,  2.63it/s]#015 88%|████████▊ | 630/718 [04:19<00:33,  2.65it/s]#015 88%|████████▊ | 631/718 [04:19<00:32,  2.68it/s]#015 88%|████████▊ | 632/718 [04:20<00:31,  2.70it/s]#015 88%|████████▊ | 633/718 [04:20<00:33,  2.56it/s]#015 88%|████████▊ | 634/718 [04:20<00:32,  2.61it/s]#015 88%|████████▊ | 635/718 [04:21<00:31,  2.66it/s]#015 89%|████████▊ | 636/718 [04:21<00:30,  2.67it/s]#015 89%|████████▊ | 637/718 [04:22<00:30,  2.65it/s]#015 89%|████████▉ | 638/718 [04:22<00:31,  2.55it/s]#015 89%|████████▉ | 639/718 [04:22<00:30,  2.61it/s]#015 89%|████████▉ | 640/718 [04:23<00:29,  2.65it/s]#015 89%|████████▉ | 641/718 [04:23<00:28,  2.69it/s]#015 89%|████████▉ | 642/718 [04:23<00:28,  2.70it/s]#015 90%|████████▉ | 643/718 [04:24<00:28,  2.61it/s]#015 90%|████████▉ | 644/718 [04:24<00:27,  2.65it/s]#015 90%|████████▉ | 645/718 [04:25<00:27,  2.68it/s]#015 90%|████████▉ | 646/718 [04:25<00:26,  2.70it/s]#015 90%|█████████ | 647/718 [04:25<00:26,  2.72it/s]#015 90%|█████████ | 648/718 [04:26<00:26,  2.62it/s]#015 90%|█████████ | 649/718 [04:26<00:25,  2.66it/s]#015 91%|█████████ | 650/718 [04:26<00:25,  2.66it/s]#015 91%|█████████ | 651/718 [04:27<00:24,  2.69it/s]#015 91%|█████████ | 652/718 [04:27<00:24,  2.68it/s]#015 91%|█████████ | 653/718 [04:28<00:25,  2.51it/s]#015 91%|█████████ | 654/718 [04:28<00:24,  2.57it/s]#015 91%|█████████ | 655/718 [04:28<00:24,  2.61it/s]#015 91%|█████████▏| 656/718 [04:29<00:23,  2.65it/s]#015 92%|█████████▏| 657/718 [04:29<00:22,  2.68it/s]#015 92%|█████████▏| 658/718 [04:30<00:23,  2.58it/s]#015 92%|█████████▏| 659/718 [04:30<00:22,  2.63it/s]#015 92%|█████████▏| 660/718 [04:30<00:21,  2.67it/s]#015 92%|█████████▏| 661/718 [04:31<00:21,  2.69it/s]#015 92%|█████████▏| 662/718 [04:31<00:20,  2.71it/s]#015 92%|█████████▏| 663/718 [04:31<00:21,  2.61it/s]#015 92%|█████████▏| 664/718 [04:32<00:20,  2.65it/s]#015 93%|█████████▎| 665/718 [04:32<00:19,  2.69it/s]#015 93%|█████████▎| 666/718 [04:32<00:19,  2.71it/s]#015 93%|█████████▎| 667/718 [04:33<00:18,  2.71it/s]#015 93%|█████████▎| 668/718 [04:33<00:19,  2.61it/s]#015 93%|█████████▎| 669/718 [04:34<00:18,  2.65it/s]#015 93%|█████████▎| 670/718 [04:34<00:17,  2.68it/s]#015 93%|█████████▎| 671/718 [04:34<00:17,  2.64it/s]#015 94%|█████████▎| 672/718 [04:35<00:17,  2.63it/s]#015 94%|█████████▎| 673/718 [04:35<00:17,  2.55it/s]#015 94%|█████████▍| 674/718 [04:36<00:16,  2.61it/s]#015 94%|█████████▍| 675/718 [04:36<00:16,  2.65it/s]#015 94%|█████████▍| 676/718 [04:36<00:15,  2.69it/s]#015 94%|█████████▍| 677/718 [04:37<00:15,  2.69it/s]#015 94%|█████████▍| 678/718 [04:37<00:15,  2.56it/s]#015 95%|█████████▍| 679/718 [04:37<00:15,  2.55it/s]#015 95%|█████████▍| 680/718 [04:38<00:14,  2.59it/s]#015 95%|█████████▍| 681/718 [04:38<00:14,  2.63it/s]#015 95%|█████████▍| 682/718 [04:39<00:13,  2.64it/s]#015 95%|█████████▌| 683/718 [04:39<00:13,  2.65it/s]#015 95%|█████████▌| 684/718 [04:39<00:13,  2.58it/s]#015 95%|█████████▌| 685/718 [04:40<00:12,  2.65it/s]#015 96%|█████████▌| 686/718 [04:40<00:12,  2.67it/s]#015 96%|█████████▌| 687/718 [04:40<00:11,  2.71it/s]#015 96%|█████████▌| 688/718 [04:41<00:10,  2.75it/s]#015 96%|█████████▌| 689/718 [04:41<00:10,  2.66it/s]#015 96%|█████████▌| 690/718 [04:42<00:10,  2.72it/s]#015 96%|█████████▌| 691/718 [04:42<00:09,  2.74it/s]#015 96%|█████████▋| 692/718 [04:42<00:09,  2.76it/s]#015 97%|█████████▋| 693/718 [04:43<00:08,  2.78it/s]#015 97%|█████████▋| 694/718 [04:43<00:08,  2.68it/s]#015 97%|█████████▋| 695/718 [04:43<00:08,  2.68it/s]#015 97%|█████████▋| 696/718 [04:44<00:08,  2.70it/s]#015 97%|█████████▋| 697/718 [04:44<00:07,  2.72it/s]#015 97%|█████████▋| 698/718 [04:44<00:07,  2.73it/s]#015 97%|█████████▋| 699/718 [04:45<00:07,  2.62it/s]#015 97%|█████████▋| 700/718 [04:45<00:06,  2.68it/s]#015 98%|█████████▊| 701/718 [04:46<00:06,  2.72it/s]#015 98%|█████████▊| 702/718 [04:46<00:05,  2.71it/s]#015 98%|█████████▊| 703/718 [04:46<00:05,  2.72it/s]#015 98%|█████████▊| 704/718 [04:47<00:05,  2.55it/s]#015 98%|█████████▊| 705/718 [04:47<00:05,  2.54it/s]#015 98%|█████████▊| 706/718 [04:48<00:04,  2.60it/s]#015 98%|█████████▊| 707/718 [04:48<00:04,  2.63it/s]#015 99%|█████████▊| 708/718 [04:48<00:03,  2.66it/s]#015 99%|█████████▊| 709/718 [04:49<00:03,  2.57it/s]#015 99%|█████████▉| 710/718 [04:49<00:03,  2.60it/s]#015 99%|█████████▉| 711/718 [04:49<00:02,  2.63it/s]#015 99%|█████████▉| 712/718 [04:50<00:02,  2.59it/s]#015 99%|█████████▉| 713/718 [04:50<00:01,  2.64it/s]#015 99%|█████████▉| 714/718 [04:51<00:01,  2.53it/s]#015100%|█████████▉| 715/718 [04:51<00:01,  2.52it/s]#015100%|█████████▉| 716/718 [04:51<00:00,  2.58it/s]#015100%|█████████▉| 717/718 [04:52<00:00,  2.62it/s]#015100%|██████████| 718/718 [04:52<00:00,  2.89it/s][INFO|trainer.py:1366] 2022-01-15 18:17:51,045 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m#015                                                 #015#015100%|██████████| 718/718 [04:52<00:00,  2.89it/s]#015100%|██████████| 718/718 [04:52<00:00,  2.45it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1935] 2022-01-15 18:17:51,046 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:391] 2022-01-15 18:17:51,047 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1001] 2022-01-15 18:17:51,774 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2020] 2022-01-15 18:17:51,775 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2026] 2022-01-15 18:17:51,775 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:520] 2022-01-15 18:17:51,825 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2181] 2022-01-15 18:17:51,828 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2183] 2022-01-15 18:17:51,828 >>   Num examples = 1442\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2186] 2022-01-15 18:17:51,828 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/46 [00:00<?, ?it/s]#015  4%|▍         | 2/46 [00:00<00:04,  9.09it/s]#015  7%|▋         | 3/46 [00:00<00:06,  6.44it/s]#015  9%|▊         | 4/46 [00:00<00:07,  5.57it/s]#015 11%|█         | 5/46 [00:00<00:08,  5.07it/s]#015 13%|█▎        | 6/46 [00:01<00:08,  4.87it/s]#015 15%|█▌        | 7/46 [00:01<00:08,  4.38it/s]#015 17%|█▋        | 8/46 [00:01<00:08,  4.43it/s]#015 20%|█▉        | 9/46 [00:01<00:08,  4.47it/s]#015 22%|██▏       | 10/46 [00:02<00:08,  4.37it/s]#015 24%|██▍       | 11/46 [00:02<00:07,  4.42it/s]#015 26%|██▌       | 12/46 [00:02<00:08,  4.00it/s]#015 28%|██▊       | 13/46 [00:02<00:07,  4.15it/s]#015 30%|███       | 14/46 [00:03<00:07,  4.29it/s]#015 33%|███▎      | 15/46 [00:03<00:07,  4.37it/s]#015 35%|███▍      | 16/46 [00:03<00:06,  4.41it/s]#015 37%|███▋      | 17/46 [00:03<00:06,  4.43it/s]#015 39%|███▉      | 18/46 [00:03<00:06,  4.02it/s]#015 41%|████▏     | 19/46 [00:04<00:06,  4.16it/s]#015 43%|████▎     | 20/46 [00:04<00:06,  4.27it/s]#015 46%|████▌     | 21/46 [00:04<00:05,  4.35it/s]#015 48%|████▊     | 22/46 [00:04<00:05,  4.40it/s]#015 50%|█████     | 23/46 [00:05<00:05,  3.91it/s]#015 52%|█████▏    | 24/46 [00:05<00:05,  4.08it/s]#015 54%|█████▍    | 25/46 [00:05<00:04,  4.22it/s]#015 57%|█████▋    | 26/46 [00:05<00:04,  4.28it/s]#015 59%|█████▊    | 27/46 [00:06<00:04,  4.22it/s]#015 61%|██████    | 28/46 [00:06<00:04,  4.31it/s]#015 63%|██████▎   | 29/46 [00:06<00:04,  3.97it/s]#015 65%|██████▌   | 30/46 [00:06<00:03,  4.12it/s]#015 67%|██████▋   | 31/46 [00:07<00:03,  4.22it/s]#015 70%|██████▉   | 32/46 [00:07<00:03,  4.32it/s]#015 72%|███████▏  | 33/46 [00:07<00:02,  4.39it/s]#015 74%|███████▍  | 34/46 [00:07<00:02,  4.43it/s]#015 76%|███████▌  | 35/46 [00:08<00:02,  4.02it/s]#015 78%|███████▊  | 36/46 [00:08<00:02,  4.06it/s]#015 80%|████████  | 37/46 [00:08<00:02,  4.20it/s]#015 83%|████████▎ | 38/46 [00:08<00:01,  4.31it/s]#015 85%|████████▍ | 39/46 [00:08<00:01,  4.38it/s]#015 87%|████████▋ | 40/46 [00:09<00:01,  3.86it/s]#015 89%|████████▉ | 41/46 [00:09<00:01,  4.05it/s]#015 91%|█████████▏| 42/46 [00:09<00:00,  4.19it/s]#015 93%|█████████▎| 43/46 [00:09<00:00,  4.18it/s]#015 96%|█████████▌| 44/46 [00:10<00:00,  4.25it/s]#015 98%|█████████▊| 45/46 [00:10<00:00,  4.21it/s]#015100%|██████████| 46/46 [00:10<00:00,  4.52it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/892 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  4%|▍         | 36/892 [00:00<00:02, 357.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  8%|▊         | 75/892 [00:00<00:02, 371.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 13%|█▎        | 113/892 [00:00<00:02, 372.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 17%|█▋        | 151/892 [00:00<00:02, 333.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 22%|██▏       | 193/892 [00:00<00:01, 359.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 26%|██▌       | 230/892 [00:00<00:02, 308.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 30%|███       | 272/892 [00:00<00:01, 338.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 35%|███▌      | 313/892 [00:00<00:01, 357.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 39%|███▉      | 350/892 [00:01<00:01, 354.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 44%|████▎     | 390/892 [00:01<00:01, 366.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 48%|████▊     | 428/892 [00:01<00:01, 369.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 52%|█████▏    | 466/892 [00:01<00:01, 341.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 57%|█████▋    | 507/892 [00:01<00:01, 359.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 61%|██████▏   | 547/892 [00:01<00:00, 370.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 66%|██████▌   | 585/892 [00:01<00:00, 363.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 70%|██████▉   | 622/892 [00:01<00:00, 332.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 74%|███████▎  | 656/892 [00:02<00:01, 122.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 76%|███████▋  | 682/892 [00:03<00:02, 88.32it/s] #033[A\u001b[0m\n",
      "\u001b[34m#015 79%|███████▉  | 706/892 [00:03<00:01, 103.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%|████████▎ | 745/892 [00:03<00:01, 139.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 86%|████████▋ | 771/892 [00:03<00:00, 156.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 89%|████████▉ | 797/892 [00:03<00:00, 113.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 94%|█████████▎| 836/892 [00:03<00:00, 151.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 98%|█████████▊| 870/892 [00:04<00:00, 182.96it/s]#033[A#015100%|██████████| 892/892 [00:04<00:00, 217.74it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 46/46 [00:16<00:00,  2.74it/s]\u001b[0m\n",
      "\n",
      "2022-01-15 18:21:44 Completed - Training job completed\n",
      "Training seconds: 823\n",
      "Billable seconds: 823\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator_qa.fit({'train': f's3://{sagemaker_session_bucket}/{s3_prefix_qa}/', 'val': f's3://{sagemaker_session_bucket}/{s3_prefix_qa}/'}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_qa = huggingface_estimator_qa.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    wait=False,\n",
    "    endpoint_name=\"turkish-qa-endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only required to create a predictor from an already deployed model\n",
    "predictor_qa = HuggingFacePredictor('turkish-qa-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: \"When did he start a vagabond life?\"\n",
    "#Predicted answer: \"On his father's death\"\n",
    "\n",
    "data = {\n",
    "\"inputs\": {\n",
    "    \"question\": \"Ne zaman avare bir hayata başladı?\",\n",
    "    \"context\": \"\"\"ABASIYANIK, Sait Faik. Hikayeci (Adapazarı 23 Kasım 1906-İstanbul 11 Mayıs 1954). \\\n",
    "İlk öğrenimine Adapazarı’nda Rehber-i Terakki Mektebi’nde başladı. İki yıl kadar Adapazarı İdadisi’nde okudu.\\\n",
    "İstanbul Erkek Lisesi’nde devam ettiği orta öğrenimini Bursa Lisesi’nde tamamladı (1928). İstanbul Edebiyat \\\n",
    "Fakültesi’ne iki yıl devam ettikten sonra babasının isteği üzerine iktisat öğrenimi için İsviçre’ye gitti. \\\n",
    "Kısa süre sonra iktisat öğrenimini bırakarak Lozan’dan Grenoble’a geçti. Üç yıl başıboş bir edebiyat öğrenimi \\\n",
    "gördükten sonra babası tarafından geri çağrıldı (1933). Bir müddet Halıcıoğlu Ermeni Yetim Mektebi'nde Türkçe \\\n",
    "gurup dersleri öğretmenliği yaptı. Ticarete atıldıysa da tutunamadı. Bir ay Haber gazetesinde adliye muhabirliği\\\n",
    "yaptı (1942). Babasının ölümü üzerine aileden kalan emlakin geliri ile avare bir hayata başladı. Evlenemedi.\\\n",
    "Yazları Burgaz adasındaki köşklerinde, kışları Şişli’deki apartmanlarında annesi ile beraber geçen bu fazla \\\n",
    "içkili bohem hayatı ömrünün sonuna kadar sürdü.\"\"\"\n",
    "    }\n",
    "}\n",
    "predictor_qa.predict(data)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question: \"When did Einstein return to Germany?\"\n",
    "#Predicted answer: \"1914\"\n",
    "\n",
    "data = {\n",
    "\"inputs\": {\n",
    "    \"question\": \"Ne zaman Almanya’ya döndü?\",\n",
    "    \"context\": \"\"\"1908’de artık oldukça tanınmış, büyük bir bilim adamı olarak tanınıyordu ve Bern \\\n",
    "Üniversitesinde öğretmen olarak atanmıştı. Sonraki sene patent ofisindeki işinden ve öğretmenlikten \\\n",
    "ayrıldı ve Zürih Üniversitesinde fizik doçentliğine başladı. 1911 yılında Prag’da Karl-Ferdinand \\\n",
    "Üniversitesinde profesörlük unvanı aldı. 1914 yılında Almanya’ya döndü, Kaiser Willhelm Fizik \\\n",
    "Enstitüsü’nde yönetici, Berlin Humboldt Üniversitesinde profesör oldu. Bu işlerindeki \\\n",
    "sözleşmelerinde öğretmenlik görevlerini oldukça azaltan maddeler vardı.\"\"\"\n",
    "    }\n",
    "}\n",
    "predictor_qa.predict(data)['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
